{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6_cRP6zuezu"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quJJ7K08uezx"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from math import ceil\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import pickle \n",
    "import scipy.ndimage\n",
    "from PIL import Image as PILImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFQfVFW3uhEb"
   },
   "source": [
    "## Mount drive if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PA2nacHdugNh",
    "outputId": "55778d93-31e7-4c65-e1a8-6fa6329f8ed7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('gdrive/')\n",
    "# os.chdir('gdrive/My Drive/Colab Notebooks/FACT/')\n",
    "# '/gdrive/My Drive/Colab Notebooks/NLP1/Practical 2/googlenews.word2vec.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqtCu9riuez3"
   },
   "source": [
    "## Helper functions\n",
    "Helper functions borrowed from original paper by Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VC4XHCWOuez4"
   },
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return torch.sum(torch.pow(X, 2), dim=1)\n",
    "\n",
    "def print_and_write(str, file):\n",
    "    '''\n",
    "    print str to the console and also write it to file\n",
    "    '''\n",
    "    print(str)\n",
    "    file.write(str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WB1Z5hI9u-6d",
    "outputId": "21b78f5d-3657-444c-b684-4be27e760641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/TomLotze/Documents/Artificial Intelligence/Year 1/FACT/FACT/Reproduction'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoYPY1rxuez8"
   },
   "source": [
    "## Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJeuIBHkuez9"
   },
   "outputs": [],
   "source": [
    "# data folder\n",
    "makedirs('./data/mnist_color28')\n",
    "\n",
    "# Models folder\n",
    "model_folder = os.path.join(os.getcwd(), \"saved_model\", \"mnist_model_color28\")\n",
    "makedirs(model_folder)\n",
    "\n",
    "# Image folder\n",
    "img_folder = os.path.join(model_folder, \"img\")\n",
    "makedirs(img_folder)\n",
    "\n",
    "# Model filename\n",
    "model_filename = \"mnist_cae_color28\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# # Transforms to perform on loaded dataset. Normalize around mean 0.1307 and std 0.3081 for optimal pytorch results. \n",
    "# # source: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "# Load datasets into reproduction/data/mnist. Download if data not present. \n",
    "mnist_train = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms))\n",
    "\n",
    "mnist_train_data = mnist_train.dataset.data\n",
    "mnist_train_targets = mnist_train.dataset.targets\n",
    "\n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train_data[0:55000]\n",
    "y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train_data[55000:60000]\n",
    "y_valid = mnist_train_targets[55000:60000]\n",
    "\n",
    "# 10000 examples in test set\n",
    "mnist_test = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=False, download=True, \n",
    "                                                   transform=transforms))\n",
    "\n",
    "x_test = mnist_test.dataset.data\n",
    "y_test = mnist_test.dataset.targets\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_dataset(raw_data):\n",
    "    N = len(raw_data)\n",
    "    raw_data = raw_data.view(N, 28, 28, 1)\n",
    "    \n",
    "    # Extend to RGB\n",
    "    data_rgb = np.concatenate([raw_data, raw_data, raw_data], axis=3)\n",
    "    \n",
    "    # Make binary\n",
    "    data_binary = (data_rgb > 0.5)\n",
    "    data_color = np.zeros((N, 28, 28, 3))\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Take a random crop of the Lena image (background)\n",
    "        x_c = np.random.randint(0, lena.size[0] - 28)\n",
    "        y_c = np.random.randint(0, lena.size[1] - 28)\n",
    "        image = lena.crop((x_c, y_c, x_c + 28, y_c + 28))\n",
    "        image = np.asarray(image) / 255.0\n",
    "        \n",
    "        # Change color distribution\n",
    "        for j in range(3):\n",
    "            image[:, :, j] = (image[:, :, j] + np.random.uniform(0, 1)) / 2.0\n",
    "\n",
    "        # Invert the colors at the location of the number\n",
    "        image[data_binary[i]] = 1 - image[data_binary[i]]\n",
    "        data_color[i] = image\n",
    "\n",
    "    return torch.from_numpy(data_color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add background to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28, 3])\n",
      "torch.Size([10000, 28, 28, 3])\n"
     ]
    }
   ],
   "source": [
    "lena = PILImage.open('./resources/lena.png')\n",
    "\n",
    "x_train_color = color_dataset(mnist_train_data)\n",
    "x_test_color = color_dataset(x_test)\n",
    "\n",
    "print(x_train_color.shape)\n",
    "print(x_test_color.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_color = TensorDataset(x_train_color, mnist_train_targets)\n",
    "test_data_color = TensorDataset(x_test_color, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/mnist_color28/MNIST_color28_train.p\", \"wb\") as f:\n",
    "    pickle.dump(train_data_color, f)\n",
    "    \n",
    "with open(\"./data/mnist_color28/MNIST_color28_test.p\", \"wb\") as f:\n",
    "    pickle.dump(test_data_color, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/mnist_color28/MNIST_color28_train.p\", \"rb\") as f:\n",
    "    mnist_train = pickle.load(f)\n",
    "\n",
    "with open(\"./data/mnist_color28/MNIST_color28_test.p\", \"rb\") as f:\n",
    "    mnist_test = pickle.load(f)\n",
    "    \n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train[0:55000][0]\n",
    "y_train = mnist_train[0:55000][1]\n",
    "# y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train[55000:60000][0]\n",
    "y_valid = mnist_train[55000:60000][1]\n",
    "\n",
    "# 10000 examples in test set\n",
    "x_test = mnist_test[:][0]\n",
    "y_test = mnist_test[:][1]\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D0eBB7Yue0B"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fytgYN8Zue0B"
   },
   "outputs": [],
   "source": [
    "# COPIED FROM THE ORIGINAL IMPLEMENTATION\n",
    "# training parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1500\n",
    "\n",
    "# frequency of testing and saving\n",
    "test_display_step = 5    # how many epochs we do evaluate on the test set once, default 100\n",
    "save_step = 50            # how frequently do we save the model to disk\n",
    "\n",
    "# elastic deformation parameters\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "\n",
    "# lambda's are the ratios between the four error terms\n",
    "lambda_class = 20\n",
    "lambda_ae = 1 # autoencoder\n",
    "lambda_1 = 1 # push prototype vectors to have meaningful decodings in pixel space\n",
    "lambda_2 = 1 # cluster training examples around prototypes in latent space\n",
    "\n",
    "\n",
    "input_height = input_width =  28    # MNIST data input shape \n",
    "n_input_channel = 3     # the number of color channels; for MNIST is 1.\n",
    "input_size = input_height * input_width * n_input_channel   # 784\n",
    "n_classes = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_prototypes = 15         # the number of prototypes\n",
    "n_layers = 4\n",
    "\n",
    "# height and width of each layers' filters\n",
    "f_1 = 3\n",
    "f_2 = 3\n",
    "f_3 = 3\n",
    "f_4 = 3\n",
    "\n",
    "# stride size in each direction for each of the layers\n",
    "s_1 = 2\n",
    "s_2 = 2\n",
    "s_3 = 2\n",
    "s_4 = 2\n",
    "\n",
    "# number of feature maps in each layer\n",
    "n_map_1 = 32\n",
    "n_map_2 = 32\n",
    "n_map_3 = 32\n",
    "n_map_4 = 10\n",
    "\n",
    "# the shapes of each layer's filter\n",
    "# [out channel, in_channel, 3, 3]\n",
    "filter_shape_1 = [n_map_1, n_input_channel, f_1, f_1]\n",
    "filter_shape_2 = [n_map_2, n_map_1, f_2, f_2]\n",
    "filter_shape_3 = [n_map_3, n_map_2, f_3, f_3]\n",
    "filter_shape_4 = [n_map_4, n_map_3, f_4, f_4]\n",
    "\n",
    "# strides for each layer (changed to tuples)\n",
    "stride_1 = [s_1, s_1]\n",
    "stride_2 = [s_2, s_2]\n",
    "stride_3 = [s_3, s_3]\n",
    "stride_4 = [s_4, s_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "077FcrZkue0D"
   },
   "source": [
    "## Model construction\n",
    "#### <font color='red'>Fix the stride and padding parameters, check if filter in tf is same as weight in pt</font>\n",
    "Padding discussion pytorch: https://github.com/pytorch/pytorch/issues/3867\n",
    "\n",
    "Blogpost: https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pfh3k6UDue0D"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder'''\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "        \n",
    "        # define layers\n",
    "        self.enc_l1 = nn.Conv2d(n_input_channel, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l4 = nn.Conv2d(32, 10, kernel_size=3, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def pad_image(self, img):\n",
    "        ''' Takes an input image (batch) and pads according to Tensorflows SAME padding'''\n",
    "        input_h = img.shape[2]\n",
    "        input_w = img.shape[3]\n",
    "        stride = 2 \n",
    "        filter_h = 3\n",
    "        filter_w = 3\n",
    "\n",
    "        output_h = int(ceil(float(input_h)) / float(stride))\n",
    "        output_w = output_h\n",
    "\n",
    "        if input_h % stride == 0:\n",
    "            pad_height = max((filter_h - stride), 0)\n",
    "        else:\n",
    "            pad_height = max((filter_h - (input_h % stride), 0))\n",
    "\n",
    "        pad_width = pad_height\n",
    "\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        padded_img = torch.zeros(img.shape[0], img.shape[1], input_h + pad_height, input_w + pad_width)\n",
    "        padded_img[:,:, pad_top:-pad_bottom, pad_left:-pad_right] = img\n",
    "\n",
    "        return padded_img\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pad_x = self.pad_image(x)\n",
    "        el1 = self.relu(self.enc_l1(pad_x))\n",
    "        \n",
    "        pad_el1 = self.pad_image(el1)\n",
    "        el2 = self.relu(self.enc_l2(pad_el1))\n",
    "    \n",
    "        pad_el2 = self.pad_image(el2)\n",
    "        el3 = self.relu(self.enc_l3(pad_el2))\n",
    "        \n",
    "        pad_el3 = self.pad_image(el3)\n",
    "        el4 = self.relu(self.enc_l4(pad_el3))\n",
    "        \n",
    "        return el4\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder'''\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "\n",
    "        # define layers\n",
    "        self.dec_l4 = nn.ConvTranspose2d(10, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l3 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=0) # the output padding here should be 1 if the images are 32x32\n",
    "        self.dec_l2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l1 = nn.ConvTranspose2d(32, n_input_channel, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        dl4 = self.relu(self.dec_l4(enc_x))\n",
    "        dl3 = self.relu(self.dec_l3(dl4))\n",
    "        dl2 = self.relu(self.dec_l2(dl3))\n",
    "        decoded_x = self.sigmoid(self.dec_l1(dl2))\n",
    "        \n",
    "        return decoded_x\n",
    "\n",
    "\n",
    "class nn_prototype(nn.Module):\n",
    "    '''Model'''\n",
    "    def __init__(self, n_prototypes=15, n_layers=4, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        # initialize prototype - currently not in correct spot\n",
    "        \n",
    "        # changed this for the colored mnist, from 40 to 160, the new shape would be 250*10*4*4\n",
    "        n_features = 40 # size of encoded x - 250 x 10 x 2 x 2\n",
    "        self.prototype_feature_vectors = nn.Parameter(torch.empty(size=(n_prototypes, n_features), \n",
    "                                                                  dtype=torch.float32).uniform_())\n",
    "        \n",
    "        self.last_layer = nn.Linear(n_prototypes,10)\n",
    "        \n",
    "    def list_of_distances(self, X, Y):\n",
    "        '''\n",
    "        Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "        Y = [y_1, ... , y_m], we return a list of vectors\n",
    "                [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "                 ...\n",
    "                 [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "        where the distance metric used is the sqared euclidean distance.\n",
    "        The computation is achieved through a clever use of broadcasting.\n",
    "        '''\n",
    "        XX = torch.reshape(self.list_of_norms(X), shape=(-1, 1))\n",
    "        YY = torch.reshape(self.list_of_norms(Y), shape=(1, -1))\n",
    "        output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def list_of_norms(self, X):\n",
    "        '''\n",
    "        X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "            [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "        function is the squared euclidean distance.\n",
    "        '''\n",
    "        return torch.sum(torch.pow(X, 2), dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"Shape of input x\", x.shape)\n",
    "        \n",
    "        #encoder step\n",
    "        enc_x = self.encoder(x)\n",
    "        \n",
    "        #print(\"Shape of encoded x\", enc_x.shape)\n",
    "        \n",
    "        #decoder step\n",
    "        dec_x = self.decoder(enc_x)\n",
    "        \n",
    "        #print(\"shape of decoded x\", dec_x.shape)\n",
    "        \n",
    "        # hardcoded input size (not needed, shape already correct)\n",
    "        # dec_x = dec_x.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        \n",
    "        # flatten encoded x to compute distance with prototypes\n",
    "        n_features = enc_x.shape[1] * enc_x.shape[2] * enc_x.shape[3]\n",
    "        feature_vectors_flat = torch.reshape(enc_x, shape=[-1, n_features])\n",
    "        \n",
    "        #print(\"Shape of flattened feature vectors\", feature_vectors_flat.shape)\n",
    "        \n",
    "        # distance to prototype\n",
    "        prototype_distances = self.list_of_distances(feature_vectors_flat, self.prototype_feature_vectors)\n",
    "        \n",
    "        # distance to feature vectors\n",
    "        feature_vector_distances = self.list_of_distances(self.prototype_feature_vectors, feature_vectors_flat)\n",
    "        \n",
    "        # classification layer\n",
    "        logits = self.last_layer(prototype_distances)\n",
    "        \n",
    "        # Softmax to prob dist not needed as cross entropy loss is used?\n",
    "        \n",
    "        return dec_x, logits, feature_vector_distances, prototype_distances\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebySJKDyue0E"
   },
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCRlYzWJue0E"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "the error function consists of 4 terms, the autoencoder loss,\n",
    "the classification loss, and the two requirements that every feature vector in\n",
    "X look like at least one of the prototype feature vectors and every prototype\n",
    "feature vector look like at least one of the feature vectors in X.\n",
    "'''\n",
    "def loss_function(X_decoded, X_true, logits, Y, feature_dist, prototype_dist, lambdas=None, print_flag=False):\n",
    "    if lambdas == None:\n",
    "        lam_class, lam_ae, lam_1, lam_2 = lambda_class, lambda_ae, lambda_1, lambda_2\n",
    "    \n",
    "    ae_error = torch.mean(list_of_norms(X_decoded - X_true))\n",
    "#     ae_error = F.binary_cross_entropy(X_decoded, X_true)\n",
    "    class_error = F.cross_entropy(logits, Y, reduction=\"mean\")\n",
    "    error_1 = torch.mean(torch.min(feature_dist, axis=1)[0])\n",
    "    error_2 = torch.mean(torch.min(prototype_dist, axis = 1)[0])\n",
    "\n",
    "    # total_error is the our minimization objective\n",
    "    total_error = lam_class * class_error +\\\n",
    "                  lam_ae * ae_error + \\\n",
    "                  lam_1 * error_1 + \\\n",
    "                  lam_2 * error_2\n",
    "    \n",
    "    if print_flag == True:\n",
    "        print('classification error', class_error.item())\n",
    "        print('AE error: ', ae_error.item())\n",
    "        print('Error 1 and 2', error_1.item(), error_2.item())\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9L6xKcLlue0G"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJPQHz5xue0G"
   },
   "outputs": [],
   "source": [
    "def compute_acc(logits, labels):\n",
    "    batch_size = labels.shape[0]\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    total_correct = torch.sum(predictions == labels).item()\n",
    "    accuracy = total_correct / batch_size\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyyShfpEue0H"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CBtrrsoue0I"
   },
   "outputs": [],
   "source": [
    "def visualize_prototypes(model, epoch, save=True):\n",
    "    # get saved prototypes\n",
    "    encoded_prototypes = model.prototype_feature_vectors\n",
    "    encoded_prototypes_reshaped = encoded_prototypes.view(n_prototypes, 10, 2, 2)\n",
    "\n",
    "    # decode prototypes\n",
    "    decoded_prototypes = model.decoder(encoded_prototypes_reshaped).detach().numpy()\n",
    "    \n",
    "    dec_prot = decoded_prototypes.transpose(0, 2, 3, 1)\n",
    "\n",
    "    for i in range(n_prototypes):\n",
    "        plt.imshow(dec_prot[i])\n",
    "        if save:\n",
    "            makedirs(img_folder+\"/prototypes_epoch_\"+ str(epoch))\n",
    "            plt.savefig(img_folder+\"/prototypes_epoch_\"+ str(epoch)+\"/\"+str(i)+\".png\")\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "am9_dX2eue0J",
    "outputId": "e3fd512a-e0d3-48df-ddc2-ad1f1bfe0cfe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last train loss of batch: 47.40250778198242\n",
      "Train acc on batch: 0.12221004566210045\n",
      "Last train acc 0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type nn_prototype. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and prototypes of epoch 0 are saved\n",
      "\n",
      "Test loss: 46.49867630004883\n",
      "Test acc: 0.1713\n",
      "classification error tensor(2.2295)\n",
      "AE error:  tensor(0.0940)\n",
      "Error 1 and 2 tensor(1.6122) tensor(0.2810)\n",
      "\n",
      "Valid loss: 46.57788848876953\n",
      "Valid acc: 0.173\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last train loss of batch: 25.742877960205078\n",
      "Train acc on batch: 0.465296803652968\n",
      "Last train acc 0.66\n",
      "classification error tensor(1.0375)\n",
      "AE error:  tensor(0.0925)\n",
      "Error 1 and 2 tensor(0.9889) tensor(2.4946)\n",
      "\n",
      "Valid loss: 24.325456619262695\n",
      "Valid acc: 0.6656\n",
      "\n",
      "Epoch: 2\n"
     ]
    }
   ],
   "source": [
    "model = nn_prototype(15,4,10)\n",
    "batch_size_ = 250\n",
    "\n",
    "# get validation and test set\n",
    "valid_dl = DataLoader(valid_data, batch_size=5000, drop_last=False, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=10000, drop_last=False, shuffle=False)\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# initialize storage for results\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "valid_accs = []\n",
    "valid_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(training_epochs):\n",
    "    print(\"\\nEpoch:\", epoch)\n",
    "\n",
    "    # load the training data and reshuffle\n",
    "    train_dl = DataLoader(train_data, batch_size=batch_size_, drop_last=False, shuffle=True)\n",
    "\n",
    "    # loop over the batches\n",
    "    for step, (x, Y) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_plot = x[0].clone()\n",
    "        \n",
    "        x = x.view(x.shape[0], n_input_channel, x.shape[1], x.shape[2]).float()\n",
    "\n",
    "        #Y = Y.long()\n",
    "        \n",
    "        # perform forward pass\n",
    "        X_decoded, logits, feature_dist, prot_dist = model(x)\n",
    "\n",
    "        # compute the loss\n",
    "        total_loss = loss_function(X_decoded, x, logits, Y, feature_dist, prot_dist)\n",
    "\n",
    "        # backpropagate over the loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute and save accuracy and loss\n",
    "        train_accuracy = compute_acc(logits, Y)\n",
    "        train_accs.append(train_accuracy)\n",
    "        train_losses.append(total_loss.item())\n",
    "    \n",
    "    # encode one training example to check:\n",
    "    plt.imshow(x_plot)\n",
    "    plt.show()\n",
    "    plt.imshow(X_decoded[0].detach().view(28, 28, 3))\n",
    "    plt.show()\n",
    "\n",
    "    # print information after a batch\n",
    "    print('Last train loss of batch:', total_loss.item())\n",
    "    print('Train acc on batch:', np.mean(train_accs[-step:]))\n",
    "    print(\"Last train acc\", train_accuracy)\n",
    "\n",
    "\n",
    "    if epoch % test_display_step == 0:\n",
    "        # save model and prototypes\n",
    "        torch.save(model, model_folder + \"/\" + model_filename + \"_epoch_\" + str(epoch) + '.pt')\n",
    "  \n",
    "        \n",
    "        # save model prototypes\n",
    "        visualize_prototypes(model, epoch, save = True)\n",
    "        print(\"Model and prototypes of epoch %d are saved\"%epoch)\n",
    "        \n",
    "        # perform testing\n",
    "        with torch.no_grad():\n",
    "            for (x_test, y_test) in test_dl:\n",
    "                x_test = x_test.view(x_test.shape[0], n_input_channel, x_test.shape[1], x_test.shape[2]).float()\n",
    "\n",
    "                #y_test = y_test.long()\n",
    "\n",
    "                # forward pass\n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_test)\n",
    "\n",
    "                # compute loss and accuracy and save\n",
    "                test_accuracy = compute_acc(logits, y_test)\n",
    "                test_loss = loss_function(X_decoded, x_test, logits, y_test, feature_dist, prot_dist)\n",
    "                test_accs.append(test_accuracy)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "            print('\\nTest loss:', test_loss.item())\n",
    "            print('Test acc:', test_accuracy)\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        for (x_valid, y_valid) in valid_dl:\n",
    "                x_valid = x_valid.view(x_valid.shape[0], n_input_channel, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "        \n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_valid)\n",
    "\n",
    "                # compute losses and accuracy and save\n",
    "                valid_accuracy = compute_acc(logits, y_valid)\n",
    "                valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist, print_flag=True)\n",
    "                valid_accs.append(valid_accuracy)\n",
    "                valid_losses.append(valid_loss)\n",
    "\n",
    "        print('\\nValid loss:', valid_loss.item())\n",
    "        print('Valid acc:', valid_accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8PVm-2Tue0L"
   },
   "source": [
    "## Loading the model and visualize prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9NsktZFue0L"
   },
   "outputs": [],
   "source": [
    "# # load the model\n",
    "# loaded_model = torch.load(model_folder+\"/\"+model_filename)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#         for step, (x_valid, y_valid) in enumerate(valid_dl):\n",
    "#                 x_valid = x_valid.view(x_valid.shape[0], 1, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "#                 X_decoded, logits, feature_dist, prot_dist = loaded_model(x_valid)\n",
    "\n",
    "#                 # Check is model is indeed trained\n",
    "#                 valid_accuracy = compute_acc(logits, y_valid)\n",
    "#                 valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist)\n",
    "\n",
    "#         print('\\nValid loss:', valid_loss.item())\n",
    "#         print('Valid acc:', valid_accuracy)\n",
    "\n",
    "# visualize_prototypes(loaded_model, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lR86Fwbue0M"
   },
   "outputs": [],
   "source": [
    "# visualize prototypes from current model (in memory, not the loaded one)\n",
    "visualize_prototypes(model, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsKJCqlJue0N"
   },
   "source": [
    "## Saving and plotting of losses and accuracies\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTsODvIyue0N"
   },
   "outputs": [],
   "source": [
    "with open(model_folder + '/train_accs.p', 'wb') as f:\n",
    "    pickle.dump(train_accs, f)\n",
    "\n",
    "with open(model_folder + '/test_accs.p', 'wb') as f:\n",
    "    pickle.dump(test_accs, f)\n",
    "\n",
    "with open(model_folder + '/valid_accs.p', 'wb') as f:\n",
    "    pickle.dump(valid_accs, f)\n",
    "\n",
    "with open(model_folder + '/train_losses.p', 'wb') as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "\n",
    "with open(model_folder + '/test_losses.p', 'wb') as f:\n",
    "    pickle.dump(test_losses, f)\n",
    "\n",
    "with open(model_folder + '/valid_losses.p', 'wb') as f:\n",
    "    pickle.dump(valid_losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MtynokTLue0O",
    "outputId": "7d9840ec-741c-488a-c12e-959eb3a956a2"
   },
   "outputs": [],
   "source": [
    "v_t_epochs = list(range(0,len(train_accs)))\n",
    "test_epochs = list(range(0,len(train_accs),test_display_step))\n",
    "print(len(train_accs))\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(v_t_epochs, train_accs, label=\"Training accuracy\")\n",
    "plt.plot(v_t_epochs, valid_accs, label=\"Valid accuracy\")\n",
    "plt.plot(test_epochs, test_accs, label=\"Test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(v_t_epochs, valid_losses, label=\"Valid loss\")\n",
    "plt.plot(v_t_epochs, train_losses, label=\"Training loss\")\n",
    "plt.plot(test_epochs, test_losses, label=\"Test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-NqDa4Yue0P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
