{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6_cRP6zuezu"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quJJ7K08uezx"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from math import ceil\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import pickle \n",
    "import scipy.ndimage\n",
    "from PIL import Image as PILImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFQfVFW3uhEb"
   },
   "source": [
    "## Mount drive if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PA2nacHdugNh",
    "outputId": "55778d93-31e7-4c65-e1a8-6fa6329f8ed7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('gdrive/')\n",
    "# os.chdir('gdrive/My Drive/Colab Notebooks/FACT/')\n",
    "# '/gdrive/My Drive/Colab Notebooks/NLP1/Practical 2/googlenews.word2vec.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqtCu9riuez3"
   },
   "source": [
    "## Helper functions\n",
    "Helper functions borrowed from original paper by Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VC4XHCWOuez4"
   },
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return torch.sum(torch.pow(X, 2), dim=1)\n",
    "\n",
    "def print_and_write(str, file):\n",
    "    '''\n",
    "    print str to the console and also write it to file\n",
    "    '''\n",
    "    print(str)\n",
    "    file.write(str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WB1Z5hI9u-6d",
    "outputId": "21b78f5d-3657-444c-b684-4be27e760641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/TomLotze/Documents/Artificial Intelligence/Year 1/FACT/FACT/Reproduction'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoYPY1rxuez8"
   },
   "source": [
    "## Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJeuIBHkuez9"
   },
   "outputs": [],
   "source": [
    "# data folder\n",
    "makedirs('./data/mnist_color28')\n",
    "\n",
    "# Models folder\n",
    "model_folder = os.path.join(os.getcwd(), \"saved_model\", \"mnist_model_color28\")\n",
    "makedirs(model_folder)\n",
    "\n",
    "# Image folder\n",
    "img_folder = os.path.join(model_folder, \"img\")\n",
    "makedirs(img_folder)\n",
    "\n",
    "# Model filename\n",
    "model_filename = \"mnist_cae_color28\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# # Transforms to perform on loaded dataset. Normalize around mean 0.1307 and std 0.3081 for optimal pytorch results. \n",
    "# # source: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "# Load datasets into reproduction/data/mnist. Download if data not present. \n",
    "mnist_train = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms))\n",
    "\n",
    "mnist_train_data = mnist_train.dataset.data\n",
    "mnist_train_targets = mnist_train.dataset.targets\n",
    "\n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train_data[0:55000]\n",
    "y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train_data[55000:60000]\n",
    "y_valid = mnist_train_targets[55000:60000]\n",
    "\n",
    "# 10000 examples in test set\n",
    "mnist_test = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=False, download=True, \n",
    "                                                   transform=transforms))\n",
    "\n",
    "x_test = mnist_test.dataset.data\n",
    "y_test = mnist_test.dataset.targets\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_dataset(raw_data):\n",
    "    N = len(raw_data)\n",
    "    raw_data = raw_data.view(N, 28, 28, 1)\n",
    "    \n",
    "    # Extend to RGB\n",
    "    data_rgb = np.concatenate([raw_data, raw_data, raw_data], axis=3)\n",
    "    \n",
    "    # Make binary\n",
    "    data_binary = (data_rgb > 0.5)\n",
    "    data_color = np.zeros((N, 28, 28, 3))\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Take a random crop of the Lena image (background)\n",
    "        x_c = np.random.randint(0, lena.size[0] - 28)\n",
    "        y_c = np.random.randint(0, lena.size[1] - 28)\n",
    "        image = lena.crop((x_c, y_c, x_c + 28, y_c + 28))\n",
    "        image = np.asarray(image) / 255.0\n",
    "        \n",
    "        # Change color distribution\n",
    "        for j in range(3):\n",
    "            image[:, :, j] = (image[:, :, j] + np.random.uniform(0, 1)) / 2.0\n",
    "\n",
    "        # Invert the colors at the location of the number\n",
    "        image[data_binary[i]] = 1 - image[data_binary[i]]\n",
    "        data_color[i] = image\n",
    "\n",
    "    return torch.from_numpy(data_color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add background to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28, 3])\n",
      "torch.Size([10000, 28, 28, 3])\n"
     ]
    }
   ],
   "source": [
    "lena = PILImage.open('./resources/lena.png')\n",
    "\n",
    "x_train_color = color_dataset(mnist_train_data)\n",
    "x_test_color = color_dataset(x_test)\n",
    "\n",
    "print(x_train_color.shape)\n",
    "print(x_test_color.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_color = TensorDataset(x_train_color, mnist_train_targets)\n",
    "test_data_color = TensorDataset(x_test_color, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/mnist_color28/MNIST_color28_train.p\", \"wb\") as f:\n",
    "    pickle.dump(train_data_color, f)\n",
    "    \n",
    "with open(\"./data/mnist_color28/MNIST_color28_test.p\", \"wb\") as f:\n",
    "    pickle.dump(test_data_color, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/mnist_color28/MNIST_color28_train.p\", \"rb\") as f:\n",
    "    mnist_train = pickle.load(f)\n",
    "\n",
    "with open(\"./data/mnist_color28/MNIST_color28_test.p\", \"rb\") as f:\n",
    "    mnist_test = pickle.load(f)\n",
    "    \n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train[0:55000][0]\n",
    "y_train = mnist_train[0:55000][1]\n",
    "# y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train[55000:60000][0]\n",
    "y_valid = mnist_train[55000:60000][1]\n",
    "\n",
    "# 10000 examples in test set\n",
    "x_test = mnist_test[:][0]\n",
    "y_test = mnist_test[:][1]\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D0eBB7Yue0B"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fytgYN8Zue0B"
   },
   "outputs": [],
   "source": [
    "# COPIED FROM THE ORIGINAL IMPLEMENTATION\n",
    "# training parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1500\n",
    "\n",
    "# frequency of testing and saving\n",
    "test_display_step = 5    # how many epochs we do evaluate on the test set once, default 100\n",
    "save_step = 50            # how frequently do we save the model to disk\n",
    "\n",
    "# elastic deformation parameters\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "\n",
    "# lambda's are the ratios between the four error terms\n",
    "lambda_class = 20\n",
    "lambda_ae = 1 # autoencoder\n",
    "lambda_1 = 1 # push prototype vectors to have meaningful decodings in pixel space\n",
    "lambda_2 = 1 # cluster training examples around prototypes in latent space\n",
    "\n",
    "\n",
    "input_height = input_width =  28    # MNIST data input shape \n",
    "n_input_channel = 3     # the number of color channels; for MNIST is 1.\n",
    "input_size = input_height * input_width * n_input_channel   # 784\n",
    "n_classes = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_prototypes = 15         # the number of prototypes\n",
    "n_layers = 4\n",
    "\n",
    "# height and width of each layers' filters\n",
    "f_1 = 3\n",
    "f_2 = 3\n",
    "f_3 = 3\n",
    "f_4 = 3\n",
    "\n",
    "# stride size in each direction for each of the layers\n",
    "s_1 = 2\n",
    "s_2 = 2\n",
    "s_3 = 2\n",
    "s_4 = 2\n",
    "\n",
    "# number of feature maps in each layer\n",
    "n_map_1 = 32\n",
    "n_map_2 = 32\n",
    "n_map_3 = 32\n",
    "n_map_4 = 10\n",
    "\n",
    "# the shapes of each layer's filter\n",
    "# [out channel, in_channel, 3, 3]\n",
    "filter_shape_1 = [n_map_1, n_input_channel, f_1, f_1]\n",
    "filter_shape_2 = [n_map_2, n_map_1, f_2, f_2]\n",
    "filter_shape_3 = [n_map_3, n_map_2, f_3, f_3]\n",
    "filter_shape_4 = [n_map_4, n_map_3, f_4, f_4]\n",
    "\n",
    "# strides for each layer (changed to tuples)\n",
    "stride_1 = [s_1, s_1]\n",
    "stride_2 = [s_2, s_2]\n",
    "stride_3 = [s_3, s_3]\n",
    "stride_4 = [s_4, s_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "077FcrZkue0D"
   },
   "source": [
    "## Model construction\n",
    "#### <font color='red'>Fix the stride and padding parameters, check if filter in tf is same as weight in pt</font>\n",
    "Padding discussion pytorch: https://github.com/pytorch/pytorch/issues/3867\n",
    "\n",
    "Blogpost: https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pfh3k6UDue0D"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder'''\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "        \n",
    "        # define layers\n",
    "        self.enc_l1 = nn.Conv2d(n_input_channel, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l4 = nn.Conv2d(32, 10, kernel_size=3, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def pad_image(self, img):\n",
    "        ''' Takes an input image (batch) and pads according to Tensorflows SAME padding'''\n",
    "        input_h = img.shape[2]\n",
    "        input_w = img.shape[3]\n",
    "        stride = 2 \n",
    "        filter_h = 3\n",
    "        filter_w = 3\n",
    "\n",
    "        output_h = int(ceil(float(input_h)) / float(stride))\n",
    "        output_w = output_h\n",
    "\n",
    "        if input_h % stride == 0:\n",
    "            pad_height = max((filter_h - stride), 0)\n",
    "        else:\n",
    "            pad_height = max((filter_h - (input_h % stride), 0))\n",
    "\n",
    "        pad_width = pad_height\n",
    "\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        padded_img = torch.zeros(img.shape[0], img.shape[1], input_h + pad_height, input_w + pad_width)\n",
    "        padded_img[:,:, pad_top:-pad_bottom, pad_left:-pad_right] = img\n",
    "\n",
    "        return padded_img\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pad_x = self.pad_image(x)\n",
    "        el1 = self.relu(self.enc_l1(pad_x))\n",
    "        \n",
    "        pad_el1 = self.pad_image(el1)\n",
    "        el2 = self.relu(self.enc_l2(pad_el1))\n",
    "    \n",
    "        pad_el2 = self.pad_image(el2)\n",
    "        el3 = self.relu(self.enc_l3(pad_el2))\n",
    "        \n",
    "        pad_el3 = self.pad_image(el3)\n",
    "        el4 = self.relu(self.enc_l4(pad_el3))\n",
    "        \n",
    "        return el4\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder'''\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "\n",
    "        # define layers\n",
    "        self.dec_l4 = nn.ConvTranspose2d(10, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l3 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=0) # the output padding here should be 1 if the images are 32x32\n",
    "        self.dec_l2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l1 = nn.ConvTranspose2d(32, n_input_channel, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        dl4 = self.relu(self.dec_l4(enc_x))\n",
    "        dl3 = self.relu(self.dec_l3(dl4))\n",
    "        dl2 = self.relu(self.dec_l2(dl3))\n",
    "        decoded_x = self.sigmoid(self.dec_l1(dl2))\n",
    "        \n",
    "        return decoded_x\n",
    "\n",
    "\n",
    "class nn_prototype(nn.Module):\n",
    "    '''Model'''\n",
    "    def __init__(self, n_prototypes=15, n_layers=4, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        # initialize prototype - currently not in correct spot\n",
    "        \n",
    "        # changed this for the colored mnist, from 40 to 160, the new shape would be 250*10*4*4\n",
    "        n_features = 40 # size of encoded x - 250 x 10 x 2 x 2\n",
    "        self.prototype_feature_vectors = nn.Parameter(torch.empty(size=(n_prototypes, n_features), \n",
    "                                                                  dtype=torch.float32).uniform_())\n",
    "        \n",
    "        self.last_layer = nn.Linear(n_prototypes,10)\n",
    "        \n",
    "    def list_of_distances(self, X, Y):\n",
    "        '''\n",
    "        Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "        Y = [y_1, ... , y_m], we return a list of vectors\n",
    "                [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "                 ...\n",
    "                 [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "        where the distance metric used is the sqared euclidean distance.\n",
    "        The computation is achieved through a clever use of broadcasting.\n",
    "        '''\n",
    "        XX = torch.reshape(self.list_of_norms(X), shape=(-1, 1))\n",
    "        YY = torch.reshape(self.list_of_norms(Y), shape=(1, -1))\n",
    "        output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def list_of_norms(self, X):\n",
    "        '''\n",
    "        X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "            [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "        function is the squared euclidean distance.\n",
    "        '''\n",
    "        return torch.sum(torch.pow(X, 2), dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"Shape of input x\", x.shape)\n",
    "        \n",
    "        #encoder step\n",
    "        enc_x = self.encoder(x)\n",
    "        \n",
    "        #print(\"Shape of encoded x\", enc_x.shape)\n",
    "        \n",
    "        #decoder step\n",
    "        dec_x = self.decoder(enc_x)\n",
    "        \n",
    "        #print(\"shape of decoded x\", dec_x.shape)\n",
    "        \n",
    "        # hardcoded input size (not needed, shape already correct)\n",
    "        # dec_x = dec_x.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        \n",
    "        # flatten encoded x to compute distance with prototypes\n",
    "        n_features = enc_x.shape[1] * enc_x.shape[2] * enc_x.shape[3]\n",
    "        feature_vectors_flat = torch.reshape(enc_x, shape=[-1, n_features])\n",
    "        \n",
    "        #print(\"Shape of flattened feature vectors\", feature_vectors_flat.shape)\n",
    "        \n",
    "        # distance to prototype\n",
    "        prototype_distances = self.list_of_distances(feature_vectors_flat, self.prototype_feature_vectors)\n",
    "        \n",
    "        # distance to feature vectors\n",
    "        feature_vector_distances = self.list_of_distances(self.prototype_feature_vectors, feature_vectors_flat)\n",
    "        \n",
    "        # classification layer\n",
    "        logits = self.last_layer(prototype_distances)\n",
    "        \n",
    "        # Softmax to prob dist not needed as cross entropy loss is used?\n",
    "        \n",
    "        return dec_x, logits, feature_vector_distances, prototype_distances\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebySJKDyue0E"
   },
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCRlYzWJue0E"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "the error function consists of 4 terms, the autoencoder loss,\n",
    "the classification loss, and the two requirements that every feature vector in\n",
    "X look like at least one of the prototype feature vectors and every prototype\n",
    "feature vector look like at least one of the feature vectors in X.\n",
    "'''\n",
    "def loss_function(X_decoded, X_true, logits, Y, feature_dist, prototype_dist, lambdas=None, print_flag=False):\n",
    "    if lambdas == None:\n",
    "        lam_class, lam_ae, lam_1, lam_2 = lambda_class, lambda_ae, lambda_1, lambda_2\n",
    "    \n",
    "    ae_error = torch.mean(list_of_norms(X_decoded - X_true))\n",
    "#     ae_error = F.binary_cross_entropy(X_decoded, X_true)\n",
    "    class_error = F.cross_entropy(logits, Y, reduction=\"mean\")\n",
    "    error_1 = torch.mean(torch.min(feature_dist, axis=1)[0])\n",
    "    error_2 = torch.mean(torch.min(prototype_dist, axis = 1)[0])\n",
    "\n",
    "    # total_error is the our minimization objective\n",
    "    total_error = lam_class * class_error +\\\n",
    "                  lam_ae * ae_error + \\\n",
    "                  lam_1 * error_1 + \\\n",
    "                  lam_2 * error_2\n",
    "    \n",
    "    if print_flag == True:\n",
    "        print('classification error', class_error.item())\n",
    "        print('AE error: ', ae_error.item())\n",
    "        print('Error 1: %f and 2: %f' %(error_1.item(), error_2.item()))\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9L6xKcLlue0G"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJPQHz5xue0G"
   },
   "outputs": [],
   "source": [
    "def compute_acc(logits, labels):\n",
    "    batch_size = labels.shape[0]\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    total_correct = torch.sum(predictions == labels).item()\n",
    "    accuracy = total_correct / batch_size\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyyShfpEue0H"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CBtrrsoue0I"
   },
   "outputs": [],
   "source": [
    "def visualize_prototypes(model, epoch, save=True):\n",
    "    # get saved prototypes\n",
    "    encoded_prototypes = model.prototype_feature_vectors\n",
    "    encoded_prototypes_reshaped = encoded_prototypes.view(n_prototypes, 10, 2, 2)\n",
    "\n",
    "    # decode prototypes\n",
    "    decoded_prototypes = model.decoder(encoded_prototypes_reshaped).detach().numpy()\n",
    "    \n",
    "    dec_prot = decoded_prototypes.transpose(0, 2, 3, 1)\n",
    "\n",
    "    for i in range(n_prototypes):\n",
    "        plt.imshow(dec_prot[i])\n",
    "        if save:\n",
    "            makedirs(img_folder+\"/prototypes_epoch_\"+ str(epoch))\n",
    "            plt.savefig(img_folder+\"/prototypes_epoch_\"+ str(epoch)+\"/\"+str(i)+\".png\")\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "am9_dX2eue0J",
    "outputId": "e3fd512a-e0d3-48df-ddc2-ad1f1bfe0cfe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last train loss of batch: 45.18730545043945\n",
      "Train acc on batch: 0.15930593607305937\n",
      "Last train acc 0.252\n",
      "Model and prototypes of epoch 0 are saved\n"
     ]
    }
   ],
   "source": [
    "model = nn_prototype(15,4,10)\n",
    "batch_size_ = 250\n",
    "\n",
    "# get validation and test set\n",
    "valid_dl = DataLoader(valid_data, batch_size=5000, drop_last=False, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=10000, drop_last=False, shuffle=False)\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# initialize storage for results\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "valid_accs = []\n",
    "valid_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(training_epochs):\n",
    "    print(\"\\nEpoch:\", epoch)\n",
    "\n",
    "    # load the training data and reshuffle\n",
    "    train_dl = DataLoader(train_data, batch_size=batch_size_, drop_last=False, shuffle=True)\n",
    "\n",
    "    # loop over the batches\n",
    "    for step, (x, Y) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_plot = x[0].clone()\n",
    "        \n",
    "        x = x.view(x.shape[0], n_input_channel, x.shape[1], x.shape[2]).float()\n",
    "\n",
    "        #Y = Y.long()\n",
    "        \n",
    "        # perform forward pass\n",
    "        X_decoded, logits, feature_dist, prot_dist = model(x)\n",
    "\n",
    "        # compute the loss\n",
    "        total_loss = loss_function(X_decoded, x, logits, Y, feature_dist, prot_dist)\n",
    "\n",
    "        # backpropagate over the loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute and save accuracy and loss\n",
    "        train_accuracy = compute_acc(logits, Y)\n",
    "        train_accs.append(train_accuracy)\n",
    "        train_losses.append(total_loss.item())\n",
    "    \n",
    "    # encode one training example to check:\n",
    "    plt.imshow(x_plot)\n",
    "    plt.show()\n",
    "    plt.imshow(X_decoded[0].detach().view(28, 28, 3))\n",
    "    plt.show()\n",
    "\n",
    "    # print information after a batch\n",
    "    print('Last train loss of batch:', total_loss.item())\n",
    "    print('Train acc on batch:', np.mean(train_accs[-step:]))\n",
    "    print(\"Last train acc\", train_accuracy)\n",
    "\n",
    "\n",
    "    if epoch % test_display_step == 0:\n",
    "        # save model and prototypes\n",
    "        torch.save(model, model_folder + \"/\" + model_filename + \"_epoch_\" + str(epoch) + '.pt')\n",
    "  \n",
    "        \n",
    "        # save model prototypes\n",
    "        visualize_prototypes(model, epoch, save = True)\n",
    "        print(\"Model and prototypes of epoch %d are saved\"%epoch)\n",
    "        \n",
    "        # perform testing\n",
    "        with torch.no_grad():\n",
    "            for (x_test, y_test) in test_dl:\n",
    "                x_test = x_test.view(x_test.shape[0], n_input_channel, x_test.shape[1], x_test.shape[2]).float()\n",
    "\n",
    "                #y_test = y_test.long()\n",
    "\n",
    "                # forward pass\n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_test)\n",
    "\n",
    "                # compute loss and accuracy and save\n",
    "                test_accuracy = compute_acc(logits, y_test)\n",
    "                test_loss = loss_function(X_decoded, x_test, logits, y_test, feature_dist, prot_dist)\n",
    "                test_accs.append(test_accuracy)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "            print('\\nTest loss:', test_loss.item())\n",
    "            print('Test acc:', test_accuracy)\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        for (x_valid, y_valid) in valid_dl:\n",
    "                x_valid = x_valid.view(x_valid.shape[0], n_input_channel, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "        \n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_valid)\n",
    "\n",
    "                # compute losses and accuracy and save\n",
    "                valid_accuracy = compute_acc(logits, y_valid)\n",
    "                valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist, print_flag=True)\n",
    "                valid_accs.append(valid_accuracy)\n",
    "                valid_losses.append(valid_loss)\n",
    "\n",
    "        print('\\nValid loss:', valid_loss.item())\n",
    "        print('Valid acc:', valid_accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8PVm-2Tue0L"
   },
   "source": [
    "## Loading the model and visualize prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9NsktZFue0L"
   },
   "outputs": [],
   "source": [
    "# # load the model\n",
    "# loaded_model = torch.load(model_folder+\"/\"+model_filename)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#         for step, (x_valid, y_valid) in enumerate(valid_dl):\n",
    "#                 x_valid = x_valid.view(x_valid.shape[0], 1, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "#                 X_decoded, logits, feature_dist, prot_dist = loaded_model(x_valid)\n",
    "\n",
    "#                 # Check is model is indeed trained\n",
    "#                 valid_accuracy = compute_acc(logits, y_valid)\n",
    "#                 valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist)\n",
    "\n",
    "#         print('\\nValid loss:', valid_loss.item())\n",
    "#         print('Valid acc:', valid_accuracy)\n",
    "\n",
    "# visualize_prototypes(loaded_model, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lR86Fwbue0M"
   },
   "outputs": [],
   "source": [
    "# visualize prototypes from current model (in memory, not the loaded one)\n",
    "visualize_prototypes(model, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsKJCqlJue0N"
   },
   "source": [
    "## Saving and plotting of losses and accuracies\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTsODvIyue0N"
   },
   "outputs": [],
   "source": [
    "with open(model_folder + '/train_accs.p', 'wb') as f:\n",
    "    pickle.dump(train_accs, f)\n",
    "\n",
    "with open(model_folder + '/test_accs.p', 'wb') as f:\n",
    "    pickle.dump(test_accs, f)\n",
    "\n",
    "with open(model_folder + '/valid_accs.p', 'wb') as f:\n",
    "    pickle.dump(valid_accs, f)\n",
    "\n",
    "with open(model_folder + '/train_losses.p', 'wb') as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "\n",
    "with open(model_folder + '/test_losses.p', 'wb') as f:\n",
    "    pickle.dump(test_losses, f)\n",
    "\n",
    "with open(model_folder + '/valid_losses.p', 'wb') as f:\n",
    "    pickle.dump(valid_losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MtynokTLue0O",
    "outputId": "7d9840ec-741c-488a-c12e-959eb3a956a2"
   },
   "outputs": [],
   "source": [
    "v_t_epochs = list(range(0,len(train_accs)))\n",
    "test_epochs = list(range(0,len(train_accs),test_display_step))\n",
    "print(len(train_accs))\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(v_t_epochs, train_accs, label=\"Training accuracy\")\n",
    "plt.plot(v_t_epochs, valid_accs, label=\"Valid accuracy\")\n",
    "plt.plot(test_epochs, test_accs, label=\"Test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(v_t_epochs, valid_losses, label=\"Valid loss\")\n",
    "plt.plot(v_t_epochs, train_losses, label=\"Training loss\")\n",
    "plt.plot(test_epochs, test_losses, label=\"Test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-NqDa4Yue0P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
