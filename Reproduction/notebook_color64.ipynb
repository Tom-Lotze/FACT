{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6_cRP6zuezu"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quJJ7K08uezx"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from math import ceil\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import pickle \n",
    "import scipy.ndimage\n",
    "from PIL import Image as PILImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFQfVFW3uhEb"
   },
   "source": [
    "## Mount drive if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PA2nacHdugNh",
    "outputId": "55778d93-31e7-4c65-e1a8-6fa6329f8ed7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('gdrive/')\n",
    "# os.chdir('gdrive/My Drive/Colab Notebooks/FACT/')\n",
    "# '/gdrive/My Drive/Colab Notebooks/NLP1/Practical 2/googlenews.word2vec.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqtCu9riuez3"
   },
   "source": [
    "## Helper functions\n",
    "Helper functions borrowed from original paper by Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VC4XHCWOuez4"
   },
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return torch.sum(torch.pow(X, 2), dim=1)\n",
    "\n",
    "def print_and_write(str, file):\n",
    "    '''\n",
    "    print str to the console and also write it to file\n",
    "    '''\n",
    "    print(str)\n",
    "    file.write(str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WB1Z5hI9u-6d",
    "outputId": "21b78f5d-3657-444c-b684-4be27e760641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/TomLotze/Documents/Artificial Intelligence/Year 1/FACT/FACT/Reproduction'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoYPY1rxuez8"
   },
   "source": [
    "## Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJeuIBHkuez9"
   },
   "outputs": [],
   "source": [
    "# data folder\n",
    "makedirs('./data/mnist_color')\n",
    "\n",
    "# Models folder\n",
    "model_folder = os.path.join(os.getcwd(), \"saved_model\", \"mnist_model_color\")\n",
    "makedirs(model_folder)\n",
    "\n",
    "# Image folder\n",
    "img_folder = os.path.join(model_folder, \"img\")\n",
    "makedirs(img_folder)\n",
    "\n",
    "# Model filename\n",
    "model_filename = \"mnist_cae_color\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UadfMKpNuez_"
   },
   "source": [
    "## Dataset - Pytorch\n",
    "#### <font color='red'>Double check the normalization mean and stdev for dataset</font>\n",
    "#### <font color='red'>Double check parameters Dataloader (e.g. shuffle on or off, different batch sizes for train/valid/test)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "b5lz09eouez_",
    "outputId": "51f24ea3-0e76-4ec2-8a31-b280ac901b76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.5%5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Transforms to perform on loaded dataset. Normalize around mean 0.1307 and std 0.3081 for optimal pytorch results. \n",
    "# source: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "# Load datasets into reproduction/data/mnist. Download if data not present. \n",
    "mnist_train = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms))\n",
    "\n",
    "mnist_train_data = mnist_train.dataset.data\n",
    "mnist_train_targets = mnist_train.dataset.targets\n",
    "\n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train_data[0:55000]\n",
    "y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train_data[55000:60000]\n",
    "y_valid = mnist_train_targets[55000:60000]\n",
    "\n",
    "# 10000 examples in test set\n",
    "mnist_test = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=False, download=True, \n",
    "                                                   transform=transforms))\n",
    "\n",
    "x_test = mnist_test.dataset.data\n",
    "y_test = mnist_test.dataset.targets\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D0eBB7Yue0B"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fytgYN8Zue0B"
   },
   "outputs": [],
   "source": [
    "# COPIED FROM THE ORIGINAL IMPLEMENTATION\n",
    "# training parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1500\n",
    "\n",
    "# frequency of testing and saving\n",
    "test_display_step = 5    # how many epochs we do evaluate on the test set once, default 100\n",
    "save_step = 50            # how frequently do we save the model to disk\n",
    "\n",
    "# elastic deformation parameters\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "\n",
    "# lambda's are the ratios between the four error terms\n",
    "lambda_class = 20\n",
    "lambda_ae = 1 # autoencoder\n",
    "lambda_1 = 1 # push prototype vectors to have meaningful decodings in pixel space\n",
    "lambda_2 = 1 # cluster training examples around prototypes in latent space\n",
    "\n",
    "\n",
    "input_height = input_width =  28    # MNIST data input shape \n",
    "n_input_channel = 1     # the number of color channels; for MNIST is 1.\n",
    "input_size = input_height * input_width * n_input_channel   # 784\n",
    "n_classes = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_prototypes = 15         # the number of prototypes\n",
    "n_layers = 4\n",
    "\n",
    "# height and width of each layers' filters\n",
    "f_1 = 3\n",
    "f_2 = 3\n",
    "f_3 = 3\n",
    "f_4 = 3\n",
    "\n",
    "# stride size in each direction for each of the layers\n",
    "s_1 = 2\n",
    "s_2 = 2\n",
    "s_3 = 2\n",
    "s_4 = 2\n",
    "\n",
    "# number of feature maps in each layer\n",
    "n_map_1 = 32\n",
    "n_map_2 = 32\n",
    "n_map_3 = 32\n",
    "n_map_4 = 10\n",
    "\n",
    "# the shapes of each layer's filter\n",
    "# [out channel, in_channel, 3, 3]\n",
    "filter_shape_1 = [n_map_1, n_input_channel, f_1, f_1]\n",
    "filter_shape_2 = [n_map_2, n_map_1, f_2, f_2]\n",
    "filter_shape_3 = [n_map_3, n_map_2, f_3, f_3]\n",
    "filter_shape_4 = [n_map_4, n_map_3, f_4, f_4]\n",
    "\n",
    "# strides for each layer (changed to tuples)\n",
    "stride_1 = [s_1, s_1]\n",
    "stride_2 = [s_2, s_2]\n",
    "stride_3 = [s_3, s_3]\n",
    "stride_4 = [s_4, s_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "077FcrZkue0D"
   },
   "source": [
    "## Model construction\n",
    "#### <font color='red'>Fix the stride and padding parameters, check if filter in tf is same as weight in pt</font>\n",
    "Padding discussion pytorch: https://github.com/pytorch/pytorch/issues/3867\n",
    "\n",
    "Blogpost: https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pfh3k6UDue0D"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder'''\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "        \n",
    "        # define layers\n",
    "        self.enc_l1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l4 = nn.Conv2d(32, 10, kernel_size=3, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def pad_image(self, img):\n",
    "        ''' Takes an input image (batch) and pads according to Tensorflows SAME padding'''\n",
    "        input_h = img.shape[2]\n",
    "        input_w = img.shape[3]\n",
    "        stride = 2 \n",
    "        filter_h = 3\n",
    "        filter_w = 3\n",
    "\n",
    "        output_h = int(ceil(float(input_h)) / float(stride))\n",
    "        output_w = output_h\n",
    "\n",
    "        if input_h % stride == 0:\n",
    "            pad_height = max((filter_h - stride), 0)\n",
    "        else:\n",
    "            pad_height = max((filter_h - (input_h % stride), 0))\n",
    "\n",
    "        pad_width = pad_height\n",
    "\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        padded_img = torch.zeros(img.shape[0], img.shape[1], input_h + pad_height, input_w + pad_width)\n",
    "        padded_img[:,:, pad_top:-pad_bottom, pad_left:-pad_right] = img\n",
    "\n",
    "        return padded_img\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pad_x = self.pad_image(x)\n",
    "        el1 = self.relu(self.enc_l1(pad_x))\n",
    "        \n",
    "        pad_el1 = self.pad_image(el1)\n",
    "        el2 = self.relu(self.enc_l2(pad_el1))\n",
    "    \n",
    "        pad_el2 = self.pad_image(el2)\n",
    "        el3 = self.relu(self.enc_l3(pad_el2))\n",
    "        \n",
    "        pad_el3 = self.pad_image(el3)\n",
    "        el4 = self.relu(self.enc_l4(pad_el3))\n",
    "        \n",
    "        return el4\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder'''\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "\n",
    "        # define layers\n",
    "        self.dec_l4 = nn.ConvTranspose2d(10, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l3 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l1 = nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        dl4 = self.relu(self.dec_l4(enc_x))\n",
    "        dl3 = self.relu(self.dec_l3(dl4))\n",
    "        dl2 = self.relu(self.dec_l2(dl3))\n",
    "        decoded_x = self.sigmoid(self.dec_l1(dl2))\n",
    "        \n",
    "        return decoded_x\n",
    "\n",
    "\n",
    "class nn_prototype(nn.Module):\n",
    "    '''Model'''\n",
    "    def __init__(self, n_prototypes=15, n_layers=4, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        # initialize prototype - currently not in correct spot\n",
    "        \n",
    "        # changed this for the colored mnist, from 40 to 160, the new shape would be 250*10*4*4\n",
    "        n_features = 160 # size of encoded x - 250 x 10 x 2 x 2\n",
    "        self.prototype_feature_vectors = nn.Parameter(torch.empty(size=(n_prototypes, n_features), \n",
    "                                                                  dtype=torch.float32).uniform_())\n",
    "        \n",
    "        self.last_layer = nn.Linear(n_prototypes,10)\n",
    "        \n",
    "    def list_of_distances(self, X, Y):\n",
    "        '''\n",
    "        Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "        Y = [y_1, ... , y_m], we return a list of vectors\n",
    "                [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "                 ...\n",
    "                 [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "        where the distance metric used is the sqared euclidean distance.\n",
    "        The computation is achieved through a clever use of broadcasting.\n",
    "        '''\n",
    "        XX = torch.reshape(self.list_of_norms(X), shape=(-1, 1))\n",
    "        YY = torch.reshape(self.list_of_norms(Y), shape=(1, -1))\n",
    "        output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def list_of_norms(self, X):\n",
    "        '''\n",
    "        X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "            [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "        function is the squared euclidean distance.\n",
    "        '''\n",
    "        return torch.sum(torch.pow(X, 2), dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"Shape of input x\", x.shape)\n",
    "        \n",
    "        #encoder step\n",
    "        enc_x = self.encoder(x)\n",
    "        \n",
    "        #print(\"Shape of encoded x\", enc_x.shape)\n",
    "        \n",
    "        #decoder step\n",
    "        dec_x = self.decoder(enc_x)\n",
    "        \n",
    "        #print(\"shape of decoded x\", dec_x.shape)\n",
    "        \n",
    "        # hardcoded input size (not needed, shape already correct)\n",
    "        # dec_x = dec_x.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        \n",
    "        # flatten encoded x to compute distance with prototypes\n",
    "        n_features = enc_x.shape[1] * enc_x.shape[2] * enc_x.shape[3]\n",
    "        feature_vectors_flat = torch.reshape(enc_x, shape=[-1, n_features])\n",
    "        \n",
    "        #print(\"Shape of flattened feature vectors\", feature_vectors_flat.shape)\n",
    "        \n",
    "        # distance to prototype\n",
    "        prototype_distances = self.list_of_distances(feature_vectors_flat, self.prototype_feature_vectors)\n",
    "        \n",
    "        # distance to feature vectors\n",
    "        feature_vector_distances = self.list_of_distances(self.prototype_feature_vectors, feature_vectors_flat)\n",
    "        \n",
    "        # classification layer\n",
    "        logits = self.last_layer(prototype_distances)\n",
    "        \n",
    "        # Softmax to prob dist not needed as cross entropy loss is used?\n",
    "        \n",
    "        return dec_x, logits, feature_vector_distances, prototype_distances\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebySJKDyue0E"
   },
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCRlYzWJue0E"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "the error function consists of 4 terms, the autoencoder loss,\n",
    "the classification loss, and the two requirements that every feature vector in\n",
    "X look like at least one of the prototype feature vectors and every prototype\n",
    "feature vector look like at least one of the feature vectors in X.\n",
    "'''\n",
    "def loss_function(X_decoded, X_true, logits, Y, feature_dist, prototype_dist, lambdas=None, print_flag=False):\n",
    "    if lambdas == None:\n",
    "        lambda_class, lambda_ae, lambda_1, lambda_2 = 20, 1, 1, 1\n",
    "    \n",
    "    ae_error = torch.mean(list_of_norms(X_decoded - X_true))\n",
    "#     ae_error = F.binary_cross_entropy(X_decoded, X_true)\n",
    "    class_error = F.cross_entropy(logits, Y, reduction=\"mean\")\n",
    "    error_1 = torch.mean(torch.min(feature_dist, axis=1)[0])\n",
    "    error_2 = torch.mean(torch.min(prototype_dist, axis = 1)[0])\n",
    "\n",
    "    # total_error is the our minimization objective\n",
    "    total_error = lambda_class * class_error +\\\n",
    "                  lambda_ae * ae_error + \\\n",
    "                  lambda_1 * error_1 + \\\n",
    "                  lambda_2 * error_2\n",
    "    \n",
    "    if print_flag == True:\n",
    "        print('AE error: ', ae_error)\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9L6xKcLlue0G"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJPQHz5xue0G"
   },
   "outputs": [],
   "source": [
    "def compute_acc(logits, labels):\n",
    "    batch_size = labels.shape[0]\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    total_correct = torch.sum(predictions == labels).item()\n",
    "    accuracy = total_correct / batch_size\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyyShfpEue0H"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lena = PILImage.open('./resources/lena.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CBtrrsoue0I"
   },
   "outputs": [],
   "source": [
    "def visualize_prototypes(model, epoch, save=True):\n",
    "    # get saved prototypes\n",
    "    encoded_prototypes = model.prototype_feature_vectors\n",
    "    encoded_prototypes_reshaped = encoded_prototypes.view(n_prototypes, 10, 4, 4)\n",
    "\n",
    "    # decode prototypes\n",
    "    decoded_prototypes = model.decoder(encoded_prototypes_reshaped).detach().numpy()\n",
    "    \n",
    "    dec_prot = decoded_prototypes.transpose(0, 2, 3, 1)\n",
    "\n",
    "    for i in range(n_prototypes):\n",
    "        plt.imshow(dec_prot[i])\n",
    "        if save:\n",
    "            makedirs(img_folder+\"/prototypes_epoch_\"+ str(epoch))\n",
    "            plt.savefig(img_folder+\"/prototypes_epoch_\"+ str(epoch)+\"/\"+str(i)+\".png\")\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partly borrowed from https://github.com/wouterbulten/deeplearning-resources/tree/master/notebooks\n",
    "\n",
    "def batch_color(batch_raw, batch_size=250, change_colors=True):\n",
    "    \n",
    "    #print(batch_raw.shape)\n",
    "    \n",
    "    # Resize\n",
    "    batch_resized = np.asarray([scipy.ndimage.zoom(image, (2.3, 2.3, 1), order=1) for image in batch_raw])\n",
    "    \n",
    "    #print(batch_resized.shape)\n",
    "    \n",
    "    # Extend to RGB\n",
    "    batch_rgb = np.concatenate([batch_resized, batch_resized, batch_resized], axis=3)\n",
    "    \n",
    "    #print(batch_rgb.shape)\n",
    "    \n",
    "    # Make binary\n",
    "    batch_binary = (batch_rgb > 0.5)\n",
    "    \n",
    "    batch = np.zeros((batch_size, 64, 64, 3))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Take a random crop of the Lena image (background)\n",
    "        x_c = np.random.randint(0, lena.size[0] - 64)\n",
    "        y_c = np.random.randint(0, lena.size[1] - 64)\n",
    "        image = lena.crop((x_c, y_c, x_c + 64, y_c + 64))\n",
    "        image = np.asarray(image) / 255.0\n",
    "\n",
    "        if change_colors:\n",
    "            # Change color distribution\n",
    "            for j in range(3):\n",
    "                image[:, :, j] = (image[:, :, j] + np.random.uniform(0, 1)) / 2.0\n",
    "\n",
    "        # Invert the colors at the location of the number\n",
    "        image[batch_binary[i]] = 1 - image[batch_binary[i]]\n",
    "        \n",
    "        batch[i] = image\n",
    "    \n",
    "    # Map the whole batch to [-1, 1]\n",
    "    #batch = batch / 0.5 - 1\n",
    "\n",
    "    return torch.from_numpy(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "am9_dX2eue0J",
    "outputId": "e3fd512a-e0d3-48df-ddc2-ad1f1bfe0cfe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Last train loss of batch: 57.05466842651367\n",
      "Train acc on batch: 0.12195433789954341\n",
      "Last train acc 0.14\n",
      "model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type nn_prototype. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 56.66398620605469\n",
      "Test acc: 0.1495\n",
      "\n",
      "Valid loss: 56.74692916870117\n",
      "Valid acc: 0.1474\n",
      "\n",
      "Epoch: 1\n",
      "Last train loss of batch: 40.73712158203125\n",
      "Train acc on batch: 0.3776986301369863\n",
      "Last train acc 0.484\n",
      "\n",
      "Valid loss: 38.505645751953125\n",
      "Valid acc: 0.586\n",
      "\n",
      "Epoch: 2\n",
      "Last train loss of batch: 30.61501693725586\n",
      "Train acc on batch: 0.6504657534246575\n",
      "Last train acc 0.72\n",
      "\n",
      "Valid loss: 27.990093231201172\n",
      "Valid acc: 0.751\n",
      "\n",
      "Epoch: 3\n",
      "Last train loss of batch: 25.37320327758789\n",
      "Train acc on batch: 0.7363470319634704\n",
      "Last train acc 0.772\n",
      "\n",
      "Valid loss: 23.179344177246094\n",
      "Valid acc: 0.7904\n",
      "\n",
      "Epoch: 4\n",
      "Last train loss of batch: 23.11391258239746\n",
      "Train acc on batch: 0.7686575342465753\n",
      "Last train acc 0.74\n",
      "\n",
      "Valid loss: 20.47463035583496\n",
      "Valid acc: 0.8124\n",
      "\n",
      "Epoch: 5\n",
      "Last train loss of batch: 20.5557804107666\n",
      "Train acc on batch: 0.7891506849315069\n",
      "Last train acc 0.808\n",
      "model is saved\n",
      "\n",
      "Test loss: 19.21782875061035\n",
      "Test acc: 0.8062\n",
      "\n",
      "Valid loss: 18.401151657104492\n",
      "Valid acc: 0.8308\n",
      "\n",
      "Epoch: 6\n",
      "Last train loss of batch: 19.781532287597656\n",
      "Train acc on batch: 0.8044018264840183\n",
      "Last train acc 0.78\n",
      "\n",
      "Valid loss: 17.22782325744629\n",
      "Valid acc: 0.8326\n",
      "\n",
      "Epoch: 7\n",
      "Last train loss of batch: 17.74722671508789\n",
      "Train acc on batch: 0.8185205479452056\n",
      "Last train acc 0.828\n",
      "\n",
      "Valid loss: 15.6006498336792\n",
      "Valid acc: 0.8526\n",
      "\n",
      "Epoch: 8\n",
      "Last train loss of batch: 16.34294891357422\n",
      "Train acc on batch: 0.8285662100456622\n",
      "Last train acc 0.808\n",
      "\n",
      "Valid loss: 14.836881637573242\n",
      "Valid acc: 0.8594\n",
      "\n",
      "Epoch: 9\n",
      "Last train loss of batch: 17.252197265625\n",
      "Train acc on batch: 0.837771689497717\n",
      "Last train acc 0.824\n",
      "\n",
      "Valid loss: 14.007776260375977\n",
      "Valid acc: 0.8674\n",
      "\n",
      "Epoch: 10\n",
      "Last train loss of batch: 15.116153717041016\n",
      "Train acc on batch: 0.8461004566210045\n",
      "Last train acc 0.84\n",
      "model is saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-d77bb716f0fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mX_decoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprot_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;31m# compute loss and accuracy and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-307381abe1bb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m#encoder step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0menc_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m#print(\"Shape of encoded x\", enc_x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-307381abe1bb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mpad_el1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_el1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mpad_el2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn_prototype(15,4,10)\n",
    "batch_size_ = 250\n",
    "\n",
    "# get validation and test set\n",
    "valid_dl = DataLoader(valid_data, batch_size=5000, drop_last=False, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=10000, drop_last=False, shuffle=False)\n",
    "\n",
    "# recolor validation set\n",
    "for x_valid, y_valid in valid_dl:\n",
    "    x_valid = x_valid.view(x_valid.shape[0], x_valid.shape[1], x_valid.shape[2], 1).float() / 255\n",
    "    x_valid = batch_color(x_valid, batch_size=5000)\n",
    "    x_valid = x_valid.view(x_valid.shape[0], 3, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "    y_valid = y_valid.long()\n",
    "    \n",
    "# recolor test set\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# initialize storage for results\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "valid_accs = []\n",
    "valid_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(training_epochs):\n",
    "    print(\"\\nEpoch:\", epoch)\n",
    "\n",
    "    # load the training data and reshuffle\n",
    "    train_dl = DataLoader(train_data, batch_size=batch_size_, drop_last=False, shuffle=True)\n",
    "\n",
    "    # loop over the batches\n",
    "    for step, (x, Y) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        # process the batch\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2], 1).float() / 255\n",
    "        x = batch_color(x)\n",
    "        \n",
    "#         plt.imshow(x[1])\n",
    "#         plt.show()\n",
    "        \n",
    "        x = x.view(x.shape[0], 3, x.shape[1], x.shape[2]).float()        \n",
    "\n",
    "        Y = Y.long()\n",
    "        \n",
    "        # perform forward pass\n",
    "        X_decoded, logits, feature_dist, prot_dist = model(x)\n",
    "\n",
    "        # compute the loss\n",
    "        total_loss = loss_function(X_decoded, x, logits, Y, feature_dist, prot_dist)\n",
    "\n",
    "        # backpropagate over the loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute and save accuracy and loss\n",
    "        train_accuracy = compute_acc(logits, Y)\n",
    "        train_accs.append(train_accuracy)\n",
    "        train_losses.append(total_loss.item())\n",
    "\n",
    "    # print information after a batch\n",
    "    print('Last train loss of batch:', total_loss.item())\n",
    "    print('Train acc on batch:', np.mean(train_accs[-step:]))\n",
    "    print(\"Last train acc\", train_accuracy)\n",
    "\n",
    "\n",
    "    if epoch % test_display_step == 0:\n",
    "        # save model and prototypes\n",
    "        torch.save(model, model_folder + \"/\" + model_filename + \"_epoch_\" + str(epoch) + '.pt')\n",
    "        print(\"model is saved\")\n",
    "        \n",
    "        # save model prototypes\n",
    "        visualize_prototypes(model, epoch, save = True)\n",
    "        \n",
    "        # perform testing\n",
    "        with torch.no_grad():\n",
    "            for step, (x_test, y_test) in enumerate(test_dl):\n",
    "                x_test = x_test.view(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1).float() / 255\n",
    "                x_test = batch_color(x_test, batch_size=10000)\n",
    "                x_test = x_test.view(x_test.shape[0], 3, x_test.shape[1], x_test.shape[2]).float()\n",
    "                \n",
    "                y_test = y_test.long()\n",
    "\n",
    "                # forward pass\n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_test)\n",
    "\n",
    "                # compute loss and accuracy and save\n",
    "                test_accuracy = compute_acc(logits, y_test)\n",
    "                test_loss = loss_function(X_decoded, x_test, logits, y_test, feature_dist, prot_dist)\n",
    "                test_accs.append(test_accuracy)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "            print('\\nTest loss:', test_loss.item())\n",
    "            print('Test acc:', test_accuracy)\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        for step, (x_valid, y_valid) in enumerate(valid_dl):\n",
    "                x_valid = x_valid.view(x_valid.shape[0], x_valid.shape[1], x_valid.shape[2], 1).float() / 255\n",
    "                \n",
    "                x_valid = batch_color(x_valid, batch_size=5000)\n",
    "                x_valid = x_valid.view(x_valid.shape[0], 3, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "                \n",
    "                y_valid = y_valid.long()\n",
    "                \n",
    "                \n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_valid)\n",
    "\n",
    "                # compute losses and accuracy and save\n",
    "                valid_accuracy = compute_acc(logits, y_valid)\n",
    "                valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist)\n",
    "                valid_accs.append(valid_accuracy)\n",
    "                valid_losses.append(valid_loss)\n",
    "\n",
    "        print('\\nValid loss:', valid_loss.item())\n",
    "        print('Valid acc:', valid_accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8PVm-2Tue0L"
   },
   "source": [
    "## Loading the model and visualize prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9NsktZFue0L"
   },
   "outputs": [],
   "source": [
    "# # load the model\n",
    "# loaded_model = torch.load(model_folder+\"/\"+model_filename)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#         for step, (x_valid, y_valid) in enumerate(valid_dl):\n",
    "#                 x_valid = x_valid.view(x_valid.shape[0], 1, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "#                 X_decoded, logits, feature_dist, prot_dist = loaded_model(x_valid)\n",
    "\n",
    "#                 # Check is model is indeed trained\n",
    "#                 valid_accuracy = compute_acc(logits, y_valid)\n",
    "#                 valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist)\n",
    "\n",
    "#         print('\\nValid loss:', valid_loss.item())\n",
    "#         print('Valid acc:', valid_accuracy)\n",
    "\n",
    "# visualize_prototypes(loaded_model, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lR86Fwbue0M"
   },
   "outputs": [],
   "source": [
    "# visualize prototypes from current model (in memory, not the loaded one)\n",
    "visualize_prototypes(model, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsKJCqlJue0N"
   },
   "source": [
    "## Saving and plotting of losses and accuracies\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTsODvIyue0N"
   },
   "outputs": [],
   "source": [
    "with open(model_folder + '/train_accs.p', 'wb') as f:\n",
    "    pickle.dump(train_accs, f)\n",
    "\n",
    "with open(model_folder + '/test_accs.p', 'wb') as f:\n",
    "    pickle.dump(test_accs, f)\n",
    "\n",
    "with open(model_folder + '/valid_accs.p', 'wb') as f:\n",
    "    pickle.dump(valid_accs, f)\n",
    "\n",
    "with open(model_folder + '/train_losses.p', 'wb') as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "\n",
    "with open(model_folder + '/test_losses.p', 'wb') as f:\n",
    "    pickle.dump(test_losses, f)\n",
    "\n",
    "with open(model_folder + '/valid_losses.p', 'wb') as f:\n",
    "    pickle.dump(valid_losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MtynokTLue0O",
    "outputId": "7d9840ec-741c-488a-c12e-959eb3a956a2"
   },
   "outputs": [],
   "source": [
    "v_t_epochs = list(range(0,len(train_accs)))\n",
    "test_epochs = list(range(0,len(train_accs),test_display_step))\n",
    "print(len(train_accs))\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(v_t_epochs, train_accs, label=\"Training accuracy\")\n",
    "plt.plot(v_t_epochs, valid_accs, label=\"Valid accuracy\")\n",
    "plt.plot(test_epochs, test_accs, label=\"Test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(v_t_epochs, valid_losses, label=\"Valid loss\")\n",
    "plt.plot(v_t_epochs, train_losses, label=\"Training loss\")\n",
    "plt.plot(test_epochs, test_losses, label=\"Test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-NqDa4Yue0P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
