{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Helper functions borrowed from original paper by Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def list_of_distances(X, Y):\n",
    "    '''\n",
    "    Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "    Y = [y_1, ... , y_m], we return a list of vectors\n",
    "            [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "             ...\n",
    "             [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "    where the distance metric used is the sqared euclidean distance.\n",
    "    The computation is achieved through a clever use of broadcasting.\n",
    "    '''\n",
    "    XX = torch.reshape(list_of_norms(X), shape=(-1, 1))\n",
    "    YY = torch.reshape(list_of_norms(Y), shape=(1, -1))\n",
    "    output = XX + YY - 2 * torch.mm(X, torch.transpose(Y))\n",
    "\n",
    "    return output\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return tf.reduce_sum(np.pow(X, 2), axis=1)\n",
    "\n",
    "def print_and_write(str, file):\n",
    "    '''\n",
    "    print str to the console and also write it to file\n",
    "    '''\n",
    "    print(str)\n",
    "    file.write(str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "makedirs('./data/mnist')\n",
    "\n",
    "# Models folder\n",
    "model_folder = os.path.join(os.getcwd(), \"saved_model\", \"mnist_model\", \"mnist_cae_1\")\n",
    "makedirs(model_folder)\n",
    "\n",
    "# Image folder\n",
    "img_folder = os.path.join(model_folder, \"img\")\n",
    "makedirs(img_folder)\n",
    "\n",
    "# Model filename\n",
    "model_filename = \"mnist_cae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Pytorch\n",
    "#### <font color='red'>Double check the normalization mean and stdev for dataset</font>\n",
    "#### <font color='red'>Double check parameters Dataloader (e.g. shuffle on or off, different batch sizes for train/valid/test)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms to perform on loaded dataset. Normalize around mean 0.1307 and std 0.3081 for optimal pytorch results. \n",
    "# source: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "# Load datasets into reproduction/data/mnist. Download if data not present. \n",
    "mnist_train = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms))\n",
    "\n",
    "mnist_train_data = mnist_train.dataset.data\n",
    "mnist_train_targets = mnist_train.dataset.targets\n",
    "\n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train_data[0:55000]\n",
    "y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train_data[55000:60000]\n",
    "y_valid = mnist_train_targets[55000:60000]\n",
    "\n",
    "# 10000 examples in test set\n",
    "\n",
    "mnist_test = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=False, download=True, transform=transforms))\n",
    "\n",
    "x_test = mnist_test.dataset.data\n",
    "y_test = mnist_test.dataset.targets\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "# Datasets in DataLoader, can be used in iteration: for (x, y) in train_dl...\n",
    "# Check parameters\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "valid_dl = DataLoader(valid_data, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM THE ORIGINAL IMPLEMENTATION\n",
    "# training parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1500\n",
    "\n",
    "# frequency of testing and saving\n",
    "test_display_step = 100   # how many epochs we do evaluate on the test set once\n",
    "save_step = 50            # how frequently do we save the model to disk\n",
    "\n",
    "# elastic deformation parameters\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "\n",
    "# lambda's are the ratios between the four error terms\n",
    "lambda_class = 20\n",
    "lambda_ae = 1 # autoencoder\n",
    "lambda_1 = 1 # push prototype vectors to have meaningful decodings in pixel space\n",
    "lambda_2 = 1 # cluster training examples around prototypes in latent space\n",
    "\n",
    "\n",
    "input_height = input_width =  28    # MNIST data input shape \n",
    "n_input_channel = 1     # the number of color channels; for MNIST is 1.\n",
    "input_size = input_height * input_width * n_input_channel   # 784\n",
    "n_classes = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_prototypes = 15         # the number of prototypes\n",
    "n_layers = 4\n",
    "\n",
    "# height and width of each layers' filters\n",
    "f_1 = 3\n",
    "f_2 = 3\n",
    "f_3 = 3\n",
    "f_4 = 3\n",
    "\n",
    "# stride size in each direction for each of the layers\n",
    "s_1 = 2\n",
    "s_2 = 2\n",
    "s_3 = 2\n",
    "s_4 = 2\n",
    "\n",
    "# number of feature maps in each layer\n",
    "n_map_1 = 32\n",
    "n_map_2 = 32\n",
    "n_map_3 = 32\n",
    "n_map_4 = 10\n",
    "\n",
    "# the shapes of each layer's filter\n",
    "# [out channel, in_channel, 3, 3]\n",
    "filter_shape_1 = [n_map_1, n_input_channel, f_1, f_1]\n",
    "filter_shape_2 = [n_map_2, n_map_1, f_2, f_2]\n",
    "filter_shape_3 = [n_map_3, n_map_2, f_3, f_3]\n",
    "filter_shape_4 = [n_map_4, n_map_3, f_4, f_4]\n",
    "\n",
    "# strides for each layer (changed to tuples)\n",
    "stride_1 = [s_1, s_1]\n",
    "stride_2 = [s_2, s_2]\n",
    "stride_3 = [s_3, s_3]\n",
    "stride_4 = [s_4, s_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_weights = 0.01\n",
    "\n",
    "weights = {\n",
    "    'enc_f1': nn.Parameter(std_weights * torch.randn(filter_shape_1,\n",
    "                                           dtype=torch.float32)),\n",
    "    'enc_f2': nn.Parameter(std_weights * torch.randn(filter_shape_2,\n",
    "                                           dtype=torch.float32)), \n",
    "    'enc_f3': nn.Parameter(std_weights * torch.randn(filter_shape_3,\n",
    "                                           dtype=torch.float32)), \n",
    "    'enc_f4': nn.Parameter(std_weights * torch.randn(filter_shape_4,\n",
    "                                           dtype=torch.float32)), \n",
    "    'dec_f4': nn.Parameter(std_weights * torch.randn(filter_shape_4,\n",
    "                                           dtype=torch.float32)), \n",
    "    'dec_f3': nn.Parameter(std_weights * torch.randn(filter_shape_3,\n",
    "                                           dtype=torch.float32)), \n",
    "    'dec_f2': nn.Parameter(std_weights * torch.randn(filter_shape_2,\n",
    "                                           dtype=torch.float32)),\n",
    "    'dec_f1': nn.Parameter(std_weights * torch.randn(filter_shape_1,\n",
    "                                           dtype=torch.float32)),\n",
    "}\n",
    "\n",
    "\n",
    "biases = {\n",
    "    'enc_b1': nn.Parameter(torch.zeros([n_map_1], dtype=torch.float32)),\n",
    "    'enc_b2': nn.Parameter(torch.zeros([n_map_2], dtype=torch.float32)),\n",
    "    'enc_b3': nn.Parameter(torch.zeros([n_map_3], dtype=torch.float32)),\n",
    "    'enc_b4': nn.Parameter(torch.zeros([n_map_4], dtype=torch.float32)),\n",
    "    'dec_b4': nn.Parameter(torch.zeros([n_map_3], dtype=torch.float32)),\n",
    "    'dec_b3': nn.Parameter(torch.zeros([n_map_2], dtype=torch.float32)),\n",
    "    'dec_b2': nn.Parameter(torch.zeros([n_map_1], dtype=torch.float32)),\n",
    "    'dec_b1': nn.Parameter(torch.zeros([n_input_channel], dtype=torch.float32)),\n",
    "}\n",
    "\n",
    "last_layer = {\n",
    "    'w': nn.Parameter(torch.randn([n_prototypes, n_classes],\n",
    "                                       dtype=torch.float32))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print shapes of all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "enc_f1 torch.Size([32, 1, 3, 3])\n",
      "enc_f2 torch.Size([32, 32, 3, 3])\n",
      "enc_f3 torch.Size([32, 32, 3, 3])\n",
      "enc_f4 torch.Size([10, 32, 3, 3])\n",
      "dec_f4 torch.Size([10, 32, 3, 3])\n",
      "dec_f3 torch.Size([32, 32, 3, 3])\n",
      "dec_f2 torch.Size([32, 32, 3, 3])\n",
      "dec_f1 torch.Size([32, 1, 3, 3])\n",
      "biases\n",
      "enc_b1 torch.Size([32])\n",
      "enc_b2 torch.Size([32])\n",
      "enc_b3 torch.Size([32])\n",
      "enc_b4 torch.Size([10])\n",
      "dec_b4 torch.Size([32])\n",
      "dec_b3 torch.Size([32])\n",
      "dec_b2 torch.Size([32])\n",
      "dec_b1 torch.Size([1])\n",
      "last_layer\n",
      "torch.Size([15, 10])\n"
     ]
    }
   ],
   "source": [
    "# Printing shapes of all parameters\n",
    "print(\"weights\")\n",
    "for weight in weights.keys():\n",
    "    print(weight, weights[weight].shape)\n",
    "print(\"biases\")\n",
    "for b in biases.keys():\n",
    "    print(b, biases[b].shape)\n",
    "print(\"last_layer\")\n",
    "print(last_layer['w'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input, filter, bias, strides, padding=\"VALID\",\n",
    "               nonlinearity = nn.ReLU()):\n",
    "    conv = F.conv2d(input, filter, bias=bias, stride=strides,\n",
    "       padding=padding)\n",
    "    out = nonlinearity(conv)\n",
    "    return out\n",
    "#### STRIDE MUST BE TUPLE FOR TORCH, IS A LIST IN TENSORFLOW\n",
    "#### PADDING IS DIFFERENT, TF USES SAME/VALID, TORCH A INT OR LIST OF INTS\n",
    "### IS THE FILTER THE SAME AS WEIGHTS ARGUMENT FOR THE CONV2D?\n",
    "\n",
    "# tensorflow's conv2d_transpose needs to know the shape of the output\n",
    "def deconv_layer(input, filter, bias, strides, padding=\"VALID\",\n",
    "                 nonlinearity=nn.ReLU()):\n",
    "    deconv = F.conv_transpose2d(input, filter, bias=bias, stride=strides,\n",
    "                                padding=padding)\n",
    "    out = nonlinearity(deconv)\n",
    "    return out\n",
    "\n",
    "def fc_layer(input, weight, bias, nonlinearity=nn.ReLU()):\n",
    "    return nonlinearity(torch.mm(input, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.empty(batch_size, n_input_channel, input_width, input_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_FLAG = 1\n",
    "# eln means the output of the nth layer of the encoder\n",
    "el1 = conv_layer(X, weights['enc_f1'], biases['enc_b1'], stride_1, PADDING_FLAG)\n",
    "el2 = conv_layer(el1, weights['enc_f2'], biases['enc_b2'], stride_2, PADDING_FLAG)\n",
    "el3 = conv_layer(el2, weights['enc_f3'], biases['enc_b3'], stride_3, PADDING_FLAG)\n",
    "el4 = conv_layer(el3, weights['enc_f4'], biases['enc_b4'], stride_4, PADDING_FLAG)\n",
    "\n",
    "\n",
    "l4_shape = el4.shape\n",
    "#print(\"l4_shape\", l4_shape)\n",
    "\n",
    "flatten_size = l4_shape[1] * l4_shape[2] * l4_shape[3]\n",
    "n_features = flatten_size\n",
    "\n",
    "# feature vectors is the flattened output of the encoder\n",
    "feature_vectors = torch.reshape(el4, shape=[-1, flatten_size])\n",
    "\n",
    "# initialize the prototype feature vectors\n",
    "prototype_feature_vectors = nn.Parameter(torch.empty(size=\n",
    "                                        [n_prototypes, n_features],\n",
    "                                        dtype=torch.float32).uniform_())\n",
    "\n",
    "#print(prototype_feature_vectors.shape)\n",
    "\n",
    "deconv_batch_size = torch.eye(feature_vectors.shape[0])\n",
    "\n",
    "# this is necessary for prototype images evaluation\n",
    "reshape_feature_vectors = torch.reshape(feature_vectors, shape=[-1, l4_shape[1],\n",
    "   l4_shape[2], l4_shape[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl4 = deconv_layer(reshape_feature_vectors, weights['dec_f4'], biases['dec_b4'],\n",
    "                   strides=stride_4, padding=PADDING_FLAG)\n",
    "dl3 = deconv_layer(dl4, weights['dec_f3'], biases['dec_b3'],\n",
    "                   strides=stride_3, padding=PADDING_FLAG)\n",
    "dl2 = deconv_layer(dl3, weights['dec_f2'], biases['dec_b2'],\n",
    "                   strides=stride_2, padding=PADDING_FLAG)\n",
    "dl1 = deconv_layer(dl2, weights['dec_f1'], biases['dec_b1'],\n",
    "                   strides=stride_1, padding=PADDING_FLAG,\n",
    "                   nonlinearity=nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 1, 17, 17])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 784]' is invalid for input of size 72250",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5a655379b30a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mX_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 784]' is invalid for input of size 72250"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X_decoded is the decoding of the encoded feature vectors in X;\n",
    "we reshape it to match the shape of the training input\n",
    "X_true is the correct output for the autoencoder\n",
    "'''\n",
    "print(dl1.shape)\n",
    "\n",
    "X_decoded = torch.reshape(dl1, shape=[-1, input_size])\n",
    "X_true = torch.eye(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b4b54edb26ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m '''\n\u001b[1;32m      7\u001b[0m prototype_distances = list_of_distances(feature_vectors,\n\u001b[0;32m----> 8\u001b[0;31m                                         prototype_feature_vectors)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprototype_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototype_distances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m feature_vector_distances = list_of_distances(prototype_feature_vectors,\n",
      "\u001b[0;32m<ipython-input-2-e0a689fd873b>\u001b[0m in \u001b[0;36mlist_of_distances\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mcomputation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0machieved\u001b[0m \u001b[0mthrough\u001b[0m \u001b[0ma\u001b[0m \u001b[0mclever\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbroadcasting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     '''\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mXX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mYY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mYY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e0a689fd873b>\u001b[0m in \u001b[0;36mlist_of_norms\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfunction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msquared\u001b[0m \u001b[0meuclidean\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     '''\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_and_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "prototype_distances is the list of distances from each x_i to every prototype\n",
    "in the latent space\n",
    "feature_vector_distances is the list of distances from each prototype to every x_i\n",
    "in the latent space\n",
    "'''\n",
    "prototype_distances = list_of_distances(feature_vectors,\n",
    "                                        prototype_feature_vectors)\n",
    "prototype_distances = torch.eye(prototype_distances)\n",
    "feature_vector_distances = list_of_distances(prototype_feature_vectors,\n",
    "                                             feature_vectors)\n",
    "feature_vector_distances = torch.eye(feature_vector_distances)\n",
    "\n",
    "# the logits are the weighted sum of distances from prototype_distances\n",
    "logits = torch.mm(prototype_distances, last_layer['w'])\n",
    "probability_distribution = F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
