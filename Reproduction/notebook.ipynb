{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Helper functions borrowed from original paper by Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def list_of_distances(X, Y):\n",
    "    '''\n",
    "    Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "    Y = [y_1, ... , y_m], we return a list of vectors\n",
    "            [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "             ...\n",
    "             [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "    where the distance metric used is the sqared euclidean distance.\n",
    "    The computation is achieved through a clever use of broadcasting.\n",
    "    '''\n",
    "    XX = torch.reshape(list_of_norms(X), shape=(-1, 1))\n",
    "    YY = torch.reshape(list_of_norms(Y), shape=(1, -1))\n",
    "    output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "    return output\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return torch.sum(torch.pow(X, 2), dim=1)\n",
    "\n",
    "def print_and_write(str, file):\n",
    "    '''\n",
    "    print str to the console and also write it to file\n",
    "    '''\n",
    "    print(str)\n",
    "    file.write(str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "makedirs('./data/mnist')\n",
    "\n",
    "# Models folder\n",
    "model_folder = os.path.join(os.getcwd(), \"saved_model\", \"mnist_model\", \"mnist_cae_1\")\n",
    "makedirs(model_folder)\n",
    "\n",
    "# Image folder\n",
    "img_folder = os.path.join(model_folder, \"img\")\n",
    "makedirs(img_folder)\n",
    "\n",
    "# Model filename\n",
    "model_filename = \"mnist_cae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Pytorch\n",
    "#### <font color='red'>Double check the normalization mean and stdev for dataset</font>\n",
    "#### <font color='red'>Double check parameters Dataloader (e.g. shuffle on or off, different batch sizes for train/valid/test)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms to perform on loaded dataset. Normalize around mean 0.1307 and std 0.3081 for optimal pytorch results. \n",
    "# source: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "# Load datasets into reproduction/data/mnist. Download if data not present. \n",
    "mnist_train = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms))\n",
    "\n",
    "mnist_train_data = mnist_train.dataset.data\n",
    "mnist_train_targets = mnist_train.dataset.targets\n",
    "\n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train_data[0:55000]\n",
    "y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train_data[55000:60000]\n",
    "y_valid = mnist_train_targets[55000:60000]\n",
    "\n",
    "# 10000 examples in test set\n",
    "\n",
    "mnist_test = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=False, download=True, transform=transforms))\n",
    "\n",
    "x_test = mnist_test.dataset.data\n",
    "y_test = mnist_test.dataset.targets\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "# Datasets in DataLoader, can be used in iteration: for (x, y) in train_dl...\n",
    "# Check parameters\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "valid_dl = DataLoader(valid_data, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM THE ORIGINAL IMPLEMENTATION\n",
    "# training parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1500\n",
    "\n",
    "# frequency of testing and saving\n",
    "test_display_step = 100   # how many epochs we do evaluate on the test set once\n",
    "save_step = 50            # how frequently do we save the model to disk\n",
    "\n",
    "# elastic deformation parameters\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "\n",
    "# lambda's are the ratios between the four error terms\n",
    "lambda_class = 20\n",
    "lambda_ae = 1 # autoencoder\n",
    "lambda_1 = 1 # push prototype vectors to have meaningful decodings in pixel space\n",
    "lambda_2 = 1 # cluster training examples around prototypes in latent space\n",
    "\n",
    "\n",
    "input_height = input_width =  28    # MNIST data input shape \n",
    "n_input_channel = 1     # the number of color channels; for MNIST is 1.\n",
    "input_size = input_height * input_width * n_input_channel   # 784\n",
    "n_classes = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_prototypes = 15         # the number of prototypes\n",
    "n_layers = 4\n",
    "\n",
    "# height and width of each layers' filters\n",
    "f_1 = 3\n",
    "f_2 = 3\n",
    "f_3 = 3\n",
    "f_4 = 3\n",
    "\n",
    "# stride size in each direction for each of the layers\n",
    "s_1 = 2\n",
    "s_2 = 2\n",
    "s_3 = 2\n",
    "s_4 = 2\n",
    "\n",
    "# number of feature maps in each layer\n",
    "n_map_1 = 32\n",
    "n_map_2 = 32\n",
    "n_map_3 = 32\n",
    "n_map_4 = 10\n",
    "\n",
    "# the shapes of each layer's filter\n",
    "# [out channel, in_channel, 3, 3]\n",
    "filter_shape_1 = [n_map_1, n_input_channel, f_1, f_1]\n",
    "filter_shape_2 = [n_map_2, n_map_1, f_2, f_2]\n",
    "filter_shape_3 = [n_map_3, n_map_2, f_3, f_3]\n",
    "filter_shape_4 = [n_map_4, n_map_3, f_4, f_4]\n",
    "\n",
    "# strides for each layer (changed to tuples)\n",
    "stride_1 = [s_1, s_1]\n",
    "stride_2 = [s_2, s_2]\n",
    "stride_3 = [s_3, s_3]\n",
    "stride_4 = [s_4, s_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_weights = 0.01\n",
    "\n",
    "weights = {\n",
    "    'enc_f1': nn.Parameter(std_weights * torch.randn(filter_shape_1,\n",
    "                                           dtype=torch.float32)),\n",
    "    'enc_f2': nn.Parameter(std_weights * torch.randn(filter_shape_2,\n",
    "                                           dtype=torch.float32)), \n",
    "    'enc_f3': nn.Parameter(std_weights * torch.randn(filter_shape_3,\n",
    "                                           dtype=torch.float32)), \n",
    "    'enc_f4': nn.Parameter(std_weights * torch.randn(filter_shape_4,\n",
    "                                           dtype=torch.float32)), \n",
    "    'dec_f4': nn.Parameter(std_weights * torch.randn(filter_shape_4,\n",
    "                                           dtype=torch.float32)), \n",
    "    'dec_f3': nn.Parameter(std_weights * torch.randn(filter_shape_3,\n",
    "                                           dtype=torch.float32)), \n",
    "    'dec_f2': nn.Parameter(std_weights * torch.randn(filter_shape_2,\n",
    "                                           dtype=torch.float32)),\n",
    "    'dec_f1': nn.Parameter(std_weights * torch.randn(filter_shape_1,\n",
    "                                           dtype=torch.float32)),\n",
    "}\n",
    "\n",
    "\n",
    "biases = {\n",
    "    'enc_b1': nn.Parameter(torch.zeros([n_map_1], dtype=torch.float32)),\n",
    "    'enc_b2': nn.Parameter(torch.zeros([n_map_2], dtype=torch.float32)),\n",
    "    'enc_b3': nn.Parameter(torch.zeros([n_map_3], dtype=torch.float32)),\n",
    "    'enc_b4': nn.Parameter(torch.zeros([n_map_4], dtype=torch.float32)),\n",
    "    'dec_b4': nn.Parameter(torch.zeros([n_map_3], dtype=torch.float32)),\n",
    "    'dec_b3': nn.Parameter(torch.zeros([n_map_2], dtype=torch.float32)),\n",
    "    'dec_b2': nn.Parameter(torch.zeros([n_map_1], dtype=torch.float32)),\n",
    "    'dec_b1': nn.Parameter(torch.zeros([n_input_channel], dtype=torch.float32)),\n",
    "}\n",
    "\n",
    "last_layer = {\n",
    "    'w': nn.Parameter(torch.randn([n_prototypes, n_classes],\n",
    "                                       dtype=torch.float32))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print shapes of all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "enc_f1 torch.Size([32, 1, 3, 3])\n",
      "enc_f2 torch.Size([32, 32, 3, 3])\n",
      "enc_f3 torch.Size([32, 32, 3, 3])\n",
      "enc_f4 torch.Size([10, 32, 3, 3])\n",
      "dec_f4 torch.Size([10, 32, 3, 3])\n",
      "dec_f3 torch.Size([32, 32, 3, 3])\n",
      "dec_f2 torch.Size([32, 32, 3, 3])\n",
      "dec_f1 torch.Size([32, 1, 3, 3])\n",
      "biases\n",
      "enc_b1 torch.Size([32])\n",
      "enc_b2 torch.Size([32])\n",
      "enc_b3 torch.Size([32])\n",
      "enc_b4 torch.Size([10])\n",
      "dec_b4 torch.Size([32])\n",
      "dec_b3 torch.Size([32])\n",
      "dec_b2 torch.Size([32])\n",
      "dec_b1 torch.Size([1])\n",
      "last_layer\n",
      "torch.Size([15, 10])\n"
     ]
    }
   ],
   "source": [
    "# Printing shapes of all parameters\n",
    "print(\"weights\")\n",
    "for weight in weights.keys():\n",
    "    print(weight, weights[weight].shape)\n",
    "print(\"biases\")\n",
    "for b in biases.keys():\n",
    "    print(b, biases[b].shape)\n",
    "print(\"last_layer\")\n",
    "print(last_layer['w'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer functions\n",
    "#### <font color='red'>Fix the stride and padding parameters, check if filter in tf is same as weight in pt</font>\n",
    "Padding discussion pytorch: https://github.com/pytorch/pytorch/issues/3867\n",
    "\n",
    "Blogpost: https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python\n",
    "\n",
    "### Padding helper\n",
    "Based on blogpost above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_img(img):\n",
    "    ''' Takes an input image (batch) and pads according to Tensorflows SAME padding'''\n",
    "    input_h = img.shape[2]\n",
    "    input_w = img.shape[3]\n",
    "    stride = 2 \n",
    "    filter_h = 3\n",
    "    filter_w = 3\n",
    "    \n",
    "    output_h = int(ceil(float(input_h)) / float(stride))\n",
    "    output_w = output_h\n",
    "    \n",
    "    if input_h % stride == 0:\n",
    "        pad_height = max((filter_h - stride), 0)\n",
    "    else:\n",
    "        pad_height = max((filter_h - (input_h % stride), 0))\n",
    "\n",
    "    pad_width = pad_height\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    padded_img = torch.zeros(img.shape[0], img.shape[1], input_h + pad_height, input_w + pad_width)\n",
    "    padded_img[:,:, pad_top:-pad_bottom, pad_left:-pad_right] = img\n",
    "\n",
    "    return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(img, kernel, bias, strides, padding=\"VALID\", nonlinearity = nn.ReLU()):\n",
    "    img = pad_img(img)\n",
    "    conv = F.conv2d(img, kernel, bias=bias, stride=strides, padding=padding)\n",
    "    out = nonlinearity(conv)\n",
    "    return out\n",
    "\n",
    "#### stride must be tuple for torch, is a list in tf\n",
    "#### padding is different, tf uses same/valid, torch a int or list of ints\n",
    "#### is the filter the same ass weights argument for conv2d?\n",
    "\n",
    "# tensorflow's conv2d_transpose needs to know the shape of the output\n",
    "def deconv_layer(img, kernel, bias, strides, padding=\"VALID\", nonlinearity=nn.ReLU(), out_padding = None):\n",
    "#     img = pad_img(img)\n",
    "    deconv = F.conv_transpose2d(img, kernel, bias=bias, stride=strides, padding=padding, output_padding=out_padding)\n",
    "    out = nonlinearity(deconv)\n",
    "    return out\n",
    "\n",
    "# def deconv_layer(img, kernel, bias, strides, padding=\"VALID\", nonlinearity=nn.ReLU(), out_size = None):\n",
    "#     in_channel = img.shape[1]\n",
    "#     out_channel = kernel.shape[1]\n",
    "#     k_size = (kernel.shape[2], kernel.shape[3])\n",
    "    \n",
    "#     img = pad_img(img)\n",
    "    \n",
    "#     upsample = nn.ConvTranspose2d(in_channel, out_channel, k_size, strides, padding=(0,0))\n",
    "    \n",
    "#     out = upsample(img, output_size=out_size)\n",
    "    \n",
    "\n",
    "def fc_layer(input, weight, bias, nonlinearity=nn.ReLU()):\n",
    "    return nonlinearity(torch.mm(input, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Dummy tensor for debugging\n",
    "# X = torch.empty(batch_size, n_input_channel, input_width, input_height)\n",
    "X,Y = next(iter(train_dl))\n",
    "X = X.view(250,1,28,28).float()\n",
    "# X in shape 250x28x28 (Batch x H x W)\n",
    "print(X.shape)\n",
    "# Y in shape 250 (B). Needs to be converted to one-hot for reproduction of paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  torch.Size([250, 1, 28, 28])\n",
      "EL1:  torch.Size([250, 32, 14, 14])\n",
      "EL2:  torch.Size([250, 32, 7, 7])\n",
      "EL3:  torch.Size([250, 32, 4, 4])\n",
      "EL4:  torch.Size([250, 10, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "PADDING_FLAG = (0,0)\n",
    "# eln means the output of the nth layer of the encoder\n",
    "print('X: ', X.shape)\n",
    "el1 = conv_layer(X, weights['enc_f1'], biases['enc_b1'], stride_1, padding=PADDING_FLAG)\n",
    "print('EL1: ',el1.shape)\n",
    "el2 = conv_layer(el1, weights['enc_f2'], biases['enc_b2'], stride_2, padding=PADDING_FLAG)\n",
    "print('EL2: ',el2.shape)\n",
    "el3 = conv_layer(el2, weights['enc_f3'], biases['enc_b3'], stride_3, padding=PADDING_FLAG)\n",
    "print('EL3: ',el3.shape)\n",
    "el4 = conv_layer(el3, weights['enc_f4'], biases['enc_b4'], stride_4, padding=PADDING_FLAG)\n",
    "print('EL4: ',el4.shape)\n",
    "\n",
    "input_shape = list(X.shape)\n",
    "l1_shape = list(el1.shape)\n",
    "l2_shape = list(el2.shape)\n",
    "l3_shape = list(el3.shape)\n",
    "l4_shape = list(el4.shape)\n",
    "\n",
    "flatten_size = l4_shape[1] * l4_shape[2] * l4_shape[3]\n",
    "n_features = flatten_size\n",
    "\n",
    "# feature vectors is the flattened output of the encoder\n",
    "feature_vectors = torch.reshape(el4, shape=[-1, flatten_size])\n",
    "#print(feature_vectors.shape)\n",
    "# initialize the prototype feature vectors\n",
    "prototype_feature_vectors = nn.Parameter(torch.empty(size=\n",
    "                                        [n_prototypes, n_features],\n",
    "                                        dtype=torch.float32).uniform_())\n",
    "\n",
    "deconv_batch_size = torch.eye(feature_vectors.shape[0])\n",
    "# this is necessary for prototype images evaluation\n",
    "reshape_feature_vectors = torch.reshape(feature_vectors, shape=[-1, l4_shape[1],\n",
    "   l4_shape[2], l4_shape[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "#### <font color='red'> Padding needs to be fixed. Currently 'hacked'</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape_feature_vectors : torch.Size([250, 10, 2, 2])\n",
      "torch.Size([250, 10, 3, 3])\n",
      "DL4:  torch.Size([250, 32, 4, 4])\n",
      "DL3:  torch.Size([250, 32, 7, 7])\n",
      "DL2:  torch.Size([250, 32, 14, 14])\n",
      "DL1:  torch.Size([250, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print('Reshape_feature_vectors :', reshape_feature_vectors.shape)\n",
    "print(pad_img(reshape_feature_vectors).shape)\n",
    "dl4 = deconv_layer(reshape_feature_vectors, weights['dec_f4'], biases['dec_b4'],\n",
    "                   strides=stride_4, padding=1, out_padding = 1)\n",
    "# want to dl4 to have shape 250 x 32 x 4 x 4\n",
    "print('DL4: ',dl4.shape)\n",
    "\n",
    "dl3 = deconv_layer(dl4, weights['dec_f3'], biases['dec_b3'],\n",
    "                   strides=stride_3, padding=1, out_padding=0)\n",
    "# want to dl3 to have shape 250 x 32 x 7 x 7\n",
    "print('DL3: ',dl3.shape)\n",
    "\n",
    "dl2 = deconv_layer(dl3, weights['dec_f2'], biases['dec_b2'],\n",
    "                   strides=stride_2, padding=1, out_padding=1)\n",
    "# want to dl2 to have shape 250 x 32 x 14 x 14\n",
    "print('DL2: ',dl2.shape)\n",
    "\n",
    "dl1 = deconv_layer(dl2, weights['dec_f1'], biases['dec_b1'],\n",
    "                   strides=stride_1, padding=1, out_padding=1,\n",
    "                   nonlinearity=nn.Sigmoid())\n",
    "# want to dl4 to have shape 250 x 1 x 28 x 28\n",
    "print('DL1: ',dl1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 784])\n",
      "torch.Size([250, 784])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X_decoded is the decoding of the encoded feature vectors in X;\n",
    "we reshape it to match the shape of the training input\n",
    "X_true is the correct output for the autoencoder\n",
    "'''\n",
    "\n",
    "X_decoded = torch.reshape(dl1, shape=(-1, input_size))\n",
    "X_true = X.view(-1, input_size)\n",
    "print(X_decoded.shape)\n",
    "print(X_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "prototype_distances is the list of distances from each x_i to every prototype\n",
    "in the latent space\n",
    "feature_vector_distances is the list of distances from each prototype to every x_i\n",
    "in the latent space\n",
    "'''\n",
    "prototype_distances = list_of_distances(feature_vectors,\n",
    "                                        prototype_feature_vectors)\n",
    "prototype_distances = torch.Tensor(prototype_distances)\n",
    "feature_vector_distances = list_of_distances(prototype_feature_vectors,\n",
    "                                             feature_vectors)\n",
    "feature_vector_distances = torch.Tensor(feature_vector_distances)\n",
    "\n",
    "# the logits are the weighted sum of distances from prototype_distances\n",
    "logits = torch.mm(prototype_distances, last_layer['w'])\n",
    "probability_distribution = F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5550998., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "the error function consists of 4 terms, the autoencoder loss,\n",
    "the classification loss, and the two requirements that every feature vector in\n",
    "X look like at least one of the prototype feature vectors and every prototype\n",
    "feature vector look like at least one of the feature vectors in X.\n",
    "'''\n",
    "ae_error = torch.mean(list_of_norms(X_decoded - X_true))\n",
    "class_error = F.cross_entropy(logits, Y)\n",
    "error_1 = torch.mean(torch.min(feature_vector_distances, axis=1)[0])\n",
    "error_2 = torch.mean(torch.min(prototype_distances, axis = 1)[0])\n",
    "\n",
    "# total_error is the our minimization objective\n",
    "total_error = lambda_class * class_error +\\\n",
    "              lambda_ae * ae_error + \\\n",
    "              lambda_1 * error_1 + \\\n",
    "              lambda_2 * error_2\n",
    "\n",
    "print(total_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-245-2dffbd2ea87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# accuracy is not the classification error term; it is the percentage accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m correct_prediction = tf.equal(tf.argmax(logits, 1),\n\u001b[0m\u001b[1;32m      3\u001b[0m                               \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               name='correct_prediction')\n\u001b[1;32m      5\u001b[0m accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# accuracy is not the classification error term; it is the percentage accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1),\n",
    "                              tf.argmax(Y, 1),\n",
    "                              name='correct_prediction')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32),\n",
    "                          name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
