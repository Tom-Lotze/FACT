{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Helper functions borrowed from original paper by Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# def list_of_distances(X, Y):\n",
    "#     '''\n",
    "#     Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "#     Y = [y_1, ... , y_m], we return a list of vectors\n",
    "#             [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "#              ...\n",
    "#              [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "#     where the distance metric used is the sqared euclidean distance.\n",
    "#     The computation is achieved through a clever use of broadcasting.\n",
    "#     '''\n",
    "#     XX = torch.reshape(list_of_norms(X), shape=(-1, 1))\n",
    "#     YY = torch.reshape(list_of_norms(Y), shape=(1, -1))\n",
    "#     output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "#     return output\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return torch.sum(torch.pow(X, 2), dim=1)\n",
    "\n",
    "def print_and_write(str, file):\n",
    "    '''\n",
    "    print str to the console and also write it to file\n",
    "    '''\n",
    "    print(str)\n",
    "    file.write(str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "makedirs('./data/mnist')\n",
    "\n",
    "# Models folder\n",
    "model_folder = os.path.join(os.getcwd(), \"saved_model\", \"mnist_model\")\n",
    "makedirs(model_folder)\n",
    "\n",
    "# Image folder\n",
    "img_folder = os.path.join(model_folder, \"img\")\n",
    "makedirs(img_folder)\n",
    "\n",
    "# Model filename\n",
    "model_filename = \"mnist_cae.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Pytorch\n",
    "#### <font color='red'>Double check the normalization mean and stdev for dataset</font>\n",
    "#### <font color='red'>Double check parameters Dataloader (e.g. shuffle on or off, different batch sizes for train/valid/test)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Transforms to perform on loaded dataset. Normalize around mean 0.1307 and std 0.3081 for optimal pytorch results. \n",
    "# source: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "# Load datasets into reproduction/data/mnist. Download if data not present. \n",
    "mnist_train = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms))\n",
    "\n",
    "mnist_train_data = mnist_train.dataset.data\n",
    "mnist_train_targets = mnist_train.dataset.targets\n",
    "\n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train_data[0:55000]\n",
    "y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train_data[55000:60000]\n",
    "y_valid = mnist_train_targets[55000:60000]\n",
    "\n",
    "# 10000 examples in test set\n",
    "mnist_test = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=False, download=True, \n",
    "                                                   transform=transforms))\n",
    "\n",
    "x_test = mnist_test.dataset.data\n",
    "y_test = mnist_test.dataset.targets\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM THE ORIGINAL IMPLEMENTATION\n",
    "# training parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1500\n",
    "\n",
    "# frequency of testing and saving\n",
    "test_display_step = 100    # how many epochs we do evaluate on the test set once, default 100\n",
    "save_step = 50            # how frequently do we save the model to disk\n",
    "\n",
    "# elastic deformation parameters\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "\n",
    "# lambda's are the ratios between the four error terms\n",
    "lambda_class = 20\n",
    "lambda_ae = 1 # autoencoder\n",
    "lambda_1 = 1 # push prototype vectors to have meaningful decodings in pixel space\n",
    "lambda_2 = 1 # cluster training examples around prototypes in latent space\n",
    "\n",
    "\n",
    "input_height = input_width =  28    # MNIST data input shape \n",
    "n_input_channel = 1     # the number of color channels; for MNIST is 1.\n",
    "input_size = input_height * input_width * n_input_channel   # 784\n",
    "n_classes = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_prototypes = 15         # the number of prototypes\n",
    "n_layers = 4\n",
    "\n",
    "# height and width of each layers' filters\n",
    "f_1 = 3\n",
    "f_2 = 3\n",
    "f_3 = 3\n",
    "f_4 = 3\n",
    "\n",
    "# stride size in each direction for each of the layers\n",
    "s_1 = 2\n",
    "s_2 = 2\n",
    "s_3 = 2\n",
    "s_4 = 2\n",
    "\n",
    "# number of feature maps in each layer\n",
    "n_map_1 = 32\n",
    "n_map_2 = 32\n",
    "n_map_3 = 32\n",
    "n_map_4 = 10\n",
    "\n",
    "# the shapes of each layer's filter\n",
    "# [out channel, in_channel, 3, 3]\n",
    "filter_shape_1 = [n_map_1, n_input_channel, f_1, f_1]\n",
    "filter_shape_2 = [n_map_2, n_map_1, f_2, f_2]\n",
    "filter_shape_3 = [n_map_3, n_map_2, f_3, f_3]\n",
    "filter_shape_4 = [n_map_4, n_map_3, f_4, f_4]\n",
    "\n",
    "# strides for each layer (changed to tuples)\n",
    "stride_1 = [s_1, s_1]\n",
    "stride_2 = [s_2, s_2]\n",
    "stride_3 = [s_3, s_3]\n",
    "stride_4 = [s_4, s_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction\n",
    "#### <font color='red'>Fix the stride and padding parameters, check if filter in tf is same as weight in pt</font>\n",
    "Padding discussion pytorch: https://github.com/pytorch/pytorch/issues/3867\n",
    "\n",
    "Blogpost: https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder'''\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "        \n",
    "        # define layers\n",
    "        self.enc_l1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l4 = nn.Conv2d(32, 10, kernel_size=3, stride=2, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def pad_image(self, img):\n",
    "        ''' Takes an input image (batch) and pads according to Tensorflows SAME padding'''\n",
    "        input_h = img.shape[2]\n",
    "        input_w = img.shape[3]\n",
    "        stride = 2 \n",
    "        filter_h = 3\n",
    "        filter_w = 3\n",
    "\n",
    "        output_h = int(ceil(float(input_h)) / float(stride))\n",
    "        output_w = output_h\n",
    "\n",
    "        if input_h % stride == 0:\n",
    "            pad_height = max((filter_h - stride), 0)\n",
    "        else:\n",
    "            pad_height = max((filter_h - (input_h % stride), 0))\n",
    "\n",
    "        pad_width = pad_height\n",
    "\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        padded_img = torch.zeros(img.shape[0], img.shape[1], input_h + pad_height, input_w + pad_width)\n",
    "        padded_img[:,:, pad_top:-pad_bottom, pad_left:-pad_right] = img\n",
    "\n",
    "        return padded_img\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pad_x = self.pad_image(x)\n",
    "        el1 = self.sigmoid(self.enc_l1(pad_x))\n",
    "        \n",
    "        pad_el1 = self.pad_image(el1)\n",
    "        el2 = self.sigmoid(self.enc_l2(pad_el1))\n",
    "    \n",
    "        pad_el2 = self.pad_image(el2)\n",
    "        el3 = self.sigmoid(self.enc_l3(pad_el2))\n",
    "        \n",
    "        pad_el3 = self.pad_image(el3)\n",
    "        el4 = self.sigmoid(self.enc_l4(pad_el3))\n",
    "        \n",
    "        return el4\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder'''\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "\n",
    "        # define layers\n",
    "        self.dec_l4 = nn.ConvTranspose2d(10, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l3 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "        self.dec_l2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l1 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        dl4 = self.sigmoid(self.dec_l4(enc_x))\n",
    "        dl3 = self.sigmoid(self.dec_l3(dl4))\n",
    "        dl2 = self.sigmoid(dec_l2(dl3))\n",
    "        decoded_x = self.sigmoid(self.dec_l1(dl2))\n",
    "        \n",
    "        return decoded_x\n",
    "\n",
    "\n",
    "class nn_prototype(nn.Module):\n",
    "    '''Model'''\n",
    "    def __init__(self, n_prototypes=15, n_layers=4, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        # initialize prototype - currently not in correct spot\n",
    "        n_features = 40 # size of encoded x - 250 x 10 x 2 x 2\n",
    "        self.prototype_feature_vectors = nn.Parameter(torch.empty(size=(n_prototypes, n_features), \n",
    "                                                                  dtype=torch.float32).uniform_())\n",
    "        \n",
    "        self.last_layer = nn.Linear(n_prototypes,10)\n",
    "        \n",
    "    def list_of_distances(self, X, Y):\n",
    "        '''\n",
    "        Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "        Y = [y_1, ... , y_m], we return a list of vectors\n",
    "                [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "                 ...\n",
    "                 [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "        where the distance metric used is the sqared euclidean distance.\n",
    "        The computation is achieved through a clever use of broadcasting.\n",
    "        '''\n",
    "        XX = torch.reshape(self.list_of_norms(X), shape=(-1, 1))\n",
    "        YY = torch.reshape(self.list_of_norms(Y), shape=(1, -1))\n",
    "        output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def list_of_norms(self, X):\n",
    "        '''\n",
    "        X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "            [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "        function is the squared euclidean distance.\n",
    "        '''\n",
    "        return torch.sum(torch.pow(X, 2), dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #encoder step\n",
    "        enc_x = self.encoder(x)\n",
    "        \n",
    "        #decoder step\n",
    "        dec_x = self.decoder(enc_x)\n",
    "        # hardcoded input size\n",
    "        dec_x = dec_x.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        \n",
    "        # flatten encoded x to compute distance with prototypes\n",
    "        n_features = enc_x.shape[1] * enc_x.shape[2] * enc_x.shape[3]\n",
    "        feature_vectors_flat = torch.reshape(enc_x, shape=[-1, n_features])\n",
    "        \n",
    "        # distance to prototype\n",
    "        prototype_distances = self.list_of_distances(feature_vectors_flat, self.prototype_feature_vectors)\n",
    "        \n",
    "        # distance to feature vectors\n",
    "        feature_vector_distances = self.list_of_distances(self.prototype_feature_vectors, feature_vectors_flat)\n",
    "        \n",
    "        # classification layer\n",
    "        logits = self.last_layer(prototype_distances)\n",
    "        \n",
    "        # Softmax to prob dist not needed as cross entropy loss is used?\n",
    "        \n",
    "        return dec_x, logits, feature_vector_distances, prototype_distances\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the error function consists of 4 terms, the autoencoder loss,\n",
    "the classification loss, and the two requirements that every feature vector in\n",
    "X look like at least one of the prototype feature vectors and every prototype\n",
    "feature vector look like at least one of the feature vectors in X.\n",
    "'''\n",
    "def loss_function(X_decoded, X_true, logits, Y, feature_dist, prototype_dist, lambdas=None):\n",
    "    if lambdas == None:\n",
    "        lambda_class, lambda_ae, lambda_1, lambda_2 = 20, 1, 1, 1\n",
    "    \n",
    "    ae_error = torch.mean(list_of_norms(X_decoded - X_true))\n",
    "    class_error = F.cross_entropy(logits, Y, reduction=\"mean\")\n",
    "    error_1 = torch.mean(torch.min(feature_dist, axis=1)[0])\n",
    "    error_2 = torch.mean(torch.min(prototype_dist, axis = 1)[0])\n",
    "\n",
    "    # total_error is the our minimization objective\n",
    "    total_error = lambda_class * class_error +\\\n",
    "                  lambda_ae * ae_error + \\\n",
    "                  lambda_1 * error_1 + \\\n",
    "                  lambda_2 * error_2\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(logits, labels):\n",
    "    batch_size = labels.shape[0]\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    total_correct = torch.sum(predictions == labels).item()\n",
    "    accuracy = total_correct / batch_size\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train loss: 2449.91162109375\n",
      "Train acc: 0.448\n",
      "model is saved\n",
      "\n",
      "Test loss: 2410.1357421875\n",
      "Test acc: 0.4851\n",
      "\n",
      "Valid loss: 2442.1455078125\n",
      "Valid acc: 0.4826\n",
      "\n",
      "Epoch: 1\n",
      "Train loss: 1908.2757568359375\n",
      "Train acc: 0.628\n",
      "\n",
      "Valid loss: 1909.469482421875\n",
      "Valid acc: 0.6818\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 1711.37060546875\n",
      "Train acc: 0.74\n",
      "\n",
      "Valid loss: 1705.4996337890625\n",
      "Valid acc: 0.756\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 1627.7833251953125\n",
      "Train acc: 0.744\n",
      "\n",
      "Valid loss: 1604.021484375\n",
      "Valid acc: 0.7892\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 1588.3890380859375\n",
      "Train acc: 0.764\n",
      "\n",
      "Valid loss: 1560.4439697265625\n",
      "Valid acc: 0.8068\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 1524.9996337890625\n",
      "Train acc: 0.784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-ea7079f52139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mx_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mX_decoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprot_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m# compute losses and accuracy and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-81aa951f2053>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m#encoder step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0menc_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m#decoder step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-81aa951f2053>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mpad_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_l1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mpad_el1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def train():\n",
    "model = nn_prototype(15,4,10)\n",
    "\n",
    "batch_size_ = 250\n",
    "\n",
    "# get validation and test set\n",
    "valid_dl = DataLoader(valid_data, batch_size=5000, drop_last=False, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=10000, drop_last=False, shuffle=True)\n",
    "\n",
    "\n",
    "#     print(\"Model parameters:\")\n",
    "#     for name, p in model.named_parameters():\n",
    "#         print(name)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# initialize storage for results\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "valid_accs = []\n",
    "valid_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(training_epochs):\n",
    "    print(\"\\nEpoch:\", epoch)\n",
    "\n",
    "    # load the training data and reshuffle\n",
    "    train_dl = DataLoader(train_data, batch_size=batch_size_, drop_last=False, shuffle=True)\n",
    "\n",
    "    # loop over the batches\n",
    "    for step, (x, Y) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.view(x.shape[0], 1, x.shape[1], x.shape[2]).float()\n",
    "\n",
    "        # perform forward pass\n",
    "        X_decoded, logits, feature_dist, prot_dist = model(x)\n",
    "\n",
    "        # compute the loss\n",
    "        total_loss = loss_function(X_decoded, x, logits, Y, feature_dist, prot_dist)\n",
    "\n",
    "        # backpropagate over the loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # compute and save accuracy and loss\n",
    "    train_accuracy = compute_acc(logits, Y)\n",
    "    train_accs.append(train_accuracy)\n",
    "    train_losses.append(total_loss.item())\n",
    "\n",
    "    # print information\n",
    "    print('Train loss:', total_loss.item())\n",
    "    print('Train acc:', train_accuracy)\n",
    "\n",
    "\n",
    "    if epoch % test_display_step == 0:\n",
    "        # save model and prototypes\n",
    "        torch.save(model, model_folder+\"/\"+model_filename)\n",
    "        print(\"model is saved\")\n",
    "\n",
    "        # perform testing\n",
    "        with torch.no_grad():\n",
    "            for step, (x_test, y_test) in enumerate(test_dl):\n",
    "                x_test = x_test.view(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2]).float()\n",
    "\n",
    "                # forward pass\n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_test)\n",
    "\n",
    "                # compute loss and accuracy and save\n",
    "                test_accuracy = compute_acc(logits, y_test)\n",
    "                test_loss = loss_function(X_decoded, x_test, logits, y_test, feature_dist, prot_dist)\n",
    "                test_accs.append(test_accuracy)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "            print('\\nTest loss:', test_loss.item())\n",
    "            print('Test acc:', test_accuracy)\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        for step, (x_valid, y_valid) in enumerate(valid_dl):\n",
    "                x_valid = x_valid.view(x_valid.shape[0], 1, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_valid)\n",
    "\n",
    "                # compute losses and accuracy and save\n",
    "                valid_accuracy = compute_acc(logits, y_valid)\n",
    "                valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist)\n",
    "                valid_accs.append(valid_accuracy)\n",
    "                valid_losses.append(valid_loss)\n",
    "\n",
    "        print('\\nValid loss:', valid_loss.item())\n",
    "        print('Valid acc:', valid_accuracy)\n",
    "\n",
    "#return train_accs, train_losses, valid_accs, valid_losses, test_accs, test_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and visualize prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = torch.load(model_folder+\"/\"+model_filename)\n",
    "\n",
    "def visualize_prototypes(model, epoch=\"\", save=True):\n",
    "    # get saved prototypes\n",
    "    encoded_prototypes = model.prototype_feature_vectors\n",
    "    encoded_prototypes_reshaped = encoded_prototypes.view(n_prototypes, 10, 2, 2)\n",
    "\n",
    "    # decode prototypes\n",
    "    decoded_prototypes = model.decoder(encoded_prototypes_reshaped).squeeze().detach().numpy()\n",
    "\n",
    "    for i in range(15):\n",
    "        plt.imshow(decoded_prototypes[i])\n",
    "        plt.save(img_folder+\"/prototype\"+ epoch)\n",
    "\n",
    "    print(model.prototype_feature_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting of losses and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
