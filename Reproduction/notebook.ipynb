{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from math import ceil\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Helper functions borrowed from original paper by Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# def list_of_distances(X, Y):\n",
    "#     '''\n",
    "#     Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "#     Y = [y_1, ... , y_m], we return a list of vectors\n",
    "#             [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "#              ...\n",
    "#              [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "#     where the distance metric used is the sqared euclidean distance.\n",
    "#     The computation is achieved through a clever use of broadcasting.\n",
    "#     '''\n",
    "#     XX = torch.reshape(list_of_norms(X), shape=(-1, 1))\n",
    "#     YY = torch.reshape(list_of_norms(Y), shape=(1, -1))\n",
    "#     output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "#     return output\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return torch.sum(torch.pow(X, 2), dim=1)\n",
    "\n",
    "def print_and_write(str, file):\n",
    "    '''\n",
    "    print str to the console and also write it to file\n",
    "    '''\n",
    "    print(str)\n",
    "    file.write(str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "makedirs('./data/mnist')\n",
    "\n",
    "# Models folder\n",
    "model_folder = os.path.join(os.getcwd(), \"saved_model\", \"mnist_model\")\n",
    "makedirs(model_folder)\n",
    "\n",
    "# Image folder\n",
    "img_folder = os.path.join(model_folder, \"img\")\n",
    "makedirs(img_folder)\n",
    "\n",
    "# Model filename\n",
    "model_filename = \"mnist_cae.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Pytorch\n",
    "#### <font color='red'>Double check the normalization mean and stdev for dataset</font>\n",
    "#### <font color='red'>Double check parameters Dataloader (e.g. shuffle on or off, different batch sizes for train/valid/test)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Transforms to perform on loaded dataset. Normalize around mean 0.1307 and std 0.3081 for optimal pytorch results. \n",
    "# source: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "# Load datasets into reproduction/data/mnist. Download if data not present. \n",
    "mnist_train = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms))\n",
    "\n",
    "mnist_train_data = mnist_train.dataset.data\n",
    "mnist_train_targets = mnist_train.dataset.targets\n",
    "\n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train_data[0:55000]\n",
    "y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train_data[55000:60000]\n",
    "y_valid = mnist_train_targets[55000:60000]\n",
    "\n",
    "# 10000 examples in test set\n",
    "mnist_test = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=False, download=True, \n",
    "                                                   transform=transforms))\n",
    "\n",
    "x_test = mnist_test.dataset.data\n",
    "y_test = mnist_test.dataset.targets\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM THE ORIGINAL IMPLEMENTATION\n",
    "# training parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1500\n",
    "\n",
    "# frequency of testing and saving\n",
    "test_display_step = 5    # how many epochs we do evaluate on the test set once, default 100\n",
    "save_step = 50            # how frequently do we save the model to disk\n",
    "\n",
    "# elastic deformation parameters\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "\n",
    "# lambda's are the ratios between the four error terms\n",
    "lambda_class = 20\n",
    "lambda_ae = 1 # autoencoder\n",
    "lambda_1 = 1 # push prototype vectors to have meaningful decodings in pixel space\n",
    "lambda_2 = 1 # cluster training examples around prototypes in latent space\n",
    "\n",
    "\n",
    "input_height = input_width =  28    # MNIST data input shape \n",
    "n_input_channel = 1     # the number of color channels; for MNIST is 1.\n",
    "input_size = input_height * input_width * n_input_channel   # 784\n",
    "n_classes = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_prototypes = 15         # the number of prototypes\n",
    "n_layers = 4\n",
    "\n",
    "# height and width of each layers' filters\n",
    "f_1 = 3\n",
    "f_2 = 3\n",
    "f_3 = 3\n",
    "f_4 = 3\n",
    "\n",
    "# stride size in each direction for each of the layers\n",
    "s_1 = 2\n",
    "s_2 = 2\n",
    "s_3 = 2\n",
    "s_4 = 2\n",
    "\n",
    "# number of feature maps in each layer\n",
    "n_map_1 = 32\n",
    "n_map_2 = 32\n",
    "n_map_3 = 32\n",
    "n_map_4 = 10\n",
    "\n",
    "# the shapes of each layer's filter\n",
    "# [out channel, in_channel, 3, 3]\n",
    "filter_shape_1 = [n_map_1, n_input_channel, f_1, f_1]\n",
    "filter_shape_2 = [n_map_2, n_map_1, f_2, f_2]\n",
    "filter_shape_3 = [n_map_3, n_map_2, f_3, f_3]\n",
    "filter_shape_4 = [n_map_4, n_map_3, f_4, f_4]\n",
    "\n",
    "# strides for each layer (changed to tuples)\n",
    "stride_1 = [s_1, s_1]\n",
    "stride_2 = [s_2, s_2]\n",
    "stride_3 = [s_3, s_3]\n",
    "stride_4 = [s_4, s_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction\n",
    "#### <font color='red'>Fix the stride and padding parameters, check if filter in tf is same as weight in pt</font>\n",
    "Padding discussion pytorch: https://github.com/pytorch/pytorch/issues/3867\n",
    "\n",
    "Blogpost: https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder'''\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "        \n",
    "        # define layers\n",
    "        self.enc_l1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l4 = nn.Conv2d(32, 10, kernel_size=3, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def pad_image(self, img):\n",
    "        ''' Takes an input image (batch) and pads according to Tensorflows SAME padding'''\n",
    "        input_h = img.shape[2]\n",
    "        input_w = img.shape[3]\n",
    "        stride = 2 \n",
    "        filter_h = 3\n",
    "        filter_w = 3\n",
    "\n",
    "        output_h = int(ceil(float(input_h)) / float(stride))\n",
    "        output_w = output_h\n",
    "\n",
    "        if input_h % stride == 0:\n",
    "            pad_height = max((filter_h - stride), 0)\n",
    "        else:\n",
    "            pad_height = max((filter_h - (input_h % stride), 0))\n",
    "\n",
    "        pad_width = pad_height\n",
    "\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        padded_img = torch.zeros(img.shape[0], img.shape[1], input_h + pad_height, input_w + pad_width)\n",
    "        padded_img[:,:, pad_top:-pad_bottom, pad_left:-pad_right] = img\n",
    "\n",
    "        return padded_img\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pad_x = self.pad_image(x)\n",
    "        el1 = self.relu(self.enc_l1(pad_x))\n",
    "        \n",
    "        pad_el1 = self.pad_image(el1)\n",
    "        el2 = self.relu(self.enc_l2(pad_el1))\n",
    "    \n",
    "        pad_el2 = self.pad_image(el2)\n",
    "        el3 = self.relu(self.enc_l3(pad_el2))\n",
    "        \n",
    "        pad_el3 = self.pad_image(el3)\n",
    "        el4 = self.sigmoid(self.enc_l4(pad_el3))\n",
    "        \n",
    "        return el4\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder'''\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "\n",
    "        # define layers\n",
    "        self.dec_l4 = nn.ConvTranspose2d(10, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l3 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "        self.dec_l2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l1 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        dl4 = self.relu(self.dec_l4(enc_x))\n",
    "        dl3 = self.relu(self.dec_l3(dl4))\n",
    "        dl2 = self.relu(self.dec_l2(dl3))\n",
    "        decoded_x = self.sigmoid(self.dec_l1(dl2))\n",
    "        \n",
    "        return decoded_x\n",
    "\n",
    "\n",
    "class nn_prototype(nn.Module):\n",
    "    '''Model'''\n",
    "    def __init__(self, n_prototypes=15, n_layers=4, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        # initialize prototype - currently not in correct spot\n",
    "        n_features = 40 # size of encoded x - 250 x 10 x 2 x 2\n",
    "        self.prototype_feature_vectors = nn.Parameter(torch.empty(size=(n_prototypes, n_features), \n",
    "                                                                  dtype=torch.float32).uniform_())\n",
    "        \n",
    "        self.last_layer = nn.Linear(n_prototypes,10)\n",
    "        \n",
    "    def list_of_distances(self, X, Y):\n",
    "        '''\n",
    "        Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "        Y = [y_1, ... , y_m], we return a list of vectors\n",
    "                [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "                 ...\n",
    "                 [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "        where the distance metric used is the sqared euclidean distance.\n",
    "        The computation is achieved through a clever use of broadcasting.\n",
    "        '''\n",
    "        XX = torch.reshape(self.list_of_norms(X), shape=(-1, 1))\n",
    "        YY = torch.reshape(self.list_of_norms(Y), shape=(1, -1))\n",
    "        output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def list_of_norms(self, X):\n",
    "        '''\n",
    "        X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "            [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "        function is the squared euclidean distance.\n",
    "        '''\n",
    "        return torch.sum(torch.pow(X, 2), dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #encoder step\n",
    "        enc_x = self.encoder(x)\n",
    "        \n",
    "        #decoder step\n",
    "        dec_x = self.decoder(enc_x)\n",
    "        # hardcoded input size\n",
    "        dec_x = dec_x.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        \n",
    "        # flatten encoded x to compute distance with prototypes\n",
    "        n_features = enc_x.shape[1] * enc_x.shape[2] * enc_x.shape[3]\n",
    "        feature_vectors_flat = torch.reshape(enc_x, shape=[-1, n_features])\n",
    "        \n",
    "        # distance to prototype\n",
    "        prototype_distances = self.list_of_distances(feature_vectors_flat, self.prototype_feature_vectors)\n",
    "        \n",
    "        # distance to feature vectors\n",
    "        feature_vector_distances = self.list_of_distances(self.prototype_feature_vectors, feature_vectors_flat)\n",
    "        \n",
    "        # classification layer\n",
    "        logits = self.last_layer(prototype_distances)\n",
    "        \n",
    "        # Softmax to prob dist not needed as cross entropy loss is used?\n",
    "        \n",
    "        return dec_x, logits, feature_vector_distances, prototype_distances\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the error function consists of 4 terms, the autoencoder loss,\n",
    "the classification loss, and the two requirements that every feature vector in\n",
    "X look like at least one of the prototype feature vectors and every prototype\n",
    "feature vector look like at least one of the feature vectors in X.\n",
    "'''\n",
    "def loss_function(X_decoded, X_true, logits, Y, feature_dist, prototype_dist, lambdas=None):\n",
    "    if lambdas == None:\n",
    "        lambda_class, lambda_ae, lambda_1, lambda_2 = 20, 1, 1, 1\n",
    "    \n",
    "    ae_error = torch.mean(list_of_norms(X_decoded - X_true))\n",
    "    class_error = F.cross_entropy(logits, Y, reduction=\"mean\")\n",
    "    error_1 = torch.mean(torch.min(feature_dist, axis=1)[0])\n",
    "    error_2 = torch.mean(torch.min(prototype_dist, axis = 1)[0])\n",
    "\n",
    "    # total_error is the our minimization objective\n",
    "    total_error = lambda_class * class_error +\\\n",
    "                  lambda_ae * ae_error + \\\n",
    "                  lambda_1 * error_1 + \\\n",
    "                  lambda_2 * error_2\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(logits, labels):\n",
    "    batch_size = labels.shape[0]\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    total_correct = torch.sum(predictions == labels).item()\n",
    "    accuracy = total_correct / batch_size\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic deformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_elastic_transform(images, sigma, alpha, height, width, random_state=None):\n",
    "    '''\n",
    "    this code is borrowed from chsasank on GitHubGist\n",
    "    Elastic deformation of images as described in [Simard 2003].\n",
    "    \n",
    "    images: a two-dimensional numpy array; we can think of it as a list of flattened images\n",
    "    sigma: the real-valued variance of the gaussian kernel\n",
    "    alpha: a real-value that is multiplied onto the displacement fields\n",
    "    \n",
    "    returns: an elastically distorted image of the same shape\n",
    "    '''\n",
    "    images = images.view(images.shape[0], -1).numpy()\n",
    "    assert len(images.shape) == 2\n",
    "\n",
    "    # the two lines below ensure we do not alter the array images\n",
    "    e_images = np.empty_like(images)\n",
    "    e_images[:] = images\n",
    "    \n",
    "    e_images = e_images.reshape(-1, height, width)\n",
    "    \n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "    x, y = np.mgrid[0:height, 0:width]\n",
    "    \n",
    "    for i in range(e_images.shape[0]):\n",
    "        \n",
    "        dx = gaussian_filter((random_state.rand(height, width) * 2 - 1), sigma, mode='constant') * alpha\n",
    "        dy = gaussian_filter((random_state.rand(height, width) * 2 - 1), sigma, mode='constant') * alpha\n",
    "        indices = x + dx, y + dy\n",
    "        e_images[i] = map_coordinates(e_images[i], indices, order=1)\n",
    "\n",
    "    return e_images.reshape(images.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train loss: 6485.63037109375\n",
      "Train acc: 0.924\n",
      "model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type nn_prototype. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 7349.41748046875\n",
      "Test acc: 0.9596\n",
      "\n",
      "Valid loss: 7173.83935546875\n",
      "Valid acc: 0.9664\n",
      "\n",
      "Epoch: 1\n",
      "Train loss: 6049.95361328125\n",
      "Train acc: 0.92\n",
      "\n",
      "Valid loss: 7171.302734375\n",
      "Valid acc: 0.9732\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 6215.80517578125\n",
      "Train acc: 0.932\n",
      "\n",
      "Valid loss: 7169.85400390625\n",
      "Valid acc: 0.9818\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 6541.84130859375\n",
      "Train acc: 0.968\n",
      "\n",
      "Valid loss: 7169.310546875\n",
      "Valid acc: 0.9826\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 6515.2421875\n",
      "Train acc: 0.964\n",
      "\n",
      "Valid loss: 7169.134765625\n",
      "Valid acc: 0.984\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 6579.7236328125\n",
      "Train acc: 0.964\n",
      "model is saved\n",
      "\n",
      "Test loss: 7344.2021484375\n",
      "Test acc: 0.9847\n",
      "\n",
      "Valid loss: 7168.82568359375\n",
      "Valid acc: 0.9864\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 6757.3330078125\n",
      "Train acc: 0.976\n",
      "\n",
      "Valid loss: 7168.74365234375\n",
      "Valid acc: 0.9854\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 6247.62109375\n",
      "Train acc: 0.96\n",
      "\n",
      "Valid loss: 7168.54248046875\n",
      "Valid acc: 0.988\n",
      "\n",
      "Epoch: 8\n",
      "Train loss: 6435.64306640625\n",
      "Train acc: 0.98\n",
      "\n",
      "Valid loss: 7168.58056640625\n",
      "Valid acc: 0.988\n",
      "\n",
      "Epoch: 9\n",
      "Train loss: 6446.1064453125\n",
      "Train acc: 0.988\n",
      "\n",
      "Valid loss: 7168.4375\n",
      "Valid acc: 0.988\n",
      "\n",
      "Epoch: 10\n",
      "Train loss: 6270.16552734375\n",
      "Train acc: 0.984\n",
      "model is saved\n",
      "\n",
      "Test loss: 7343.837890625\n",
      "Test acc: 0.986\n",
      "\n",
      "Valid loss: 7168.45703125\n",
      "Valid acc: 0.987\n",
      "\n",
      "Epoch: 11\n",
      "Train loss: 6460.09130859375\n",
      "Train acc: 0.98\n",
      "\n",
      "Valid loss: 7168.30810546875\n",
      "Valid acc: 0.9896\n",
      "\n",
      "Epoch: 12\n",
      "Train loss: 6553.431640625\n",
      "Train acc: 0.98\n",
      "\n",
      "Valid loss: 7168.29150390625\n",
      "Valid acc: 0.9892\n",
      "\n",
      "Epoch: 13\n",
      "Train loss: 6847.724609375\n",
      "Train acc: 0.98\n",
      "\n",
      "Valid loss: 7168.29638671875\n",
      "Valid acc: 0.9872\n",
      "\n",
      "Epoch: 14\n",
      "Train loss: 6341.3466796875\n",
      "Train acc: 0.968\n",
      "\n",
      "Valid loss: 7168.16357421875\n",
      "Valid acc: 0.9888\n",
      "\n",
      "Epoch: 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2c0b3024b61d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# compute and save accuracy and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fact/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def train():\n",
    "\n",
    "model = nn_prototype(15,4,10)\n",
    "batch_size_ = 250\n",
    "\n",
    "# get validation and test set\n",
    "valid_dl = DataLoader(valid_data, batch_size=5000, drop_last=False, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=10000, drop_last=False, shuffle=True)\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# initialize storage for results\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "valid_accs = []\n",
    "valid_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(training_epochs):\n",
    "    print(\"\\nEpoch:\", epoch)\n",
    "\n",
    "    # load the training data and reshuffle\n",
    "    train_dl = DataLoader(train_data, batch_size=batch_size_, drop_last=False, shuffle=True)\n",
    "\n",
    "    # loop over the batches\n",
    "    for step, (x, Y) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.view(x.shape[0], 1, x.shape[1], x.shape[2]).float()\n",
    "        x = torch.from_numpy(batch_elastic_transform(x, sigma=sigma, alpha=alpha, \n",
    "                                                     height=input_height, width=input_width))\n",
    "        \n",
    "        # perform forward pass\n",
    "        X_decoded, logits, feature_dist, prot_dist = model(x)\n",
    "\n",
    "        # compute the loss\n",
    "        total_loss = loss_function(X_decoded, x, logits, Y, feature_dist, prot_dist)\n",
    "\n",
    "        # backpropagate over the loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # compute and save accuracy and loss\n",
    "    train_accuracy = compute_acc(logits, Y)\n",
    "    train_accs.append(train_accuracy)\n",
    "    train_losses.append(total_loss.item())\n",
    "\n",
    "    # print information\n",
    "    print('Train loss:', total_loss.item())\n",
    "    print('Train acc:', train_accuracy)\n",
    "\n",
    "\n",
    "    if epoch % test_display_step == 0:\n",
    "        # save model and prototypes\n",
    "        torch.save(model, model_folder + \"/\" + model_filename)\n",
    "        print(\"model is saved\")\n",
    "\n",
    "        # perform testing\n",
    "        with torch.no_grad():\n",
    "            for step, (x_test, y_test) in enumerate(test_dl):\n",
    "                x_test = x_test.view(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2]).float()\n",
    "\n",
    "                # forward pass\n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_test)\n",
    "\n",
    "                # compute loss and accuracy and save\n",
    "                test_accuracy = compute_acc(logits, y_test)\n",
    "                test_loss = loss_function(X_decoded, x_test, logits, y_test, feature_dist, prot_dist)\n",
    "                test_accs.append(test_accuracy)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "            print('\\nTest loss:', test_loss.item())\n",
    "            print('Test acc:', test_accuracy)\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        for step, (x_valid, y_valid) in enumerate(valid_dl):\n",
    "                x_valid = x_valid.view(x_valid.shape[0], 1, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "                X_decoded, logits, feature_dist, prot_dist = model(x_valid)\n",
    "\n",
    "                # compute losses and accuracy and save\n",
    "                valid_accuracy = compute_acc(logits, y_valid)\n",
    "                valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist)\n",
    "                valid_accs.append(valid_accuracy)\n",
    "                valid_losses.append(valid_loss)\n",
    "\n",
    "        print('\\nValid loss:', valid_loss.item())\n",
    "        print('Valid acc:', valid_accuracy)\n",
    "\n",
    "#return train_accs, train_losses, valid_accs, valid_losses, test_accs, test_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and visualize prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prototypes(model, epoch, save=True):\n",
    "    # get saved prototypes\n",
    "    encoded_prototypes = model.prototype_feature_vectors\n",
    "    encoded_prototypes_reshaped = encoded_prototypes.view(n_prototypes, 10, 2, 2)\n",
    "\n",
    "    # decode prototypes\n",
    "    decoded_prototypes = model.decoder(encoded_prototypes_reshaped).squeeze().detach().numpy()\n",
    "\n",
    "    for i in range(n_prototypes):\n",
    "        plt.imshow(decoded_prototypes[i])\n",
    "        if save:\n",
    "            plt.savefig(img_folder+\"/prototype\"+ str(epoch)+\"_\"+str(i)+\".png\")\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "loaded_model = torch.load(model_folder+\"/\"+model_filename)\n",
    "\n",
    "with torch.no_grad():\n",
    "        for step, (x_valid, y_valid) in enumerate(valid_dl):\n",
    "                x_valid = x_valid.view(x_valid.shape[0], 1, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "                X_decoded, logits, feature_dist, prot_dist = loaded_model(x_valid)\n",
    "\n",
    "                # compute losses and accuracy and save\n",
    "                valid_accuracy = compute_acc(logits, y_valid)\n",
    "                valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist)\n",
    "                #valid_accs.append(valid_accuracy)\n",
    "                #valid_losses.append(valid_loss)\n",
    "\n",
    "        print('\\nValid loss:', valid_loss.item())\n",
    "        print('Valid acc:', valid_accuracy)\n",
    "\n",
    "visualize_prototypes(loaded_model, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADhxJREFUeJzt3WGMHPV5x/Hfz+ezTWxQbVKMMY5NCYqCUHGSk1OFCLlFpARFMlQKxS9SU1GcSpA0Ei+K/AaUKhWqalKStlGd4sQ0CQkKcXEVKwFZlUgKGM5gBROnhVoHOLZsqF2ME2yf756+uDG9mNvZ9e7szp6f70dCtzvP7M7Dyr+d3f3PzN8RIQD5zKi7AQD1IPxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ka2cuNzfLsmKO5vdwkkMox/Uon4rhbWbej8Nu+TtL9kgYk/XNE3Fu2/hzN1Ud9TSebBFBie2xred22P/bbHpD0D5I+KelySattX97u8wHorU6+86+Q9HJE7ImIE5K+K2lVNW0B6LZOwr9Y0muT7u8tlv0G22ttD9seHtXxDjYHoEqdhH+qHxXedX5wRGyIiKGIGBrU7A42B6BKnYR/r6Qlk+5fLGlfZ+0A6JVOwv+spMtsX2J7lqSbJW2ppi0A3db2UF9EnLR9h6Qfa2Kob2NEvFhZZ6jEzEuWltZPjrxa/gR1XunJTYaruQpVRzoa54+IrZK2VtQLgB7i8F4gKcIPJEX4gaQIP5AU4QeSIvxAUj09nx/d8ebW9zes/ceVD5c+dsDdff//6uHGxxl8bv4rHT330fFjpfXrb/98w9o5jz7T0bbPBuz5gaQIP5AU4QeSIvxAUoQfSIrwA0k5enha5HleEBmv3nvwjo+V1i/aPFJa/9bT3y+tzx94z5m2lN4fXrS87ha6Ynts05E41NKlu9nzA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSjPNX4JG9T5fW582Y06NO0Kqnj42V1u++dKj8Cfr0suGM8wNoivADSRF+ICnCDyRF+IGkCD+QFOEHkuro0t22RyS9JWlM0smIaDI4enb63/GTpfV5vMX2naUz3y5foU/H8atUxXX7fz8i3qjgeQD0EPskIKlOwx+SHrO9w/baKhoC0Budfuy/KiL22b5A0uO2fxERT0xeoXhTWCtJc8S15oB+0dGePyL2FX8PStosacUU62yIiKGIGBrU7E42B6BCbYff9lzb5566LekTknZV1RiA7urkY/9CSZttn3qe70TEjyrpCkDXtR3+iNgj6coKe5m2Lp45r+4WcIYG3NIp72c1hvqApAg/kBThB5Ii/EBShB9IivADSVVxVl8ODA2dVUZOzqq7hdqx5weSIvxAUoQfSIrwA0kRfiApwg8kRfiBpBjnb9GPf/l83S2gQqMxUHcLtWPPDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJMc7fouuWvmsyonf86JVnetgJqvCl5SubrHGkF23Uij0/kBThB5Ii/EBShB9IivADSRF+ICnCDyTVdJzf9kZJn5J0MCKuKJYtkPQ9ScskjUi6KSIOd6/N+u354kdKqozzTzfjbx+ru4XatbLn/6ak605bdpekbRFxmaRtxX0A00jT8EfEE5IOnbZ4laRNxe1Nkm6ouC8AXdbud/6FEbFfkoq/F1TXEoBe6Pqx/bbXSlorSXP0nm5vDkCL2t3zH7C9SJKKvwcbrRgRGyJiKCKGBjW7zc0BqFq74d8iaU1xe42kR6tpB0CvNA2/7YckPSXpA7b32r5V0r2SrrX9kqRri/sAppGm3/kjYnWD0jUV99LXXvyTvy+pcg34fvTM8dGGtRg90cNO+hNH+AFJEX4gKcIPJEX4gaQIP5AU4QeS4tLdhYGF5acnDJrhvH4zFuOl9du/+PmGtQV6qup2ph32/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOP8hbEDDS9GhGnq/G/taFiLHvbRr9jzA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSjPOfMoPz9aebkxorX6HJ+f7ZsecHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaSajvPb3ijpU5IORsQVxbJ7JN0m6fVitXURsbVbTfbEePmY8Z37P9ywtn7Rc1V3gxZ8481lpfUYa3IcQHKt7Pm/Kem6KZZ/OSKWF/9N7+ADCTUNf0Q8IelQD3oB0EOdfOe/w/bPbG+0Pb+yjgD0RLvh/5qkSyUtl7Rf0vpGK9pea3vY9vCojre5OQBVayv8EXEgIsYiYlzS1yWtKFl3Q0QMRcTQoGa32yeAirUVftuLJt29UdKuatoB0CutDPU9JGmlpPfa3ivpbkkrbS/XxBWQRyR9tos9AuiCpuGPiNVTLH6gC730tV0faXxu+PXn/0HpYx94fktpfdHMeW31dLYbjfJx+q/+y6rS+pJZJdftP87vTxzhByRF+IGkCD+QFOEHkiL8QFKEH0iKS3e3aObSJQ1rW558tPSxA2Yorx2DLr+c+lf+7J9K63/9zC0NazO3NR4GzII9P5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kxTj/KXZp+YdP/VtJlffQOlw950Rp/a73zWpYW1B1M9MQ/2qBpAg/kBThB5Ii/EBShB9IivADSRF+ICnG+Qszzjmn7hZQsTc/0LjGOD97fiAtwg8kRfiBpAg/kBThB5Ii/EBShB9Iquk4v+0lkh6UdKGkcUkbIuJ+2wskfU/SMkkjkm6KiMPda7W7Xvqr322yxpM96QPVOf/Kg3W30Nda2fOflHRnRHxQ0u9Jut325ZLukrQtIi6TtK24D2CaaBr+iNgfEc8Vt9+StFvSYkmrJG0qVtsk6YZuNQmgemf0nd/2MkkfkrRd0sKI2C9NvEFIuqDq5gB0T8vhtz1P0iOSvhARR87gcWttD9seHtXxdnoE0AUthd/2oCaC/+2I+EGx+IDtRUV9kaQpf12JiA0RMRQRQ4OaXUXPACrQNPy2LekBSbsj4r5JpS2S1hS310gqn6oWQF9p5ZTeqyR9RtILtncWy9ZJulfSw7ZvlfSqpE93p8XeeP+658tXWN2bPtC6GSq/3PqxHy5sWDtP/111O9NO0/BHxE+lhq/yNdW2A6BXOMIPSIrwA0kRfiApwg8kRfiBpAg/kBSX7j5lPErLYzHesDZg3kO7YTTGSutHx8sPF7/o+43H8k+21dHZhX+1QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4/yFGD1RWv/cvo81rP3j4qerbgeS3hh7u7T+p3/056X1OPBile2cddjzA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSjPOf4vJrwO9cv7xh7fj6n5Q+drYH22rpbHd0/Fhp/cZdt5TW5/98T2m9/AoNYM8PJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0k5onw01PYSSQ9KulDSuKQNEXG/7Xsk3Sbp9WLVdRGxtey5zvOC+KjPvlm9Z8yZU1o/57FzS+tfWba5tP5bM8oPx5g3o3z7nTgeo6X1A2Pl187/xYn5DWv33fzHpY+NHU3Ox2/ybzej7bFNR+JQ+UErhVYO8jkp6c6IeM72uZJ22H68qH05Iv623UYB1Kdp+CNiv6T9xe23bO+WtLjbjQHorjP6zm97maQPSdpeLLrD9s9sb7Q95ec722ttD9seHlX5R0QAvdNy+G3Pk/SIpC9ExBFJX5N0qaTlmvhksH6qx0XEhogYioihQc2uoGUAVWgp/LYHNRH8b0fEDyQpIg5ExFhEjEv6uqQV3WsTQNWaht+2JT0gaXdE3Ddp+aJJq90oaVf17QHollZ+7b9K0mckvWB7Z7FsnaTVtpdr4szJEUmf7UqH08D4sfJTU391dXn9Vn28o+0PzG88nPbabR8sfeyFT/66tD7rtf8prcevy//fYvFvN67tZH9Rp1Z+7f+ppKnGDUvH9AH0N47wA5Ii/EBShB9IivADSRF+ICnCDyTFpbvPAmOHDzesLfnGS6WPHS95rCSNDQyU1uNE+dTmM2c2fvx46SPRbez5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpppfurnRj9uuSXpm06L2S3uhZA2emX3vr174kemtXlb0tjYjGF1GYpKfhf9fG7eGIGKqtgRL92lu/9iXRW7vq6o2P/UBShB9Iqu7wb6h5+2X6tbd+7Uuit3bV0lut3/kB1KfuPT+AmtQSftvX2f5P2y/bvquOHhqxPWL7Bds7bQ/X3MtG2wdt75q0bIHtx22/VPxtfN3u3vd2j+1fFq/dTtvX19TbEtv/bnu37Rdt/0WxvNbXrqSvWl63nn/stz0g6b8kXStpr6RnJa2OiJ/3tJEGbI9IGoqI2seEbV8t6aikByPiimLZ30g6FBH3Fm+c8yPiL/ukt3skHa175uZiQplFk2eWlnSDpFtU42tX0tdNquF1q2PPv0LSyxGxJyJOSPqupFU19NH3IuIJSYdOW7xK0qbi9iZN/OPpuQa99YWI2B8RzxW335J0ambpWl+7kr5qUUf4F0t6bdL9veqvKb9D0mO2d9heW3czU1hYTJt+avr0C2ru53RNZ27updNmlu6b166dGa+rVkf4p5r9p5+GHK6KiA9L+qSk24uPt2hNSzM398oUM0v3hXZnvK5aHeHfK2nJpPsXS9pXQx9Tioh9xd+Dkjar/2YfPnBqktTi78Ga+3lHP83cPNXM0uqD166fZryuI/zPSrrM9iW2Z0m6WdKWGvp4F9tzix9iZHuupE+o/2Yf3iJpTXF7jaRHa+zlN/TLzM2NZpZWza9dv814XctBPsVQxt9JGpC0MSK+1PMmpmD7dzSxt5cmrmz8nTp7s/2QpJWaOOvrgKS7Jf2rpIclvU/Sq5I+HRE9/+GtQW8rNfHR9Z2Zm099x+5xbx+X9BNJL+j/LxK8ThPfr2t77Ur6Wq0aXjeO8AOS4gg/ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJ/R/b6fo5TakL2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_prototypes(model, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting of losses and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
