{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Helper functions borrowed from original paper by Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def list_of_distances(X, Y):\n",
    "    '''\n",
    "    Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "    Y = [y_1, ... , y_m], we return a list of vectors\n",
    "            [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "             ...\n",
    "             [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "    where the distance metric used is the sqared euclidean distance.\n",
    "    The computation is achieved through a clever use of broadcasting.\n",
    "    '''\n",
    "    XX = torch.reshape(list_of_norms(X), shape=(-1, 1))\n",
    "    YY = torch.reshape(list_of_norms(Y), shape=(1, -1))\n",
    "    output = XX + YY - 2 * torch.mm(X, torch.transpose(Y))\n",
    "\n",
    "    return output\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return tf.reduce_sum(np.pow(X, 2), axis=1)\n",
    "\n",
    "def print_and_write(str, file):\n",
    "    '''\n",
    "    print str to the console and also write it to file\n",
    "    '''\n",
    "    print(str)\n",
    "    file.write(str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "makedirs('./data/mnist')\n",
    "\n",
    "# Models folder\n",
    "model_folder = os.path.join(os.getcwd(), \"saved_model\", \"mnist_model\", \"mnist_cae_1\")\n",
    "makedirs(model_folder)\n",
    "\n",
    "# Image folder\n",
    "img_folder = os.path.join(model_folder, \"img\")\n",
    "makedirs(img_folder)\n",
    "\n",
    "# Model filename\n",
    "model_filename = \"mnist_cae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Pytorch\n",
    "#### <font color='red'>Double check the normalization mean and stdev for dataset</font>\n",
    "#### <font color='red'>Double check parameters Dataloader (e.g. shuffle on or off, different batch sizes for train/valid/test)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms to perform on loaded dataset. Normalize around mean 0.1307 and std 0.3081 for optimal pytorch results. \n",
    "# source: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "# Load datasets into reproduction/data/mnist. Download if data not present. \n",
    "mnist_train = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms))\n",
    "\n",
    "mnist_train_data = mnist_train.dataset.data\n",
    "mnist_train_targets = mnist_train.dataset.targets\n",
    "\n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train_data[0:55000]\n",
    "y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train_data[55000:60000]\n",
    "y_valid = mnist_train_targets[55000:60000]\n",
    "\n",
    "# 10000 examples in test set\n",
    "\n",
    "mnist_test = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=False, download=True, transform=transforms))\n",
    "\n",
    "x_test = mnist_test.dataset.data\n",
    "y_test = mnist_test.dataset.targets\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "# Datasets in DataLoader, can be used in iteration: for (x, y) in train_dl...\n",
    "# Check parameters\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "valid_dl = DataLoader(valid_data, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM THE ORIGINAL IMPLEMENTATION\n",
    "# training parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1500\n",
    "\n",
    "# frequency of testing and saving\n",
    "test_display_step = 100   # how many epochs we do evaluate on the test set once\n",
    "save_step = 50            # how frequently do we save the model to disk\n",
    "\n",
    "# elastic deformation parameters\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "\n",
    "# lambda's are the ratios between the four error terms\n",
    "lambda_class = 20\n",
    "lambda_ae = 1 # autoencoder\n",
    "lambda_1 = 1 # push prototype vectors to have meaningful decodings in pixel space\n",
    "lambda_2 = 1 # cluster training examples around prototypes in latent space\n",
    "\n",
    "\n",
    "input_height = input_width =  28    # MNIST data input shape \n",
    "n_input_channel = 1     # the number of color channels; for MNIST is 1.\n",
    "input_size = input_height * input_width * n_input_channel   # 784\n",
    "n_classes = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_prototypes = 15         # the number of prototypes\n",
    "n_layers = 4\n",
    "\n",
    "# height and width of each layers' filters\n",
    "f_1 = 3\n",
    "f_2 = 3\n",
    "f_3 = 3\n",
    "f_4 = 3\n",
    "\n",
    "# stride size in each direction for each of the layers\n",
    "s_1 = 2\n",
    "s_2 = 2\n",
    "s_3 = 2\n",
    "s_4 = 2\n",
    "\n",
    "# number of feature maps in each layer\n",
    "n_map_1 = 32\n",
    "n_map_2 = 32\n",
    "n_map_3 = 32\n",
    "n_map_4 = 10\n",
    "\n",
    "# the shapes of each layer's filter\n",
    "# [out channel, in_channel, 3, 3]\n",
    "filter_shape_1 = [n_map_1, n_input_channel, f_1, f_1]\n",
    "filter_shape_2 = [n_map_2, n_map_1, f_2, f_2]\n",
    "filter_shape_3 = [n_map_3, n_map_2, f_3, f_3]\n",
    "filter_shape_4 = [n_map_4, n_map_3, f_4, f_4]\n",
    "\n",
    "# strides for each layer (changed to tuples)\n",
    "stride_1 = [s_1, s_1]\n",
    "stride_2 = [s_2, s_2]\n",
    "stride_3 = [s_3, s_3]\n",
    "stride_4 = [s_4, s_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_weights = 0.01\n",
    "\n",
    "weights = {\n",
    "    'enc_f1': nn.Parameter(std_weights * torch.randn(filter_shape_1,\n",
    "                                           dtype=torch.float32)),\n",
    "    'enc_f2': nn.Parameter(std_weights * torch.randn(filter_shape_2,\n",
    "                                           dtype=torch.float32)), \n",
    "    'enc_f3': nn.Parameter(std_weights * torch.randn(filter_shape_3,\n",
    "                                           dtype=torch.float32)), \n",
    "    'enc_f4': nn.Parameter(std_weights * torch.randn(filter_shape_4,\n",
    "                                           dtype=torch.float32)), \n",
    "    'dec_f4': nn.Parameter(std_weights * torch.randn(filter_shape_4,\n",
    "                                           dtype=torch.float32)), \n",
    "    'dec_f3': nn.Parameter(std_weights * torch.randn(filter_shape_3,\n",
    "                                           dtype=torch.float32)), \n",
    "    'dec_f2': nn.Parameter(std_weights * torch.randn(filter_shape_2,\n",
    "                                           dtype=torch.float32)),\n",
    "    'dec_f1': nn.Parameter(std_weights * torch.randn(filter_shape_1,\n",
    "                                           dtype=torch.float32)),\n",
    "}\n",
    "\n",
    "\n",
    "biases = {\n",
    "    'enc_b1': nn.Parameter(torch.zeros([n_map_1], dtype=torch.float32)),\n",
    "    'enc_b2': nn.Parameter(torch.zeros([n_map_2], dtype=torch.float32)),\n",
    "    'enc_b3': nn.Parameter(torch.zeros([n_map_3], dtype=torch.float32)),\n",
    "    'enc_b4': nn.Parameter(torch.zeros([n_map_4], dtype=torch.float32)),\n",
    "    'dec_b4': nn.Parameter(torch.zeros([n_map_3], dtype=torch.float32)),\n",
    "    'dec_b3': nn.Parameter(torch.zeros([n_map_2], dtype=torch.float32)),\n",
    "    'dec_b2': nn.Parameter(torch.zeros([n_map_1], dtype=torch.float32)),\n",
    "    'dec_b1': nn.Parameter(torch.zeros([n_input_channel], dtype=torch.float32)),\n",
    "}\n",
    "\n",
    "last_layer = {\n",
    "    'w': nn.Parameter(torch.randn([n_prototypes, n_classes],\n",
    "                                       dtype=torch.float32))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print shapes of all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "enc_f1 torch.Size([32, 1, 3, 3])\n",
      "enc_f2 torch.Size([32, 32, 3, 3])\n",
      "enc_f3 torch.Size([32, 32, 3, 3])\n",
      "enc_f4 torch.Size([10, 32, 3, 3])\n",
      "dec_f4 torch.Size([10, 32, 3, 3])\n",
      "dec_f3 torch.Size([32, 32, 3, 3])\n",
      "dec_f2 torch.Size([32, 32, 3, 3])\n",
      "dec_f1 torch.Size([32, 1, 3, 3])\n",
      "biases\n",
      "enc_b1 torch.Size([32])\n",
      "enc_b2 torch.Size([32])\n",
      "enc_b3 torch.Size([32])\n",
      "enc_b4 torch.Size([10])\n",
      "dec_b4 torch.Size([32])\n",
      "dec_b3 torch.Size([32])\n",
      "dec_b2 torch.Size([32])\n",
      "dec_b1 torch.Size([1])\n",
      "last_layer\n",
      "torch.Size([15, 10])\n"
     ]
    }
   ],
   "source": [
    "# Printing shapes of all parameters\n",
    "print(\"weights\")\n",
    "for weight in weights.keys():\n",
    "    print(weight, weights[weight].shape)\n",
    "print(\"biases\")\n",
    "for b in biases.keys():\n",
    "    print(b, biases[b].shape)\n",
    "print(\"last_layer\")\n",
    "print(last_layer['w'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer functions\n",
    "#### <font color='red'>Fix the stride and padding parameters, check if filter in tf is same as weight in pt</font>\n",
    "Padding discussion pytorch: https://github.com/pytorch/pytorch/issues/3867\n",
    "\n",
    "Blogpost: https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python\n",
    "\n",
    "### Padding helper\n",
    "Based on blogpost above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_img(img):\n",
    "    ''' Takes an input image (batch) and pads according to Tensorflows SAME padding'''\n",
    "    input_h = img.shape[2]\n",
    "    input_w = img.shape[3]\n",
    "    stride = 2 \n",
    "    filter_h = 3\n",
    "    filter_w = 3\n",
    "    \n",
    "    output_h = int(ceil(float(input_h)) / float(stride))\n",
    "    output_w = output_h\n",
    "    \n",
    "    if input_h % stride == 0:\n",
    "        pad_height = max((filter_h - stride), 0)\n",
    "    else:\n",
    "        pad_height = max((filter_h - (input_h % stride), 0))\n",
    "\n",
    "    pad_width = pad_height\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    padded_img = torch.zeros(img.shape[0], img.shape[1], input_h + pad_height, input_w + pad_width)\n",
    "    padded_img[:,:, pad_top:-pad_bottom, pad_left:-pad_right] = img\n",
    "\n",
    "    return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(img, kernel, bias, strides, padding=\"VALID\", nonlinearity = nn.ReLU()):\n",
    "    img = pad_img(img)\n",
    "    conv = F.conv2d(img, kernel, bias=bias, stride=strides, padding=padding)\n",
    "    out = nonlinearity(conv)\n",
    "    return out\n",
    "\n",
    "#### stride must be tuple for torch, is a list in tf\n",
    "#### padding is different, tf uses same/valid, torch a int or list of ints\n",
    "#### is the filter the same ass weights argument for conv2d?\n",
    "\n",
    "# tensorflow's conv2d_transpose needs to know the shape of the output\n",
    "def deconv_layer(img, kernel, bias, strides, padding=\"VALID\", nonlinearity=nn.ReLU(), out_padding = None):\n",
    "#     img = pad_img(img)\n",
    "    deconv = F.conv_transpose2d(img, kernel, bias=bias, stride=strides, padding=padding, output_padding=out_padding)\n",
    "    out = nonlinearity(deconv)\n",
    "    return out\n",
    "\n",
    "# def deconv_layer(img, kernel, bias, strides, padding=\"VALID\", nonlinearity=nn.ReLU(), out_size = None):\n",
    "#     in_channel = img.shape[1]\n",
    "#     out_channel = kernel.shape[1]\n",
    "#     k_size = (kernel.shape[2], kernel.shape[3])\n",
    "    \n",
    "#     img = pad_img(img)\n",
    "    \n",
    "#     upsample = nn.ConvTranspose2d(in_channel, out_channel, k_size, strides, padding=(0,0))\n",
    "    \n",
    "#     out = upsample(img, output_size=out_size)\n",
    "    \n",
    "\n",
    "def fc_layer(input, weight, bias, nonlinearity=nn.ReLU()):\n",
    "    return nonlinearity(torch.mm(input, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Dummy tensor for debugging\n",
    "# X = torch.empty(batch_size, n_input_channel, input_width, input_height)\n",
    "X,Y = next(iter(train_dl))\n",
    "X = X.view(250,1,28,28).float()\n",
    "# X in shape 250x28x28 (Batch x H x W)\n",
    "print(X.shape)\n",
    "# Y in shape 250 (B). Needs to be converted to one-hot for reproduction of paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  torch.Size([250, 1, 28, 28])\n",
      "EL1:  torch.Size([250, 32, 14, 14])\n",
      "EL2:  torch.Size([250, 32, 7, 7])\n",
      "EL3:  torch.Size([250, 32, 4, 4])\n",
      "EL4:  torch.Size([250, 10, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "PADDING_FLAG = (0,0)\n",
    "# eln means the output of the nth layer of the encoder\n",
    "print('X: ', X.shape)\n",
    "el1 = conv_layer(X, weights['enc_f1'], biases['enc_b1'], stride_1, padding=PADDING_FLAG)\n",
    "print('EL1: ',el1.shape)\n",
    "el2 = conv_layer(el1, weights['enc_f2'], biases['enc_b2'], stride_2, padding=PADDING_FLAG)\n",
    "print('EL2: ',el2.shape)\n",
    "el3 = conv_layer(el2, weights['enc_f3'], biases['enc_b3'], stride_3, padding=PADDING_FLAG)\n",
    "print('EL3: ',el3.shape)\n",
    "el4 = conv_layer(el3, weights['enc_f4'], biases['enc_b4'], stride_4, padding=PADDING_FLAG)\n",
    "print('EL4: ',el4.shape)\n",
    "\n",
    "input_shape = list(X.shape)\n",
    "l1_shape = list(el1.shape)\n",
    "l2_shape = list(el2.shape)\n",
    "l3_shape = list(el3.shape)\n",
    "l4_shape = list(el4.shape)\n",
    "\n",
    "flatten_size = l4_shape[1] * l4_shape[2] * l4_shape[3]\n",
    "n_features = flatten_size\n",
    "\n",
    "# feature vectors is the flattened output of the encoder\n",
    "feature_vectors = torch.reshape(el4, shape=[-1, flatten_size])\n",
    "#print(feature_vectors.shape)\n",
    "# initialize the prototype feature vectors\n",
    "prototype_feature_vectors = nn.Parameter(torch.empty(size=\n",
    "                                        [n_prototypes, n_features],\n",
    "                                        dtype=torch.float32).uniform_())\n",
    "\n",
    "deconv_batch_size = torch.eye(feature_vectors.shape[0])\n",
    "# this is necessary for prototype images evaluation\n",
    "reshape_feature_vectors = torch.reshape(feature_vectors, shape=[-1, l4_shape[1],\n",
    "   l4_shape[2], l4_shape[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape_feature_vectors : torch.Size([250, 10, 2, 2])\n",
      "torch.Size([250, 10, 3, 3])\n",
      "DL4:  torch.Size([250, 32, 4, 4])\n",
      "DL3:  torch.Size([250, 32, 5, 5])\n",
      "DL2:  torch.Size([250, 32, 9, 9])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv_transpose2d(): argument 'output_padding' must be tuple of ints, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-25f230684525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m dl1 = deconv_layer(dl2, weights['dec_f1'], biases['dec_b1'],\n\u001b[1;32m     19\u001b[0m                    \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPADDING_FLAG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                    nonlinearity=nn.Sigmoid())\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# want to dl4 to have shape 250 x 1 x 28 x 28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DL1: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-7ff6cd3c3dd7>\u001b[0m in \u001b[0;36mdeconv_layer\u001b[0;34m(img, kernel, bias, strides, padding, nonlinearity, out_padding)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"VALID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_padding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     img = pad_img(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdeconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_transpose2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_padding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_padding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv_transpose2d(): argument 'output_padding' must be tuple of ints, not NoneType"
     ]
    }
   ],
   "source": [
    "print('Reshape_feature_vectors :', reshape_feature_vectors.shape)\n",
    "print(pad_img(reshape_feature_vectors).shape)\n",
    "dl4 = deconv_layer(reshape_feature_vectors, weights['dec_f4'], biases['dec_b4'],\n",
    "                   strides=stride_4, padding=1, out_padding = 1)\n",
    "# want to dl4 to have shape 250 x 32 x 4 x 4\n",
    "print('DL4: ',dl4.shape)\n",
    "\n",
    "dl3 = deconv_layer(dl4, weights['dec_f3'], biases['dec_b3'],\n",
    "                   strides=stride_3, padding=2, out_padding=0)\n",
    "# want to dl3 to have shape 250 x 32 x 7 x 7\n",
    "print('DL3: ',dl3.shape)\n",
    "\n",
    "dl2 = deconv_layer(dl3, weights['dec_f2'], biases['dec_b2'],\n",
    "                   strides=stride_2, padding=1, out_padding=0)\n",
    "# want to dl2 to have shape 250 x 32 x 14 x 14\n",
    "print('DL2: ',dl2.shape)\n",
    "\n",
    "dl1 = deconv_layer(dl2, weights['dec_f1'], biases['dec_b1'],\n",
    "                   strides=stride_1, padding=PADDING_FLAG,\n",
    "                   nonlinearity=nn.Sigmoid())\n",
    "# want to dl4 to have shape 250 x 1 x 28 x 28\n",
    "print('DL1: ',dl1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 784]' is invalid for input of size 72250",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ae86e6d0bf75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m '''\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 784]' is invalid for input of size 72250"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X_decoded is the decoding of the encoded feature vectors in X;\n",
    "we reshape it to match the shape of the training input\n",
    "X_true is the correct output for the autoencoder\n",
    "'''\n",
    "\n",
    "X_decoded = torch.reshape(dl1, shape=(-1, input_size))\n",
    "X_true = torch.eye(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b4b54edb26ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m '''\n\u001b[1;32m      7\u001b[0m prototype_distances = list_of_distances(feature_vectors,\n\u001b[0;32m----> 8\u001b[0;31m                                         prototype_feature_vectors)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprototype_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototype_distances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m feature_vector_distances = list_of_distances(prototype_feature_vectors,\n",
      "\u001b[0;32m<ipython-input-2-e0a689fd873b>\u001b[0m in \u001b[0;36mlist_of_distances\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mcomputation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0machieved\u001b[0m \u001b[0mthrough\u001b[0m \u001b[0ma\u001b[0m \u001b[0mclever\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbroadcasting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     '''\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mXX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mYY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mYY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e0a689fd873b>\u001b[0m in \u001b[0;36mlist_of_norms\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfunction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msquared\u001b[0m \u001b[0meuclidean\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     '''\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_and_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "prototype_distances is the list of distances from each x_i to every prototype\n",
    "in the latent space\n",
    "feature_vector_distances is the list of distances from each prototype to every x_i\n",
    "in the latent space\n",
    "'''\n",
    "prototype_distances = list_of_distances(feature_vectors,\n",
    "                                        prototype_feature_vectors)\n",
    "prototype_distances = torch.eye(prototype_distances)\n",
    "feature_vector_distances = list_of_distances(prototype_feature_vectors,\n",
    "                                             feature_vectors)\n",
    "feature_vector_distances = torch.eye(feature_vector_distances)\n",
    "\n",
    "# the logits are the weighted sum of distances from prototype_distances\n",
    "logits = torch.mm(prototype_distances, last_layer['w'])\n",
    "probability_distribution = F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
