{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Helper functions borrowed from original paper by Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    '''\n",
    "    if path does not exist in the file system, create it\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def list_of_distances(X, Y):\n",
    "    '''\n",
    "    Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "    Y = [y_1, ... , y_m], we return a list of vectors\n",
    "            [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "             ...\n",
    "             [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "    where the distance metric used is the sqared euclidean distance.\n",
    "    The computation is achieved through a clever use of broadcasting.\n",
    "    '''\n",
    "    XX = torch.reshape(list_of_norms(X), shape=(-1, 1))\n",
    "    YY = torch.reshape(list_of_norms(Y), shape=(1, -1))\n",
    "    output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "    return output\n",
    "\n",
    "def list_of_norms(X):\n",
    "    '''\n",
    "    X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "        [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "    function is the squared euclidean distance.\n",
    "    '''\n",
    "    return torch.sum(torch.pow(X, 2), dim=1)\n",
    "\n",
    "def print_and_write(str, file):\n",
    "    '''\n",
    "    print str to the console and also write it to file\n",
    "    '''\n",
    "    print(str)\n",
    "    file.write(str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "makedirs('./data/mnist')\n",
    "\n",
    "# Models folder\n",
    "model_folder = os.path.join(os.getcwd(), \"saved_model\", \"mnist_model\", \"mnist_cae_1\")\n",
    "makedirs(model_folder)\n",
    "\n",
    "# Image folder\n",
    "img_folder = os.path.join(model_folder, \"img\")\n",
    "makedirs(img_folder)\n",
    "\n",
    "# Model filename\n",
    "model_filename = \"mnist_cae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Pytorch\n",
    "#### <font color='red'>Double check the normalization mean and stdev for dataset</font>\n",
    "#### <font color='red'>Double check parameters Dataloader (e.g. shuffle on or off, different batch sizes for train/valid/test)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Transforms to perform on loaded dataset. Normalize around mean 0.1307 and std 0.3081 for optimal pytorch results. \n",
    "# source: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "# Load datasets into reproduction/data/mnist. Download if data not present. \n",
    "mnist_train = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms))\n",
    "\n",
    "mnist_train_data = mnist_train.dataset.data\n",
    "mnist_train_targets = mnist_train.dataset.targets\n",
    "\n",
    "# first 55000 examples for training\n",
    "x_train = mnist_train_data[0:55000]\n",
    "y_train = mnist_train_targets[0:55000]\n",
    "\n",
    "# 5000 examples for validation set\n",
    "x_valid = mnist_train_data[55000:60000]\n",
    "y_valid = mnist_train_targets[55000:60000]\n",
    "\n",
    "# 10000 examples in test set\n",
    "\n",
    "mnist_test = DataLoader(torchvision.datasets.MNIST('./data/mnist', train=False, download=True, \n",
    "                                                   transform=transforms))\n",
    "\n",
    "x_test = mnist_test.dataset.data\n",
    "y_test = mnist_test.dataset.targets\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "valid_data = TensorDataset(x_valid, y_valid)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "# Datasets in DataLoader, can be used in iteration: for (x, y) in train_dl...\n",
    "# Check parameters\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "valid_dl = DataLoader(valid_data, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM THE ORIGINAL IMPLEMENTATION\n",
    "# training parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 1500\n",
    "\n",
    "# frequency of testing and saving\n",
    "test_display_step = 100   # how many epochs we do evaluate on the test set once\n",
    "save_step = 50            # how frequently do we save the model to disk\n",
    "\n",
    "# elastic deformation parameters\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "\n",
    "# lambda's are the ratios between the four error terms\n",
    "lambda_class = 20\n",
    "lambda_ae = 1 # autoencoder\n",
    "lambda_1 = 1 # push prototype vectors to have meaningful decodings in pixel space\n",
    "lambda_2 = 1 # cluster training examples around prototypes in latent space\n",
    "\n",
    "\n",
    "input_height = input_width =  28    # MNIST data input shape \n",
    "n_input_channel = 1     # the number of color channels; for MNIST is 1.\n",
    "input_size = input_height * input_width * n_input_channel   # 784\n",
    "n_classes = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_prototypes = 15         # the number of prototypes\n",
    "n_layers = 4\n",
    "\n",
    "# height and width of each layers' filters\n",
    "f_1 = 3\n",
    "f_2 = 3\n",
    "f_3 = 3\n",
    "f_4 = 3\n",
    "\n",
    "# stride size in each direction for each of the layers\n",
    "s_1 = 2\n",
    "s_2 = 2\n",
    "s_3 = 2\n",
    "s_4 = 2\n",
    "\n",
    "# number of feature maps in each layer\n",
    "n_map_1 = 32\n",
    "n_map_2 = 32\n",
    "n_map_3 = 32\n",
    "n_map_4 = 10\n",
    "\n",
    "# the shapes of each layer's filter\n",
    "# [out channel, in_channel, 3, 3]\n",
    "filter_shape_1 = [n_map_1, n_input_channel, f_1, f_1]\n",
    "filter_shape_2 = [n_map_2, n_map_1, f_2, f_2]\n",
    "filter_shape_3 = [n_map_3, n_map_2, f_3, f_3]\n",
    "filter_shape_4 = [n_map_4, n_map_3, f_4, f_4]\n",
    "\n",
    "# strides for each layer (changed to tuples)\n",
    "stride_1 = [s_1, s_1]\n",
    "stride_2 = [s_2, s_2]\n",
    "stride_3 = [s_3, s_3]\n",
    "stride_4 = [s_4, s_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Dummy tensor for debugging\n",
    "# X = torch.empty(batch_size, n_input_channel, input_width, input_height)\n",
    "X,Y = next(iter(train_dl))\n",
    "X = X.view(250,1,28,28).float()\n",
    "# X in shape 250x28x28 (Batch x H x W)\n",
    "print(X.shape)\n",
    "# Y in shape 250 (B). Needs to be converted to one-hot for reproduction of paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "#### <font color='red'>Fix the stride and padding parameters, check if filter in tf is same as weight in pt</font>\n",
    "Padding discussion pytorch: https://github.com/pytorch/pytorch/issues/3867\n",
    "\n",
    "Blogpost: https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder'''\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "        \n",
    "        # define layers\n",
    "        self.enc_l1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l4 = nn.Conv2d(32, 10, kernel_size=3, stride=2, padding=0)\n",
    "        \n",
    "    def pad_image(self, img):\n",
    "        ''' Takes an input image (batch) and pads according to Tensorflows SAME padding'''\n",
    "        input_h = img.shape[2]\n",
    "        input_w = img.shape[3]\n",
    "        stride = 2 \n",
    "        filter_h = 3\n",
    "        filter_w = 3\n",
    "\n",
    "        output_h = int(ceil(float(input_h)) / float(stride))\n",
    "        output_w = output_h\n",
    "\n",
    "        if input_h % stride == 0:\n",
    "            pad_height = max((filter_h - stride), 0)\n",
    "        else:\n",
    "            pad_height = max((filter_h - (input_h % stride), 0))\n",
    "\n",
    "        pad_width = pad_height\n",
    "\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        padded_img = torch.zeros(img.shape[0], img.shape[1], input_h + pad_height, input_w + pad_width)\n",
    "        padded_img[:,:, pad_top:-pad_bottom, pad_left:-pad_right] = img\n",
    "\n",
    "        return padded_img\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pad_x = self.pad_image(x)\n",
    "        el1 = self.enc_l1(pad_x)\n",
    "        \n",
    "        pad_el1 = self.pad_image(el1)\n",
    "        el2 = self.enc_l2(pad_el1)\n",
    "    \n",
    "        pad_el2 = self.pad_image(el2)\n",
    "        el3 = self.enc_l3(pad_el2)\n",
    "        \n",
    "        pad_el3 = self.pad_image(el3)\n",
    "        el4 = self.enc_l4(pad_el3)\n",
    "        \n",
    "        return el4\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder'''\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "\n",
    "        # define layers\n",
    "        self.dec_l4 = nn.ConvTranspose2d(10, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l3 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "        self.dec_l2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l1 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        dl4 = self.dec_l4(enc_x)\n",
    "        dl3 = self.dec_l3(dl4)\n",
    "        dl2 = self.dec_l2(dl3)\n",
    "        decoded_x = self.dec_l1(dl2)\n",
    "        \n",
    "        return decoded_x\n",
    "        \n",
    "class nn_prototype(nn.Module):\n",
    "    '''Model'''\n",
    "    def __init__(self, n_prototypes=15, n_layers=4, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        # initialize prototype - currently not in correct spot\n",
    "        n_features = 40 # size of encoded x - 250 x 10 x 2 x 2\n",
    "        self.prototype_feature_vectors = nn.Parameter(torch.empty(size=(n_prototypes, n_features), dtype=torch.float32).uniform_())\n",
    "        \n",
    "        self.last_layer = nn.Linear(n_prototypes,10)\n",
    "        \n",
    "    def list_of_distances(self, X, Y):\n",
    "        '''\n",
    "        Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "        Y = [y_1, ... , y_m], we return a list of vectors\n",
    "                [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "                 ...\n",
    "                 [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "        where the distance metric used is the sqared euclidean distance.\n",
    "        The computation is achieved through a clever use of broadcasting.\n",
    "        '''\n",
    "        XX = torch.reshape(list_of_norms(X), shape=(-1, 1))\n",
    "        YY = torch.reshape(list_of_norms(Y), shape=(1, -1))\n",
    "        output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def list_of_norms(self, X):\n",
    "        '''\n",
    "        X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "            [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "        function is the squared euclidean distance.\n",
    "        '''\n",
    "        return torch.sum(torch.pow(X, 2), dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #encoder step\n",
    "        enc_x = self.encoder(x)\n",
    "        \n",
    "        #decoder step\n",
    "        dec_x = self.decoder(enc_x)\n",
    "        # hardcoded input size\n",
    "        dec_x = dec_x.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        \n",
    "        # flatten encoded x to compute distance with prototypes\n",
    "        n_features = enc_x.shape[1] * enc_x.shape[2] * enc_x.shape[3]\n",
    "        feature_vectors_flat = torch.reshape(enc_x, shape=[-1, n_features])\n",
    "        \n",
    "        # distance to prototype\n",
    "        prototype_distances = list_of_distances(feature_vectors_flat, self.prototype_feature_vectors)\n",
    "        \n",
    "        # distance to feature vectors\n",
    "        feature_vector_distances = list_of_distances(self.prototype_feature_vectors, feature_vectors_flat)\n",
    "        \n",
    "        # classification layer\n",
    "        logits = self.last_layer(prototype_distances)\n",
    "        \n",
    "        # Softmax to prob dist not needed as cross entropy loss is used?\n",
    "        \n",
    "        return dec_x, logits, feature_vector_distances, prototype_distances\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the error function consists of 4 terms, the autoencoder loss,\n",
    "the classification loss, and the two requirements that every feature vector in\n",
    "X look like at least one of the prototype feature vectors and every prototype\n",
    "feature vector look like at least one of the feature vectors in X.\n",
    "'''\n",
    "def loss_function(X_decoded, X_true, logits, Y, feature_dist, prototype_dist, lambdas=None):\n",
    "    if lambdas == None:\n",
    "        lambda_class, lambda_ae, lambda_1, lambda_2 = 20, 1, 1, 1\n",
    "    \n",
    "    batch_size = X_true.shape[0]\n",
    "    \n",
    "    ae_error = torch.mean(list_of_norms(X_decoded - X_true))\n",
    "    class_error = F.cross_entropy(logits, Y)\n",
    "    error_1 = torch.mean(torch.min(feature_dist, axis=1)[0])\n",
    "    error_2 = torch.mean(torch.min(prototype_dist, axis = 1)[0])\n",
    "\n",
    "    # total_error is the our minimization objective\n",
    "    total_error = lambda_class * class_error +\\\n",
    "                  lambda_ae * ae_error + \\\n",
    "                  lambda_1 * error_1 + \\\n",
    "                  lambda_2 * error_2\n",
    "\n",
    "    return total_error/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "### <font color='red'> Currently not working properly </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(logits, labels, batch_size):\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    total_correct = torch.sum(predictions == labels).item()\n",
    "    accuracy = total_correct / batch_size\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = nn_prototype(15,4,10)\n",
    "#     print(\"Model parameters:\")\n",
    "#     for name, p in model.named_parameters():\n",
    "#         print(name)\n",
    "#     print(\"\\n\")\n",
    "    \n",
    "    # initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # initialize storage for results\n",
    "    train_acc = []\n",
    "    train_losses = []\n",
    "    test_acc = []\n",
    "    test_losses = []\n",
    "    valid_acc = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(training_epochs):\n",
    "        print(\"epoch: \", epoch)\n",
    "\n",
    "        # loop over the batches\n",
    "        for step, (x, Y) in enumerate(train_dl):\n",
    "            # x is non_flattened batch of images (since using convnets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x = x.view(x.shape[0], 1, x.shape[1], x.shape[2]).float()\n",
    "            \n",
    "            # perform forward pass\n",
    "            X_decoded, logits, feature_dist, prot_dist = model(x)\n",
    "\n",
    "            # compute the loss\n",
    "            total_loss = loss_function(X_decoded, x, logits, Y, feature_dist, prot_dist)\n",
    "\n",
    "            # backpropagate over the loss\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute and save accuracy and loss\n",
    "            accuracy = compute_acc(F.softmax(logits, dim=1), Y, batch_size)\n",
    "            train_acc.append(accuracy)\n",
    "            train_losses.append(total_loss.item())\n",
    "    \n",
    "            # print information\n",
    "            if step % 20 == 0:\n",
    "                print('\\nStep: ', step)\n",
    "                print('Train loss: ', total_loss.item())\n",
    "                print('Train acc: ', accuracy)\n",
    "        \n",
    "        # testing\n",
    "        if epoch % test_display_step == 0:\n",
    "            # perform testing\n",
    "            with torch.no_grad():\n",
    "                for step, (x_test, y_test) in enumerate(test_dl):\n",
    "                    x_test = x.view(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2]).float()\n",
    "                    \n",
    "                    # forward pass\n",
    "                    X_decoded, logits, feature_dist, prot_dist = model(x_test)\n",
    "\n",
    "                    # compute loss and accuracy and save\n",
    "                    total_loss = loss_function(X_decoded, x_test, logits, y_test, feature_dist, prot_dist)\n",
    "                    test_accuracy = compute_acc(logits, y_test, batch_size)\n",
    "                    test_loss = loss_function(X_decoded, x_test, logits, y_test, feature_dist, prot_dist)\n",
    "                    test_acc.append(test_accuracy)\n",
    "                    test_losses.append(test_loss)\n",
    "                    \n",
    "                print('Test loss: ', test_loss.item())\n",
    "                print('Test acc: ', test_accuracy)\n",
    "                    \n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            for step, (x_valid, y_valid) in enumerate(valid_dl):\n",
    "                    x_valid = x.view(x_valid.shape[0], 1, x_valid.shape[1], x_valid.shape[2]).float()\n",
    "                    X_decoded, logits, feature_dist, prot_dist = model(x_valid)\n",
    "                    total_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist)\n",
    "                    valid_accuracy = compute_acc(logits, y_valid, batch_size)\n",
    "                    valid_loss = loss_function(X_decoded, x_valid, logits, y_valid, feature_dist, prot_dist)\n",
    "                    valid_acc.append(valid_accuracy)\n",
    "                    valid_losses.append(valid_loss)\n",
    "            print('Valid loss: ', valid_loss.item())\n",
    "            print('Valid acc: ', valid_accuracy)\n",
    "        \n",
    "    return train_acc, train_loss, test_acc, test_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "\n",
      "Step:  0\n",
      "Train loss:  169.23463439941406\n",
      "Train acc:  0.092\n",
      "\n",
      "Step:  20\n",
      "Train loss:  32.5084114074707\n",
      "Train acc:  0.088\n",
      "\n",
      "Step:  40\n",
      "Train loss:  21.734891891479492\n",
      "Train acc:  0.156\n",
      "\n",
      "Step:  60\n",
      "Train loss:  17.311674118041992\n",
      "Train acc:  0.128\n",
      "\n",
      "Step:  80\n",
      "Train loss:  17.28351402282715\n",
      "Train acc:  0.172\n",
      "\n",
      "Step:  100\n",
      "Train loss:  15.273221969604492\n",
      "Train acc:  0.38\n",
      "\n",
      "Step:  120\n",
      "Train loss:  14.073651313781738\n",
      "Train acc:  0.388\n",
      "\n",
      "Step:  140\n",
      "Train loss:  12.967559814453125\n",
      "Train acc:  0.312\n",
      "\n",
      "Step:  160\n",
      "Train loss:  11.939696311950684\n",
      "Train acc:  0.48\n",
      "\n",
      "Step:  180\n",
      "Train loss:  11.770564079284668\n",
      "Train acc:  0.464\n",
      "\n",
      "Step:  200\n",
      "Train loss:  10.481990814208984\n",
      "Train acc:  0.456\n",
      "\n",
      "Epoch 0 finished\n",
      "Test loss:  10.159423828125\n",
      "Test acc:  0.416\n",
      "Test loss:  10.267666816711426\n",
      "Test acc:  0.096\n",
      "Valid loss:  10.264192581176758\n",
      "Valid acc:  0.084\n",
      "epoch:  1\n",
      "\n",
      "Step:  0\n",
      "Train loss:  9.999153137207031\n",
      "Train acc:  0.5\n",
      "\n",
      "Step:  20\n",
      "Train loss:  9.716378211975098\n",
      "Train acc:  0.54\n",
      "\n",
      "Step:  40\n",
      "Train loss:  8.785962104797363\n",
      "Train acc:  0.592\n",
      "\n",
      "Step:  60\n",
      "Train loss:  9.12709903717041\n",
      "Train acc:  0.632\n",
      "\n",
      "Step:  80\n",
      "Train loss:  9.269392967224121\n",
      "Train acc:  0.508\n",
      "\n",
      "Step:  100\n",
      "Train loss:  8.808485984802246\n",
      "Train acc:  0.644\n",
      "\n",
      "Step:  120\n",
      "Train loss:  8.597360610961914\n",
      "Train acc:  0.532\n",
      "\n",
      "Step:  140\n",
      "Train loss:  8.281108856201172\n",
      "Train acc:  0.652\n",
      "\n",
      "Step:  160\n",
      "Train loss:  7.858309268951416\n",
      "Train acc:  0.684\n",
      "\n",
      "Step:  180\n",
      "Train loss:  8.08008098602295\n",
      "Train acc:  0.672\n",
      "\n",
      "Step:  200\n",
      "Train loss:  7.679106712341309\n",
      "Train acc:  0.636\n",
      "\n",
      "Epoch 1 finished\n",
      "Test loss:  7.610802173614502\n",
      "Test acc:  0.624\n",
      "Valid loss:  7.838609218597412\n",
      "Valid acc:  0.108\n",
      "epoch:  2\n",
      "\n",
      "Step:  0\n",
      "Train loss:  7.494666576385498\n",
      "Train acc:  0.664\n",
      "\n",
      "Step:  20\n",
      "Train loss:  7.400575637817383\n",
      "Train acc:  0.7\n",
      "\n",
      "Step:  40\n",
      "Train loss:  7.730474948883057\n",
      "Train acc:  0.764\n",
      "\n",
      "Step:  60\n",
      "Train loss:  7.716811180114746\n",
      "Train acc:  0.744\n",
      "\n",
      "Step:  80\n",
      "Train loss:  7.738919258117676\n",
      "Train acc:  0.664\n",
      "\n",
      "Step:  100\n",
      "Train loss:  7.406308650970459\n",
      "Train acc:  0.78\n",
      "\n",
      "Step:  120\n",
      "Train loss:  7.417786598205566\n",
      "Train acc:  0.632\n",
      "\n",
      "Step:  140\n",
      "Train loss:  7.226009368896484\n",
      "Train acc:  0.748\n",
      "\n",
      "Step:  160\n",
      "Train loss:  6.9414262771606445\n",
      "Train acc:  0.72\n",
      "\n",
      "Step:  180\n",
      "Train loss:  7.223001956939697\n",
      "Train acc:  0.732\n",
      "\n",
      "Step:  200\n",
      "Train loss:  6.832212448120117\n",
      "Train acc:  0.716\n",
      "\n",
      "Epoch 2 finished\n",
      "Test loss:  6.939188003540039\n",
      "Test acc:  0.716\n",
      "Valid loss:  7.2110443115234375\n",
      "Valid acc:  0.1\n",
      "epoch:  3\n",
      "\n",
      "Step:  0\n",
      "Train loss:  6.7450785636901855\n",
      "Train acc:  0.724\n",
      "\n",
      "Step:  20\n",
      "Train loss:  6.805266857147217\n",
      "Train acc:  0.732\n",
      "\n",
      "Step:  40\n",
      "Train loss:  7.144042015075684\n",
      "Train acc:  0.78\n",
      "\n",
      "Step:  60\n",
      "Train loss:  7.16785192489624\n",
      "Train acc:  0.796\n",
      "\n",
      "Step:  80\n",
      "Train loss:  7.123167991638184\n",
      "Train acc:  0.72\n",
      "\n",
      "Step:  100\n",
      "Train loss:  6.897398948669434\n",
      "Train acc:  0.788\n",
      "\n",
      "Step:  120\n",
      "Train loss:  6.922967910766602\n",
      "Train acc:  0.668\n",
      "\n",
      "Step:  140\n",
      "Train loss:  6.748678207397461\n",
      "Train acc:  0.768\n",
      "\n",
      "Step:  160\n",
      "Train loss:  6.494755268096924\n",
      "Train acc:  0.764\n",
      "\n",
      "Step:  180\n",
      "Train loss:  6.7022480964660645\n",
      "Train acc:  0.764\n",
      "\n",
      "Step:  200\n",
      "Train loss:  6.369790077209473\n",
      "Train acc:  0.756\n",
      "\n",
      "Epoch 3 finished\n",
      "Test loss:  6.544060230255127\n",
      "Test acc:  0.736\n",
      "Valid loss:  6.841311931610107\n",
      "Valid acc:  0.084\n",
      "epoch:  4\n",
      "\n",
      "Step:  0\n",
      "Train loss:  6.341357231140137\n",
      "Train acc:  0.764\n",
      "\n",
      "Step:  20\n",
      "Train loss:  6.463166236877441\n",
      "Train acc:  0.764\n",
      "\n",
      "Step:  40\n",
      "Train loss:  6.3002753257751465\n",
      "Train acc:  0.808\n",
      "\n",
      "Step:  60\n",
      "Train loss:  6.858966827392578\n",
      "Train acc:  0.796\n",
      "\n",
      "Step:  80\n",
      "Train loss:  6.741482734680176\n",
      "Train acc:  0.7\n",
      "\n",
      "Step:  100\n",
      "Train loss:  6.573251724243164\n",
      "Train acc:  0.804\n",
      "\n",
      "Step:  120\n",
      "Train loss:  6.644390106201172\n",
      "Train acc:  0.692\n",
      "\n",
      "Step:  140\n",
      "Train loss:  6.43609619140625\n",
      "Train acc:  0.808\n",
      "\n",
      "Step:  160\n",
      "Train loss:  6.199606418609619\n",
      "Train acc:  0.804\n",
      "\n",
      "Step:  180\n",
      "Train loss:  6.411838054656982\n",
      "Train acc:  0.764\n",
      "\n",
      "Step:  200\n",
      "Train loss:  6.0859694480896\n",
      "Train acc:  0.76\n",
      "\n",
      "Epoch 4 finished\n",
      "Test loss:  6.3042521476745605\n",
      "Test acc:  0.768\n",
      "Valid loss:  6.608465194702148\n",
      "Valid acc:  0.08\n",
      "epoch:  5\n",
      "\n",
      "Step:  0\n",
      "Train loss:  6.055039405822754\n",
      "Train acc:  0.8\n",
      "\n",
      "Step:  20\n",
      "Train loss:  6.202426433563232\n",
      "Train acc:  0.792\n",
      "\n",
      "Step:  40\n",
      "Train loss:  6.004855155944824\n",
      "Train acc:  0.824\n",
      "\n",
      "Step:  60\n",
      "Train loss:  7.765965461730957\n",
      "Train acc:  0.812\n",
      "\n",
      "Step:  80\n",
      "Train loss:  6.719648838043213\n",
      "Train acc:  0.74\n",
      "\n",
      "Step:  100\n",
      "Train loss:  6.376311779022217\n",
      "Train acc:  0.824\n",
      "\n",
      "Step:  120\n",
      "Train loss:  6.442899227142334\n",
      "Train acc:  0.696\n",
      "\n",
      "Step:  140\n",
      "Train loss:  6.243549823760986\n",
      "Train acc:  0.844\n",
      "\n",
      "Step:  160\n",
      "Train loss:  6.003837585449219\n",
      "Train acc:  0.808\n",
      "\n",
      "Step:  180\n",
      "Train loss:  6.222528457641602\n",
      "Train acc:  0.796\n",
      "\n",
      "Step:  200\n",
      "Train loss:  5.890354156494141\n",
      "Train acc:  0.776\n",
      "\n",
      "Epoch 5 finished\n",
      "Test loss:  6.143876075744629\n",
      "Test acc:  0.78\n",
      "Valid loss:  6.447458267211914\n",
      "Valid acc:  0.08\n",
      "epoch:  6\n",
      "\n",
      "Step:  0\n",
      "Train loss:  5.885095596313477\n",
      "Train acc:  0.828\n",
      "\n",
      "Step:  20\n",
      "Train loss:  6.0312981605529785\n",
      "Train acc:  0.8\n",
      "\n",
      "Step:  40\n",
      "Train loss:  5.803370952606201\n",
      "Train acc:  0.832\n",
      "\n",
      "Step:  60\n",
      "Train loss:  6.309686183929443\n",
      "Train acc:  0.796\n",
      "\n",
      "Step:  80\n",
      "Train loss:  6.412823677062988\n",
      "Train acc:  0.724\n",
      "\n",
      "Step:  100\n",
      "Train loss:  6.299393653869629\n",
      "Train acc:  0.8\n",
      "\n",
      "Step:  120\n",
      "Train loss:  6.528190612792969\n",
      "Train acc:  0.704\n",
      "\n",
      "Step:  140\n",
      "Train loss:  6.202939987182617\n",
      "Train acc:  0.852\n",
      "\n",
      "Step:  160\n",
      "Train loss:  5.845290184020996\n",
      "Train acc:  0.824\n",
      "\n",
      "Step:  180\n",
      "Train loss:  6.090970516204834\n",
      "Train acc:  0.796\n",
      "\n",
      "Step:  200\n",
      "Train loss:  5.765215873718262\n",
      "Train acc:  0.764\n",
      "\n",
      "Epoch 6 finished\n",
      "Test loss:  6.0018510818481445\n",
      "Test acc:  0.788\n",
      "Valid loss:  6.329177379608154\n",
      "Valid acc:  0.08\n",
      "epoch:  7\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
