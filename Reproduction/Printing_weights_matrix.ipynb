{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder'''\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "        \n",
    "        # define layers\n",
    "        self.enc_l1 = nn.Conv2d(n_input_channel, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.enc_l4 = nn.Conv2d(32, 10, kernel_size=3, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def pad_image(self, img):\n",
    "        ''' Takes an input image (batch) and pads according to Tensorflows SAME padding'''\n",
    "        input_h = img.shape[2]\n",
    "        input_w = img.shape[3]\n",
    "        stride = 2 \n",
    "        filter_h = 3\n",
    "        filter_w = 3\n",
    "\n",
    "        output_h = int(ceil(float(input_h)) / float(stride))\n",
    "        output_w = output_h\n",
    "\n",
    "        if input_h % stride == 0:\n",
    "            pad_height = max((filter_h - stride), 0)\n",
    "        else:\n",
    "            pad_height = max((filter_h - (input_h % stride), 0))\n",
    "\n",
    "        pad_width = pad_height\n",
    "\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        padded_img = torch.zeros(img.shape[0], img.shape[1], input_h + pad_height, input_w + pad_width)\n",
    "        padded_img[:,:, pad_top:-pad_bottom, pad_left:-pad_right] = img\n",
    "\n",
    "        return padded_img\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pad_x = self.pad_image(x)\n",
    "        el1 = self.relu(self.enc_l1(pad_x))\n",
    "        \n",
    "        pad_el1 = self.pad_image(el1)\n",
    "        el2 = self.relu(self.enc_l2(pad_el1))\n",
    "    \n",
    "        pad_el2 = self.pad_image(el2)\n",
    "        el3 = self.relu(self.enc_l3(pad_el2))\n",
    "        \n",
    "        pad_el3 = self.pad_image(el3)\n",
    "        el4 = self.relu(self.enc_l4(pad_el3))\n",
    "        \n",
    "        return el4\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder'''\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # height and width of each layers' filters\n",
    "        f_1 = 3\n",
    "        f_2 = 3\n",
    "        f_3 = 3\n",
    "        f_4 = 3\n",
    "\n",
    "        # define layers\n",
    "        self.dec_l4 = nn.ConvTranspose2d(10, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l3 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=0) # the output padding here should be 1 if the images are 32x32\n",
    "        self.dec_l2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_l1 = nn.ConvTranspose2d(32, n_input_channel, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        dl4 = self.relu(self.dec_l4(enc_x))\n",
    "        dl3 = self.relu(self.dec_l3(dl4))\n",
    "        dl2 = self.relu(self.dec_l2(dl3))\n",
    "        decoded_x = self.sigmoid(self.dec_l1(dl2))\n",
    "        \n",
    "        return decoded_x\n",
    "\n",
    "\n",
    "class nn_prototype(nn.Module):\n",
    "    '''Model'''\n",
    "    def __init__(self, n_prototypes=15, n_layers=4, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        # initialize prototype - currently not in correct spot\n",
    "        \n",
    "        # changed this for the colored mnist, from 40 to 160, the new shape would be 250*10*4*4\n",
    "        n_features = 40 # size of encoded x - 250 x 10 x 2 x 2\n",
    "        self.prototype_feature_vectors = nn.Parameter(torch.empty(size=(n_prototypes, n_features), \n",
    "                                                                  dtype=torch.float32).uniform_())\n",
    "        \n",
    "        self.last_layer = nn.Linear(n_prototypes,10)\n",
    "        \n",
    "    def list_of_distances(self, X, Y):\n",
    "        '''\n",
    "        Given a list of vectors, X = [x_1, ..., x_n], and another list of vectors,\n",
    "        Y = [y_1, ... , y_m], we return a list of vectors\n",
    "                [[d(x_1, y_1), d(x_1, y_2), ... , d(x_1, y_m)],\n",
    "                 ...\n",
    "                 [d(x_n, y_1), d(x_n, y_2), ... , d(x_n, y_m)]],\n",
    "        where the distance metric used is the sqared euclidean distance.\n",
    "        The computation is achieved through a clever use of broadcasting.\n",
    "        '''\n",
    "        XX = torch.reshape(self.list_of_norms(X), shape=(-1, 1))\n",
    "        YY = torch.reshape(self.list_of_norms(Y), shape=(1, -1))\n",
    "        output = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def list_of_norms(self, X):\n",
    "        '''\n",
    "        X is a list of vectors X = [x_1, ..., x_n], we return\n",
    "            [d(x_1, x_1), d(x_2, x_2), ... , d(x_n, x_n)], where the distance\n",
    "        function is the squared euclidean distance.\n",
    "        '''\n",
    "        return torch.sum(torch.pow(X, 2), dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"Shape of input x\", x.shape)\n",
    "        \n",
    "        #encoder step\n",
    "        enc_x = self.encoder(x)\n",
    "        \n",
    "        #print(\"Shape of encoded x\", enc_x.shape)\n",
    "        \n",
    "        #decoder step\n",
    "        dec_x = self.decoder(enc_x)\n",
    "        \n",
    "        #print(\"shape of decoded x\", dec_x.shape)\n",
    "        \n",
    "        # hardcoded input size (not needed, shape already correct)\n",
    "        # dec_x = dec_x.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        \n",
    "        # flatten encoded x to compute distance with prototypes\n",
    "        n_features = enc_x.shape[1] * enc_x.shape[2] * enc_x.shape[3]\n",
    "        feature_vectors_flat = torch.reshape(enc_x, shape=[-1, n_features])\n",
    "        \n",
    "        #print(\"Shape of flattened feature vectors\", feature_vectors_flat.shape)\n",
    "        \n",
    "        # distance to prototype\n",
    "        prototype_distances = self.list_of_distances(feature_vectors_flat, self.prototype_feature_vectors)\n",
    "        \n",
    "        # distance to feature vectors\n",
    "        feature_vector_distances = self.list_of_distances(self.prototype_feature_vectors, feature_vectors_flat)\n",
    "        \n",
    "        # classification layer\n",
    "        logits = self.last_layer(prototype_distances)\n",
    "        \n",
    "        # Softmax to prob dist not needed as cross entropy loss is used?\n",
    "        \n",
    "        return dec_x, logits, feature_vector_distances, prototype_distances\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:419: UserWarning: Couldn't retrieve source code for container of type nn_prototype. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:419: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/Users/TomLotze/miniconda3/envs/fact/lib/python3.7/site-packages/torch/serialization.py:419: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "loaded_model = torch.load(\"./saved_model/gray_mnist_model_color28_20_10_1_1\"+\"/gray_mnist_cae_color28_20_10_1_1_epoch_30.pt\", \n",
    "                         map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & min index\\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/0.png} & 0.36 & 0.33 & -0.2 & -0.72 & 0.67 & -0.13 & 0.81 & -0.89 & 0.3 & 0.14 & 7 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/1.png} & -0.22 & -0.3 & -1.58 & -0.22 & 0.28 & 0.77 & 0.12 & 0.16 & 0.47 & 0.04 & 2 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/2.png} & 0.14 & -0.32 & 0.15 & -0.23 & 0.28 & 0.47 & 0.35 & 0.16 & -1.64 & 0.08 & 8 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/3.png} & -1.03 & 0.47 & -0.25 & 0.48 & 0.23 & -0.31 & 0.18 & 0.07 & -0.2 & 0.49 & 0 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/4.png} & 0.2 & -0.51 & 0.36 & 0.28 & -1.24 & 0.3 & -0.23 & 0.06 & 0.24 & 0.12 & 4 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/5.png} & -0.03 & -0.08 & -0.24 & -0.08 & 0.16 & -0.15 & -0.84 & 0.55 & 0.23 & 0.16 & 6 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/6.png} & -0.99 & 0.49 & 0.02 & 0.37 & -0.12 & 0.19 & -0.2 & -0.48 & 0.05 & 0.64 & 0 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/7.png} & 0.7 & -1.4 & 0.21 & -0.68 & 0.28 & 0.85 & 0.41 & -0.02 & -0.46 & 0.46 & 1 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/8.png} & -0.34 & 0.57 & 0.45 & 0.34 & -0.04 & -0.87 & -1.17 & 0.39 & -0.75 & -0.05 & 6 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/9.png} & -0.02 & -0.37 & 0.25 & 0.52 & -1.08 & -0.1 & -0.1 & -0.03 & 0.57 & 0.24 & 4 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/10.png} & 0.11 & 0.05 & -0.16 & -0.34 & 0.41 & -0.17 & -1.15 & 0.48 & 0.49 & -0.36 & 6 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/11.png} & 0.4 & 0.19 & -0.13 & 0.37 & 0.17 & 0.12 & 0.22 & -0.27 & 0.31 & -0.93 & 9 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/12.png} & 0.38 & 0.26 & 0.13 & 0.57 & -0.01 & 0.12 & 0.22 & 0.22 & -0.51 & -1.07 & 9 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/13.png} & 0.13 & 0.29 & 0.51 & 0.18 & -0.07 & -0.3 & 0.48 & -0.31 & 0.14 & -0.91 & 9 \\\\\\hline\n",
      "\\includegraphics[width=0.035\\textwidth]{Images/14.png} & 0.26 & -0.0 & 0.71 & -1.65 & 0.22 & -0.9 & 0.37 & 0.07 & 0.54 & 0.28 & 3 \\\\\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"& \"+\" & \".join([str(i) for i in list(range(10))])+ \" & min index\\\\\\\\\\\\hline\")\n",
    "for i, row in enumerate(loaded_model.last_layer.weight.t()):\n",
    "    #img = plt.imread(\"./saved_model/gray_mnist_model_color28_20_10_1_1/img/prototypes_epoch_30/\"+str(i)+\".png\")\n",
    "    #plt.imshow(img)\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    row = np.round(row.detach().numpy(), 2)\n",
    "    print(\"\\\\includegraphics[width=0.035\\\\textwidth]{Images/%d.png}\"%i+\" & \"+\" & \".join([str(i) for i in row]), \"&\", np.argmin(row), \"\\\\\\\\\\\\hline\")\n",
    "    #print(row, np.argmin(row), np.min(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.64  0.37 -0.56  0.29 -1.15  0.45  0.36  0.54  0.02  0.03] 4 -1.15\n",
      "[ 0.53  0.35  0.49 -0.48  0.21 -1.3  -0.2   0.24 -0.01 -0.37] 5 -1.3\n",
      "[ 0.27 -0.53 -0.17  0.13  0.08 -0.13  0.2  -0.02  0.42 -0.54] 9 -0.54\n",
      "[-1.63  0.42  0.04  0.44  0.16  0.11 -0.07  0.27  0.3  -0.25] 0 -1.63\n",
      "[ 0.15  0.06 -0.16 -1.29  0.45 -0.06  0.62  0.02  0.34  0.11] 3 -1.29\n",
      "[ 0.64 -0.36 -0.05 -0.26 -0.81  0.33 -0.06 -0.14  0.72  0.63] 4 -0.81\n",
      "[-0.09  0.44  0.44  0.09 -0.85  0.02  0.24 -0.3  -0.14 -1.54] 9 -1.54\n",
      "[ 0.01 -0.41 -0.23  0.38  0.67 -0.59 -0.15 -0.08 -0.03  0.45] 5 -0.59\n",
      "[-0.12  0.23 -0.14  0.15  0.48  0.36  0.5  -1.42  0.51  0.14] 7 -1.42\n",
      "[-0.35  0.17 -0.7  -0.74  1.09  0.2  -0.04  0.38 -0.82  0.46] 8 -0.82\n",
      "[ 0.03 -0.77  0.34  0.17 -0.31  0.33  0.18 -0.06 -0.78 -0.26] 8 -0.78\n",
      "[-0.05 -0.63  0.44  0.02  0.19 -0.12 -0.24  0.19 -0.13  0.16] 1 -0.63\n",
      "[-0.3  -0.39  0.24  0.03 -0.33  0.38 -0.32 -0.35  0.05  0.4 ] 1 -0.39\n",
      "[ 0.33  0.37  0.02  0.21 -0.22  0.24  0.06  0.38 -1.39  0.13] 8 -1.39\n",
      "[-0.22  0.06  0.    0.67 -0.34 -0.05 -1.22  0.12  0.44  0.28] 6 -1.22\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "original = torch.load(\"./saved_model/mnist_model/mnist_cae_epoch_30.pt\"+\"\", \n",
    "                         map_location=torch.device('cpu'))\n",
    "\n",
    "for i, row in enumerate(original.last_layer.weight.t()):\n",
    "    #img = plt.imread(\"./saved_model/mnist_model/img/prototypes_epoch_30/\"+str(i)+\".png\")\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    row = np.round(row.detach().numpy(), 2)\n",
    "    print(row, np.argmin(row), np.min(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
